{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/neuromatch/NeuroAI_Course/blob/main/tutorials/W1D3_ComparingArtificialAndBiologicalNetworks/student/W1D3_Tutorial1.ipynb\" target=\"_blank\"><img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"/></a>   <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/neuromatch/NeuroAI_Course/main/tutorials/W1D3_ComparingArtificialAndBiologicalNetworks/student/W1D3_Tutorial1.ipynb\" target=\"_blank\"><img alt=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Tutorial 1: Generalization and representational geometry\n",
    "\n",
    "**Week 1, Day 3: Comparing artificial and biological neural networks**\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__ JohnMark Taylor & Zhuofan Josh Ying\n",
    "\n",
    "__Content reviewers:__ Samuele Bolotta, Yizhou Chen, RyeongKyung Yoon, Ruiyi Zhang, Lily Chamakura, Hlib Solodzhuk\n",
    "\n",
    "__Production editors:__ Konstantine Tsafatinos, Ella Batty, Spiros Chavlis, Samuele Bolotta, Hlib Solodzhuk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "___\n",
    "\n",
    "\n",
    "# Tutorial Objectives\n",
    "\n",
    "*Estimated timing of tutorial: 30 minutes*\n",
    "\n",
    "Welcome to Tutorial 1 on Generalization and Representational Geometry. This tutorial aims to bridge the gap between theoretical concepts and practical applications in machine learning, focusing on the relationship between generalization and condition similarities based on linear models. \n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "\n",
    "- Understand the connection between generalization and condition similarities in the linear case, including a nod towards the kernel trick discussed in lectures.\n",
    "- Understand how the analytic solution for linear regression can be seen as a weighted sum of training values, weighted by the test stimuli’s similarity to the training data.\n",
    "\n",
    "Exercises include:\n",
    "\n",
    "1.   Question on comparing RDMs\n",
    "2.   Question on the relationship between RDMs and model performances\n",
    "3.   Interactive exercise on how similarity structure affects predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @markdown\n",
    "from IPython.display import IFrame\n",
    "from ipywidgets import widgets\n",
    "out = widgets.Output()\n",
    "with out:\n",
    "    print(f\"If you want to download the slides: https://osf.io/download/qmv5r/\")\n",
    "    display(IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/qmv5r/?direct%26mode=render%26action=download%26mode=render\", width=730, height=410))\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Install dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Install dependencies\n",
    "#! pip install ipympl ipywidgets mpl_interactions[\"jupyter\"] rsatoolbox torchlens\n",
    "#! pip install graphviz\n",
    "\n",
    "# To install jupyter-matplotlib (ipympl) via pip\n",
    "!pip install -q torchlens\n",
    "!pip install -q ipympl ipywidgets matplotlib numpy scikit-learn torch torchvision rsatoolbox scipy\n",
    "\n",
    "\n",
    "# To install jupyter-matplotlib via conda (comment out if you are not using conda)\n",
    "# !conda install -c conda-forge ipympl\n",
    "\n",
    "# To install the JupyterLab extension for ipywidgets\n",
    "# !jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    "\n",
    "# To install the JupyterLab extension for jupyter-matplotlib\n",
    "# !jupyter labextension install jupyter-matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Import dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Import dependencies\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "import torchlens as tl\n",
    "import warnings\n",
    "\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from collections import OrderedDict\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "import numpy as np\n",
    "\n",
    "import rsatoolbox\n",
    "from rsatoolbox.data import Dataset\n",
    "from rsatoolbox.rdm.calc import calc_rdm\n",
    "from scipy import stats\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Figure settings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "# @markdown\n",
    "\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' # perfrom high definition rendering for images and plots\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Plotting functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Plotting functions\n",
    "\n",
    "def sample_images(data_loader, n=5, plot=False):\n",
    "    \"\"\"\n",
    "    Samples a specified number of images from a data loader.\n",
    "\n",
    "    Args:\n",
    "    - data_loader (torch.utils.data.DataLoader): Data loader containing images and labels.\n",
    "    - n (int): Number of images to sample per class.\n",
    "    - plot (bool): Whether to plot the sampled images using matplotlib.\n",
    "\n",
    "    Returns:\n",
    "    - imgs (torch.Tensor): Sampled images.\n",
    "    - labels (torch.Tensor): Corresponding labels for the sampled images.\n",
    "    \"\"\"\n",
    "\n",
    "    with plt.xkcd():\n",
    "\n",
    "        imgs, labels = next(iter(data_loader))\n",
    "\n",
    "        imgs_o = []\n",
    "        targets = []\n",
    "        for value in range(10):\n",
    "            imgs_o.append(imgs[np.where(labels == value)][0:n])\n",
    "            targets.append([value]*5)\n",
    "\n",
    "        imgs = torch.cat(imgs_o, dim=0)\n",
    "        targets = torch.tensor(targets).flatten()\n",
    "\n",
    "        if plot:\n",
    "            plt.imshow(torch.moveaxis(make_grid(imgs, nrow=5, padding=0, normalize=False, pad_value=0), 0,-1))\n",
    "            plt.axis('off')\n",
    "\n",
    "    return imgs, targets\n",
    "\n",
    "\n",
    "def plot_maps(model_features, model_name):\n",
    "\n",
    "    with plt.xkcd():\n",
    "\n",
    "        fig = plt.figure(figsize=(14, 4))\n",
    "        fig.suptitle(f\"RDMs across layers for {model_name}\")\n",
    "        # and we add one plot per reference point\n",
    "        gs = fig.add_gridspec(1, len(model_features))\n",
    "        fig.subplots_adjust(wspace=0.2, hspace=0.2)\n",
    "\n",
    "        for l in range(len(model_features)):\n",
    "\n",
    "            layer = list(model_features.keys())[l]\n",
    "            map_ = np.squeeze(model_features[layer])\n",
    "\n",
    "            if len(map_.shape) < 2:\n",
    "                map_ = map_.reshape( (int(np.sqrt(map_.shape[0])), int(np.sqrt(map_.shape[0]))) )\n",
    "\n",
    "            map_ = map_ / np.max(map_)\n",
    "\n",
    "            ax = plt.subplot(gs[0,l])\n",
    "            ax_ = ax.imshow(map_, cmap='magma_r')\n",
    "            ax.set_title(f'{layer}')\n",
    "            ax.set_xlabel(\"input index\")\n",
    "            if l==0:\n",
    "              ax.set_ylabel(\"input index\")\n",
    "\n",
    "        fig.subplots_adjust(right=0.9)\n",
    "        cbar_ax = fig.add_axes([1.01, 0.18, 0.01, 0.53])\n",
    "        cbar = fig.colorbar(ax_, cax=cbar_ax)\n",
    "        cbar.set_label('Dissimilarity', rotation=270, labelpad=15)\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Helper functions\n",
    "\n",
    "def train_one_epoch(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if args.dry_run:\n",
    "                break\n",
    "\n",
    "def test(model, device, test_loader, return_features=False):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "def build_args():\n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--epochs', type=int, default=2, metavar='N',\n",
    "                        help='number of epochs to train (default: 14)')\n",
    "    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
    "                        help='learning rate (default: 1.0)')\n",
    "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "                        help='Learning rate step gamma (default: 0.7)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--no-mps', action='store_true', default=False,\n",
    "                        help='disables macOS GPU training')\n",
    "    parser.add_argument('--dry-run', action='store_true', default=False,\n",
    "                        help='quickly check a single pass')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=50, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                        help='For Saving the current Model')\n",
    "    args = parser.parse_args('')\n",
    "\n",
    "    use_cuda = torch.cuda.is_available() #not args.no_cuda and\n",
    "    use_mps = not args.no_mps and torch.backends.mps.is_available()\n",
    "\n",
    "    if use_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif use_mps:\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    args.use_cuda = use_cuda\n",
    "    args.device = device\n",
    "    return args\n",
    "\n",
    "def fetch_dataloaders(args):\n",
    "\n",
    "    train_kwargs = {'batch_size': args.batch_size}\n",
    "    test_kwargs = {'batch_size': args.test_batch_size}\n",
    "    if args.use_cuda:\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transform)\n",
    "    dataset2 = datasets.MNIST('../data', train=False,\n",
    "                       transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def train_model(args, model, optimizer):\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train_one_epoch(args, model, args.device, train_loader, optimizer, epoch)\n",
    "        test(model, args.device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
    "\n",
    "def train_one_epoch_adversarial(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # train the model on adversarial images\n",
    "        epsilon = 0.2\n",
    "        data = generate_adversarial(model, data, target, epsilon)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if args.dry_run:\n",
    "                break\n",
    "\n",
    "def train_model_adversarial(args, model, optimizer):\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train_one_epoch_adversarial(args, model, args.device, train_loader, optimizer, epoch)\n",
    "        test(model, args.device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), \"robust_mnist_cnn.pt\")\n",
    "\n",
    "def calc_rdms(model_features, method='correlation'):\n",
    "    ds_list = []\n",
    "    for l in range(len(model_features)):\n",
    "\n",
    "        layer = list(model_features.keys())[l]\n",
    "        feats = model_features[layer]\n",
    "\n",
    "        if type(feats) is list:\n",
    "            feats = feats[-1]\n",
    "\n",
    "        if args.use_cuda:\n",
    "            feats = feats.cpu()\n",
    "\n",
    "        if len(feats.shape)>2:\n",
    "            feats = feats.flatten(1)\n",
    "\n",
    "        feats = feats.detach().numpy()\n",
    "        # feats = stats.zscore(feats)\n",
    "\n",
    "        ds = Dataset(feats, descriptors=dict(layer=layer))\n",
    "        ds_list.append(ds)\n",
    "\n",
    "    rdms = calc_rdm(ds_list, method=method)\n",
    "\n",
    "    rdms_dict = {list(model_features.keys())[i]: rdms.get_matrices()[i] for i in range(len(model_features))}\n",
    "\n",
    "    return rdms, rdms_dict\n",
    "\n",
    "# modified from https://pytorch.org/tutorials/beginner/fgsm_tutorial.html\n",
    "\n",
    "# FGSM attack code\n",
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    # Collect the element-wise sign of the data gradient\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    # Create the perturbed image by adjusting each pixel of the input image\n",
    "    perturbed_image = image + epsilon*sign_data_grad\n",
    "    # Adding clipping to maintain [0,1] range\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    # Return the perturbed image\n",
    "    return perturbed_image\n",
    "\n",
    "# restores the tensors to their original scale\n",
    "def denorm(batch, mean=[0.1307], std=[0.3081]):\n",
    "    \"\"\"\n",
    "    Convert a batch of tensors to their original scale.\n",
    "\n",
    "    Args:\n",
    "        batch (torch.Tensor): Batch of normalized tensors.\n",
    "        mean (torch.Tensor or list): Mean used for normalization.\n",
    "        std (torch.Tensor or list): Standard deviation used for normalization.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: batch of tensors without normalization applied to them.\n",
    "    \"\"\"\n",
    "    if isinstance(mean, list):\n",
    "        mean = torch.tensor(mean).to(batch.device)\n",
    "    if isinstance(std, list):\n",
    "        std = torch.tensor(std).to(batch.device)\n",
    "\n",
    "    return batch * std.view(1, -1, 1, 1) + mean.view(1, -1, 1, 1)\n",
    "\n",
    "def generate_adversarial(model, imgs, targets, epsilon):\n",
    "\n",
    "    # Accuracy counter\n",
    "    correct = 0\n",
    "    adv_imgs = []\n",
    "\n",
    "    # Loop over all examples in test set\n",
    "    for img, target in zip(imgs, targets):\n",
    "\n",
    "        img = img.unsqueeze(0)\n",
    "        target = target.unsqueeze(0)\n",
    "        # Set requires_grad attribute of tensor. Important for Attack\n",
    "        img.requires_grad = True\n",
    "\n",
    "        # Forward pass the data through the model\n",
    "        output = model(img)\n",
    "        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        # Zero all existing gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Calculate gradients of model in backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Collect ``datagrad``\n",
    "        data_grad = img.grad.data\n",
    "\n",
    "        # Restore the data to its original scale\n",
    "        data_denorm = denorm(img)\n",
    "\n",
    "        # Call FGSM Attack\n",
    "        perturbed_data = fgsm_attack(data_denorm, epsilon, data_grad)\n",
    "\n",
    "        # Reapply normalization\n",
    "        perturbed_data_normalized = transforms.Normalize((0.1307,), (0.3081,))(perturbed_data)\n",
    "\n",
    "        adv_imgs.append(perturbed_data_normalized.detach())\n",
    "\n",
    "    return torch.cat(adv_imgs)\n",
    "\n",
    "def test_adversarial(model, imgs, targets):\n",
    "\n",
    "    # Accuracy counter\n",
    "    correct = 0\n",
    "    adv_examples = []\n",
    "\n",
    "    output = model(imgs)\n",
    "\n",
    "    pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "    correct += pred.eq(targets.view_as(pred)).sum().item()\n",
    "\n",
    "    final_acc = correct/float(len(imgs))\n",
    "    print(f\"adversarial test accuracy = {correct} / {len(imgs)} = {final_acc}\")\n",
    "\n",
    "def extract_features(model, imgs, return_layers, plot ='none'):\n",
    "\n",
    "    model_history = tl.log_forward_pass(model, imgs, layers_to_save='all', vis_opt=plot)\n",
    "    model_features = {}\n",
    "    for layer in return_layers:\n",
    "        model_features[layer] = model_history[layer].tensor_contents.flatten(1)\n",
    "\n",
    "    return model_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Data retrieval\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import hashlib\n",
    "\n",
    "# Variables for file and download URL\n",
    "fnames = [\"standard_model.pth\", \"adversarial_model.pth\"] # The names of the files to be downloaded\n",
    "urls = [\"https://osf.io/s5rt6/download\", \"https://osf.io/qv5eb/download\"] # URLs from where the files will be downloaded\n",
    "expected_md5s = [\"2e63c2cd77bc9f1fa67673d956ec910d\", \"25fb34497377921b54368317f68a7aa7\"] # MD5 hashes for verifying files integrity\n",
    "\n",
    "for fname, url, expected_md5 in zip(fnames, urls, expected_md5s):\n",
    "    if not os.path.isfile(fname):\n",
    "        try:\n",
    "            # Attempt to download the file\n",
    "            r = requests.get(url) # Make a GET request to the specified URL\n",
    "        except requests.ConnectionError:\n",
    "            # Handle connection errors during the download\n",
    "            print(\"!!! Failed to download data !!!\")\n",
    "        else:\n",
    "            # No connection errors, proceed to check the response\n",
    "            if r.status_code != requests.codes.ok:\n",
    "                # Check if the HTTP response status code indicates a successful download\n",
    "                print(\"!!! Failed to download data !!!\")\n",
    "            elif hashlib.md5(r.content).hexdigest() != expected_md5:\n",
    "                # Verify the integrity of the downloaded file using MD5 checksum\n",
    "                print(\"!!! Data download appears corrupted !!!\")\n",
    "            else:\n",
    "                # If download is successful and data is not corrupted, save the file\n",
    "                with open(fname, \"wb\") as fid:\n",
    "                    fid.write(r.content) # Write the downloaded content to a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Set device (GPU or CPU)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Set device (GPU or CPU)\n",
    "# @markdown\n",
    "\n",
    "def set_device():\n",
    "    \"\"\"\n",
    "    Determines and sets the computational device for PyTorch operations based on the availability of a CUDA-capable GPU.\n",
    "\n",
    "    Outputs:\n",
    "    - device (str): The device that PyTorch will use for computations ('cuda' or 'cpu'). This string can be directly used\n",
    "    in PyTorch operations to specify the device.\n",
    "    \"\"\"\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if device != \"cuda\":\n",
    "        print(\"GPU is not enabled in this notebook. \\n\"\n",
    "              \"If you want to enable it, in the menu under `Runtime` -> \\n\"\n",
    "              \"`Hardware accelerator.` and select `GPU` from the dropdown menu\")\n",
    "    else:\n",
    "        print(\"GPU is enabled in this notebook. \\n\"\n",
    "              \"If you want to disable it, in the menu under `Runtime` -> \\n\"\n",
    "              \"`Hardware accelerator.` and select `None` from the dropdown menu\")\n",
    "\n",
    "    return device\n",
    "\n",
    "device = set_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "## Section 1: MNIST DNN Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @markdown Video\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "video_ids = [('Youtube', 'AjzqdrOrfgg'), ('Bilibili', '')]\n",
    "tab_contents = display_videos(video_ids, W=730, H=410)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In this activity, you will use linear regression on representations of two neural networks trained on MNIST, one with adversarial training and one without. The objective is to observe:\n",
    "\n",
    "- How predictions for clean images of numbers are similar for a deep layer and closely align with the category structure.\n",
    "- The dissimilarity matrix reflects a category structure and how matrices from the two networks compare.\n",
    "- The significant differences in predictions and dissimilarity matrices when applying adversarial examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Understanding the MNIST DNNs\n",
    "\n",
    "We start by defining two models on the MNIST dataset: a standard model and an adversarially robust model. The training process for the adaversarially robust model includes a step where adversarial examples are generated and used to train the robust model.\n",
    "\n",
    "You are already provided with the trained models to save the time, still you are free to explore training code being included in the tutorial.\n",
    "\n",
    "**Defining the MNIST Model**\n",
    "\n",
    "First, we define a neural network model that will be trained on the MNIST dataset. The MNIST dataset consists of 28x28 pixel images of handwritten digits (0 through 9). Our model, based on the LeNet architecture, includes two convolutional layers followed by dropout layers to reduce overfitting, and finally, fully connected layers to perform classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " MNIST Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @markdown MNIST Model\n",
    "\n",
    "# modified and reorganized from https://github.com/pytorch/examples/blob/main/mnist/main.py\n",
    "\n",
    "#lenet model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Training the Standard Model**\n",
    "\n",
    "Once our model is defined, we proceed to train it on the MNIST dataset. Training involves feeding batches of images and their corresponding labels to the model, adjusting the model's weights based on the loss computed from its predictions and the actual labels through backpropagation and gradient descent. This process is iterated over multiple epochs to improve the model's accuracy.\n",
    "\n",
    "Training code provided here is commented and you can miss it by heading towards the load of the already trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Build and train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @markdown Build and train the model\n",
    "\n",
    "# args = build_args()\n",
    "# torch.manual_seed(args.seed)\n",
    "# train_loader, test_loader = fetch_dataloaders(args)\n",
    "\n",
    "# # build_model\n",
    "# model = Net().to(args.device)\n",
    "\n",
    "# print(model)\n",
    "# optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "\n",
    "# # train model\n",
    "# train_model(args, model, optimizer) #train the model for 2 epochs ~ %99 accuracy ~ 30 sec colab gpu\n",
    "\n",
    "# torch.save(model, \"standard_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Grab a pretrained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @markdown Grab a pretrained model\n",
    "\n",
    "args = build_args()\n",
    "train_loader, test_loader = fetch_dataloaders(args)\n",
    "path = \"standard_model.pth\"\n",
    "model = torch.load(path, map_location=args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Adversarial Attack & Model Robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Generating Adversarial Examples**\n",
    "\n",
    "Adversarial examples are inputs to machine learning models that an attacker has intentionally designed to cause the model to make a mistake. They are often **indistinguishable** from real data by humans but result in incorrect predictions by the model. Generating these examples and using them in training can improve a model's robustness against such attach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Grab 5 test images from each category and visualize them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @markdown Grab 5 test images from each category and visualize them\n",
    "imgs, targets = sample_images(test_loader, n=5, plot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Here, we generate adversarial images using the Fast Gradient Sign Method (FGSM). This is a popular and straightforward adversarial attack technique designed to test the robustness of neural networks. Developed by Ian Goodfellow and his colleagues in 2014, FGSM backprops through the neural network to create perturbed inputs that maximize the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Generate adversarial images for the standard model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @markdown Generate adversarial images for the standard model\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "eps = 0.2\n",
    "imgs =imgs.to(args.device)\n",
    "targets = targets.to(args.device)\n",
    "\n",
    "adv_imgs = generate_adversarial(model, imgs, targets, eps)\n",
    "adv_imgs.shape\n",
    "\n",
    "plt.imshow(torch.moveaxis(make_grid(adv_imgs.cpu(), nrow=5, padding=0, normalize=False, pad_value=0), 0,-1))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Training the Adversarially Robust Model**\n",
    "\n",
    "With adversarial examples at hand, we now train a model designed to be robust against such examples. The idea is to include adversarial examples in the training process, enabling the model to learn from and defend against them.\n",
    "\n",
    "Again, there is training code defined below which is commented and you are presented with already adversarially trained network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Train adversirally robust model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @markdown Train adversirally robust model\n",
    "\n",
    "# model_robust = Net().to(args.device)\n",
    "# optimizer = optim.Adadelta(model_robust.parameters(), lr=args.lr)\n",
    "# train_model_adversarial(args, model_robust, optimizer) #train the model for 2 epochs ~ %98 accuracy ~ 2 minutes on cpu\n",
    "# torch.save(model_robust, \"adversarial_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Grab an adversarially pretrained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @markdown Grab an adversarially pretrained model\n",
    "\n",
    "path = \"adversarial_model.pth\"\n",
    "model_robust = torch.load(path, map_location=args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Generate adversarial images for the adversarially trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @markdown Generate adversarial images for the adversarially trained model\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "eps = 0.2\n",
    "imgs =imgs.to(args.device)\n",
    "targets = targets.to(args.device)\n",
    "\n",
    "adv_imgs_advmodel = generate_adversarial(model_robust, imgs, targets, eps)\n",
    "adv_imgs_advmodel.shape\n",
    "\n",
    "plt.imshow(torch.moveaxis(make_grid(adv_imgs_advmodel.cpu(), nrow=5, padding=0, normalize=False, pad_value=0), 0,-1))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Evaluating Model Robustness**\n",
    "\n",
    "After training, it's essential to evaluate how well the models perform, particularly in the presence of adversarial examples. We do this by testing the models on both standard and adversarial images and comparing their accuracies.\n",
    "\n",
    "Note that these adversarially generated images are indistinguishable from the real ones, but they decrease standard model performances dramatically, while adversarially trained model is robust to this kind of attack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Test model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @markdown Test model\n",
    "\n",
    "# test adversairal robustness\n",
    "print(\"For standard model trained without adversarial examples:\")\n",
    "test_adversarial(model, adv_imgs, targets)\n",
    "print(\"For adversrially trained model:\")\n",
    "test_adversarial(model_robust, adv_imgs_advmodel, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "You should observe that the adversarially trained model has much higher accuracy on the adversarial images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Extracting Model Features and Analyzing Representations\n",
    "\n",
    "With the models trained and adversarial images generated, we proceed to extract features from different layers of our networks using [torchlens](https://github.com/johnmarktaylor91/torchlens), a package for extracting neural network activations and visualizing their computational graph.  This step is crucial for understanding how data representations differ across layers and models, especially under adversarial conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Extract model features with torchlens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @markdown Extract model features with torchlens\n",
    "\n",
    "return_layers = ['input_1', 'conv1', 'conv2', 'fc1', 'fc2']\n",
    "features_model_imgs = extract_features(model, imgs, return_layers, plot = 'rolled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "features_model_advimgs = extract_features(model, adv_imgs, return_layers)\n",
    "features_advmodel_imgs = extract_features(model_robust, imgs, return_layers)\n",
    "features_advmodel_advimgs = extract_features(model_robust, adv_imgs_advmodel, return_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Creating and Comparing Representation Dissimilarity Matrices\n",
    "Using the RSA toolbox, you will create representational dissimilarity matrices (RDMs) for each model and condition (standard and adversarial). These matrices provide a visual representation of how stimuli are represented within the network, offering insights into the models' generalization capabilities and the effects of adversarial training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**What is a RDM?**\n",
    "\n",
    "Before diving into the practical applications, let's clarify what an RDM is. An RDM is a tool used to measure and visualize the dissimilarities in the responses of a neural system to different inputs. It is represented as a matrix $M$ where each element $M_{ij}$ measures how dissimilar the network's responses are to the $i$-th and $j$-th input. In the context of this tutorial, we use correlation distance to calculate the elements of the RDMs. Correlation distance is based on the Pearson correlation coefficient, which quantifies the linear relationship between two variables. Specifically, correlation distance is defined as: $\\text{Correlation distance} = 1 - r$, where $r$ is the Pearson correlation coefficient. Therefore, the element $M_{ij} = 1 - r(x_i, x_j)$, where $x_i, x_j$ refer to the model representations of the $i$-th and $j$-th inputs.\n",
    "\n",
    "RDMs are crucial for understanding how well a model can differentiate between various types of inputs, which is a key aspect of generalization. They are particularly useful in deep learning as they help illustrate the level at which different layers of a network or different netowrks recognize and differentiate between input categories. A well-performing model will show clear patterns of dissimilarity corresponding to different input categories, especially in deeper layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We first visualize the category structure by computing the RDM of the labels. To see clear blocks in RDMs—where blocks indicate that inputs of the same class are represented similarly—we need to ensure that input images are organized and grouped by their class labels before calculating the RDM. Here, we have 10 classes (number 0-9), each class with 5 samples, which sums up to 50 input images. Recall that each element of the RDM $M_{ij}$ refers the dissimilarity between the representations of the $i$-th and $j$-th inputs. Make sure you understand the figure of RDM of the labels below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " RDMs for the labels (see the category structures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @markdown RDMs for the labels (see the category structures)\n",
    "\n",
    "one_hot_labels = {\"labels\": F.one_hot(targets, num_classes=10) }\n",
    "rdms, rdms_dict = calc_rdms(one_hot_labels, method='euclidean')\n",
    "\n",
    "# plot rdm\n",
    "with plt.xkcd():\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.imshow(rdms_dict['labels']/rdms_dict['labels'].max(), cmap='magma_r')\n",
    "    plt.title(\"RDM of the labels\")\n",
    "    plt.xlabel(\"input index\")\n",
    "    plt.ylabel(\"input index\")\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.set_label('Dissimilarity', rotation=270, labelpad=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now we compute and visualize the RDMs for **standard images** for both the standard model and the adversarially trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For standard model + standard images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @markdown For standard model + standard images\n",
    "rdms, rdms_dict = calc_rdms(features_model_imgs)\n",
    "plot_maps(rdms_dict, \"Standard Model with Standard Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For adversarially trained model + standard images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @markdown For adversarially trained model + standard images\n",
    "rdms, rdms_dict = calc_rdms(features_advmodel_imgs)\n",
    "plot_maps(rdms_dict, \"Adversarial Trained Model with Standard Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "#### Question\n",
    "\n",
    "1. For the standard clean images, how do the RDMs change across the model layers, how do they compare to the category structure, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#to_remove explanation\n",
    "\n",
    "\"\"\"\n",
    "Question: For adversarial images, how do the RDMs change when comparing representations from the standard model to the adversarially trained model?\n",
    "\n",
    "For clean images representing the same digit, their representations in the deeper layers of the network are remarkably similar and align well with the inherent category structure, manifesting as a distinct block effect. In contrast, this block effect is less pronounced in the earlier layers of the network. The initial layers focus more on capturing general and granular visual features. This progression from generic to more refined feature extraction across layers underscores the hierarchical nature of learning in deep neural networks, where complex representations are built upon the simpler ones extracted at earlier stages.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "RDMs of the standard model and the adversarially trained model are very similar for clean images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now we compute and visualize the RDMs for **adversarial images** for both the standard model and the adversarially trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For standard model + adversarial images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @markdown For standard model + adversarial images\n",
    "rdms, rdms_dict = calc_rdms(features_model_advimgs)\n",
    "plot_maps(rdms_dict, \"Standard Model with Adversarial Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For adversarially trained model + adversarial images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @markdown For adversarially trained model + adversarial images\n",
    "rdms, rdms_dict = calc_rdms(features_advmodel_advimgs)\n",
    "plot_maps(rdms_dict, \"Adversarial Trained Model with Adversarial Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "#### Question\n",
    "\n",
    "1. For adversarial images, how do the RDMs change when comparing representations from the standard model to the adversarially trained model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#to_remove explanation\n",
    "\n",
    "\"\"\"\n",
    "Question: For adversarial images, how do the RDMs change when comparing representations from the standard model to the adversarially trained model?\n",
    "\n",
    "Note for adversarial images, the representation similarity of the standard model within each category (the block effect) is notably disrupted in the deeper layers.\n",
    "Conversely, for the adversarially trained model, despite the introduction of adversarial examples, the representation similarity matrix for preserves its block-like structure across categories.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "#### Question\n",
    "\n",
    "1. How does the RDMs relate to the performances of the standard and the adversarially trained models on clean and adversarial images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#to_remove explanation\n",
    "\n",
    "\"\"\"\n",
    "Question: How does the RDMs relate to the performances of the standard and the adversarially trained models on clean and adversarial images?\n",
    "\n",
    "RDMs provide a visual and quantitative way to analyze how a neural network processes and represents different stimuli. By comparing the similarity of responses within the network across various inputs, the RDM can reveal significant insights into the network's internal representations and, consequently, its ability to generalize.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Both the standard and the adversarially trained models show clear, distinct blocks in the RDM when inputs from the same category are represented similarly (high intra-class similarity) and inputs from different categories are distinct (low inter-class similarity). This differentiation is crucial for the model to achieve high accuracies on unseen test data from the same distribution.\n",
    "\n",
    "However, for adversarial images, the block effect of the standard model is notably disrupted. This disruption correlates with a diminished test accuracy when the model is evaluated on adversarial images, highlighting the vulnerability of the standard model to adversarial perturbations.\n",
    "For adversarially trained model, the structural preservation of its RDMs indicates that the model's internal representations of the stimuli remain robust against adversarial manipulation, allowing it to maintain high accuracy on adversarial images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Section 2: Interactive Exploration with Widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In Section 1, we saw that networks generalize better when their test stimuli are \"similar\" to their training stimuli: the network trained only on normal MNIST images generalized poorly to adversarially generated images since they were not part of its training diet, whereas the network trained with adversarial images generalized more effectively to new adversarial images. It turns out that we can sometimes be mathematically precise about how the similarity of training and test examples affects generalization performance. In this interactive example, you will explore this connection between similarity and generalization in the case of linear regression.\n",
    "\n",
    "The manner in which a readout function generalizes to new inputs depends closely on the similarity of the new inputs to training stimuli--in other words, the more similar a test stimulus is to a training stimulus, the more similar the output of the readout will be. But what do we mean by \"similar\"? The relevant way to measure \"similarity\" in each case will depend on the readout function in question. In the case of linear regression, the relevant similarity metric is the kernel (dot product) similarity of the inputs. To see this, you can inspect the matrix form of the least squares solution for regression below:\n",
    "\n",
    "$$y_{pred} = X_{pred} \\left( X_{train}^T X_{train} \\right)^{-1} X_{train}^T y_{train}$$\n",
    "\n",
    "Note that the predicted outputs for the test stimuli are a weighted combination of the outputs for training stimuli, where the weights depend on the inner products of the training and test stimuli. **Thus, the more similar a training stimulus is to a given test stimulus, the more strongly the output for that training stimulus will contribute to the output for that test stimulus**. To give another example, in the case of a radial basis function readout, the relevant notion of similarity would be the Euclidean distance between the inputs.\n",
    "\n",
    "In this interactive exercise, you will explore how the similarity between training and test stimuli predicts how the network generalizes. The task is to use linear regression on activations from our MNIST network from above in order to predict the \"legibility\" of images of different digit images. You play the role of the human rater providing the outputs for the training data, and will rate the legibility of each individual training image from 1-10 using the provided sliders (on the left), where a 10 is perfectly legible, and a 1 is totally illegible. Based on these provided ratings, a ridge regression is trained under the hood to predict those ratings using the activations from the selected layer of the neural network, and the resulting regression model is then applied to new images, yielding predicted legibility ratings for these images (red numbers at the bottom).\n",
    "\n",
    "The color-coded matrix shows the dot product similarity between activations for the training images (rows) and test images (columns)--yellow means that the two images are highly similar based on the dot product of the activations for those images, and dark blue means the two images are highly dissimilar. The goal of this exercise is to explore how the predicted legibility of the test stimuli is determined by your legibility ratings of the training stimuli, and the similarity of the training and test stimuli.\n",
    "\n",
    "1) Using the matrix, find a training and test image that are highly similar, and play around with the rating of the training image. How much does the predicted legibility rating for the test image change? Try this for a few different pairs.\n",
    "\n",
    "2) Now find a highly dissimilar pair, and play with the rating. How much does the predicted legibility of the test image change? Try this for a few different pairs.\n",
    "\n",
    "3) Using the dropdown menu, you can choose a different layer of the neural network to predict the legibiligy of the images, reflecting different stages of processing in the network. Try a few different layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# from google.colab import output\n",
    "# output.enable_custom_widget_manager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Execute to see the widget!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @markdown Execute to see the widget!\n",
    "%matplotlib widget\n",
    "\n",
    "plt.rcParams['xtick.bottom'] = plt.rcParams['xtick.labelbottom'] = False\n",
    "plt.rcParams['xtick.top'] = plt.rcParams['xtick.labeltop'] = True\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "train_dataset = datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transform)\n",
    "test_dataset = datasets.MNIST('../data', train=False, download=True,\n",
    "                       transform=transform)\n",
    "\n",
    "num_train_samples = 10\n",
    "num_test_samples = 5\n",
    "\n",
    "train_data = torch.stack([train_dataset[i][0] for i in range(num_train_samples)])\n",
    "test_data = torch.stack([test_dataset[i][0] for i in range(num_test_samples)])\n",
    "\n",
    "train_patterns = tl.log_forward_pass(model, train_data)\n",
    "test_patterns = tl.log_forward_pass(model, test_data)\n",
    "\n",
    "out = widgets.Output()\n",
    "with plt.ioff():\n",
    "  with out:\n",
    "    fig, ax = plt.subplots(constrained_layout=True, figsize=(6, 9))\n",
    "fig.first = True\n",
    "\n",
    "def update_and_visualize(layer, rating1, rating2, rating3, rating4, rating5,\n",
    "                         rating6, rating7, rating8, rating9, rating10):\n",
    "    X_train = train_patterns[layer].tensor_contents.flatten(start_dim=1).detach().cpu().numpy()\n",
    "    X_test = test_patterns[layer].tensor_contents.flatten(start_dim=1).detach().cpu().numpy()\n",
    "    Y_train = [rating1, rating2, rating3, rating4, rating5,\n",
    "               rating6, rating7, rating8, rating9, rating10]\n",
    "    Y_train = np.array(Y_train)\n",
    "\n",
    "    # Ridge Regression\n",
    "    with warnings.catch_warnings():\n",
    "      warnings.filterwarnings('ignore')\n",
    "      model = Ridge(alpha=.99)\n",
    "      model.fit(X_train, Y_train)\n",
    "      Y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Dot product matrix visualization\n",
    "    dot_product_matrix = np.dot(X_train, X_test.T)\n",
    "    padded_matrix = np.zeros((dot_product_matrix.shape[0]+1,\n",
    "                              dot_product_matrix.shape[1]+1))\n",
    "    padded_matrix[0:-1, 1:] = dot_product_matrix\n",
    "    dot_product_matrix = padded_matrix\n",
    "\n",
    "    im = ax.imshow(dot_product_matrix, cmap='viridis')\n",
    "    if len(fig.axes) == 1:\n",
    "      plt.colorbar(im, label='Dot Product Similarity', shrink=.8)\n",
    "    im.set_clim([dot_product_matrix.min(), dot_product_matrix.max()])\n",
    "\n",
    "    dummy_image = np.zeros(train_data[0].shape)\n",
    "    dummy_image = dummy_image + float(train_data[0].max())\n",
    "    dummy_image[-1, 0, -1] = 0\n",
    "\n",
    "    # Set up figure if the first run\n",
    "\n",
    "    if fig.first:\n",
    "      ax.set_xticks([])\n",
    "      ax.set_yticks([])\n",
    "      ax.set_ylabel(\"Training Images\", fontsize=15)\n",
    "      ax.xaxis.set_label_position('top')\n",
    "      ax.set_xlabel(\"Test Images\", fontsize=15)\n",
    "      imagebox = OffsetImage(dummy_image.squeeze(), zoom=1.92, cmap='gray')\n",
    "      ab = AnnotationBbox(imagebox, (0, num_train_samples+.05), frameon=False, box_alignment=(0.5, .5))\n",
    "      ax.add_artist(ab)\n",
    "      for i in range(num_train_samples):\n",
    "          imagebox = OffsetImage(train_data[i].squeeze(), zoom=1.9, cmap='gray')\n",
    "          ab = AnnotationBbox(imagebox, (0, i), frameon=False, box_alignment=(.5, 0.5))\n",
    "          ax.add_artist(ab)\n",
    "\n",
    "      for i in range(num_test_samples):\n",
    "          imagebox = OffsetImage(test_data[i].squeeze(), zoom=1.9, cmap='gray')\n",
    "          ab = AnnotationBbox(imagebox, (i+1, num_train_samples), frameon=False, box_alignment=(0.5, .5))\n",
    "          ax.add_artist(ab)\n",
    "      t = ax.text(0, X_train.shape[0], 'X:', va='top', ha='center',\n",
    "                fontsize=15, color='black', backgroundcolor='white', fontweight='bold')\n",
    "      t = ax.text(0, X_train.shape[0]+.65, 'Y:', va='top', ha='center',\n",
    "                fontsize=15, color='black', backgroundcolor='white', fontweight='bold')\n",
    "    fig.first = False\n",
    "\n",
    "\n",
    "    # Annotate with predicted Y_test values\n",
    "\n",
    "    for i in range(num_test_samples):\n",
    "        t = ax.text(i+1, X_train.shape[0]+.65, f'{Y_test_pred[i]:.1f}', va='top', ha='center',\n",
    "                fontsize=15, color='red', backgroundcolor='white', fontweight='bold')\n",
    "\n",
    "\n",
    "w = widgets.interactive(update_and_visualize,\n",
    "         layer = widgets.Dropdown(\n",
    "            options=['conv1', 'conv2', 'dropout1', 'fc1', 'dropout2', 'fc2'],\n",
    "            value='fc2',\n",
    "            description='Layer',\n",
    "            disabled=False,\n",
    "            layout = widgets.Layout(margin='40px 10px 0 0')),\n",
    "         rating1 = widgets.FloatSlider(description='Rating', value=5, min=1, max=10, step=1, layout = widgets.Layout(margin='50px 0 43px 0'), style={'font_weight': 'bold'}),\n",
    "         rating2 = widgets.FloatSlider(description='Rating',value=5, min=1, max=10, step=1, layout = widgets.Layout(margin='0 0 43px 0'), style={'font_weight': 'bold'}),\n",
    "         rating3 = widgets.FloatSlider(description='Rating',value=5, min=1, max=10, step=1, layout = widgets.Layout(margin='0 0 43px 0'), style={'font_weight': 'bold'}),\n",
    "         rating4 = widgets.FloatSlider(description='Rating',value=5, min=1, max=10, step=1, layout = widgets.Layout(margin='0 0 43px 0'), style={'font_weight': 'bold'}),\n",
    "         rating5 = widgets.FloatSlider(description='Rating',value=5, min=1, max=10, step=1, layout = widgets.Layout(margin='0 0 43px 0'), style={'font_weight': 'bold'}),\n",
    "         rating6 = widgets.FloatSlider(description='Rating',value=5, min=1, max=10, step=1, layout = widgets.Layout(margin='0 0 43px 0'), style={'font_weight': 'bold'}),\n",
    "         rating7 = widgets.FloatSlider(description='Rating',value=5, min=1, max=10, step=1, layout = widgets.Layout(margin='0 0 43px 0'), style={'font_weight': 'bold'}),\n",
    "         rating8 = widgets.FloatSlider(description='Rating',value=5, min=1, max=10, step=1, layout = widgets.Layout(margin='0 0 43px 0'), style={'font_weight': 'bold'}),\n",
    "         rating9 = widgets.FloatSlider(description='Rating',value=5, min=1, max=10, step=1, layout = widgets.Layout(margin='0 0 43px 0'), style={'font_weight': 'bold'}),\n",
    "         rating10 = widgets.FloatSlider(description='Rating',value=5, min=1, max=10, step=1, layout = widgets.Layout(margin='0 0 43px 0'), style={'font_weight': 'bold'})\n",
    "         )\n",
    "widgets.HBox([w, fig.canvas], layout=widgets.Layout(width='100%', display='flex', align_items='stretch'))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "gpuType": "T4",
   "include_colab_link": true,
   "name": "W1D3_Tutorial1",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}