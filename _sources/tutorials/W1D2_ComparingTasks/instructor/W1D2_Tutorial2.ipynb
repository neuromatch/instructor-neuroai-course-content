{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dee6d49a-374b-4631-9069-8b16aac31afc",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/neuromatch/NeuroAI_Course/blob/main/tutorials/W1D2_ComparingTasks/student/W1D2_Tutorial2.ipynb\" target=\"_blank\"><img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"/></a> Â  <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/neuromatch/NeuroAI_Course/main/tutorials/W1D2_ComparingTasks/student/W1D2_Tutorial2.ipynb\" target=\"_blank\"><img alt=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6221f0d3-4a18-416b-b548-3d9a5b82dedf",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Tutorial 2: Contrastive learning for object recognition\n",
    "\n",
    "**Week 1, Day 2: Comparing Tasks**\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__ Andrew F. Luo, Leila Wehbe\n",
    "\n",
    "__Content reviewers:__ Samuele Bolotta, Yizhou Chen, RyeongKyung Yoon, Ruiyi Zhang, Lily Chamakura\n",
    "\n",
    "__Production editors:__ Konstantine Tsafatinos, Ella Batty, Spiros Chavlis, Samuele Bolotta, Hlib Solodzhuk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2734dc52-6dc9-4669-a7c7-99ef87e47b8b",
   "metadata": {
    "execution": {}
   },
   "source": [
    "___\n",
    "\n",
    "\n",
    "# Tutorial Objectives\n",
    "\n",
    "*Estimated timing of tutorial: 20 minutes*\n",
    "\n",
    "By the end of this tutorial, participants will be able to:\n",
    "1. Understand why we want to do contrastive learning.\n",
    "2. Understand the losses in contrastive learning.\n",
    "3. Run an example on contrastive learning using MNIST.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceddeef-ea13-4224-8ca8-70563edbcf00",
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @markdown\n",
    "from IPython.display import IFrame\n",
    "from ipywidgets import widgets\n",
    "out = widgets.Output()\n",
    "with out:\n",
    "    print(f\"If you want to download the slides: https://osf.io/download/d4r6g/\")\n",
    "    display(IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/d4r6g/?direct%26mode=render%26action=download%26mode=render\", width=730, height=410))\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273ae64d-bc44-4e0e-b077-487da8391334",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a0ea4d",
   "metadata": {
    "execution": {},
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##  Install and import feedback gadget\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Install and import feedback gadget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eeb07c-5de3-428e-a422-3a58f1124b43",
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Install and import feedback gadget\n",
    "\n",
    "!pip install vibecheck numpy matplotlib torch torchvision tqdm ipysankeywidget ipywidgets --quiet\n",
    "\n",
    "from vibecheck import DatatopsContentReviewContainer\n",
    "def content_review(notebook_section: str):\n",
    "    return DatatopsContentReviewContainer(\n",
    "        \"\",  # No text prompt - leave this as is\n",
    "        notebook_section,\n",
    "        {\n",
    "        \"url\": \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab\",\n",
    "        \"name\": \"sciencematch_sm\", # change the name of the course : neuromatch_dl, climatematch_ct, etc\n",
    "        \"user_key\": \"y1x3mpx5\",\n",
    "        },\n",
    "    ).render()\n",
    "\n",
    "feedback_prefix = \"W1D2_T2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Import dependencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56685b9-3f17-4a55-acca-73dad5623992",
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Import dependencies\n",
    "# @markdown\n",
    "\n",
    "import logging\n",
    "import gc\n",
    "\n",
    "# PyTorch and related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "\n",
    "# Set up PyTorch backend configurations\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# Numpy for numerical operations\n",
    "import numpy as np\n",
    "\n",
    "# Matplotlib for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn for machine learning utilities\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910f242f",
   "metadata": {
    "execution": {},
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Figure settings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Figure settings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebc5798-a8dd-4985-b0ec-d5facf4ee700",
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "# @markdown\n",
    "\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' # perform high definition rendering for images and plots\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01471a7f",
   "metadata": {
    "execution": {},
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##  Plotting functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Plotting functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598cc45f-c4b7-406c-a80b-6adfae387583",
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Plotting functions\n",
    "# @markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808d728a",
   "metadata": {
    "execution": {},
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##  Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Helper functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386b49a0-9409-496a-8656-579ca3e4af5f",
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "#@title Helper functions\n",
    "# @markdown\n",
    "\n",
    "# This is code from the pytorch metric learning package\n",
    "\n",
    "def neg_inf(dtype):\n",
    "    # Returns the smallest possible value for the given data type\n",
    "    return torch.finfo(dtype).min\n",
    "\n",
    "def small_val(dtype):\n",
    "    # Returns the smallest positive value greater than zero for the given data type\n",
    "    return torch.finfo(dtype).tiny\n",
    "\n",
    "def to_dtype(x, tensor=None, dtype=None):\n",
    "    # Converts tensor `x` to the specified `dtype`, or to the same dtype as `tensor`\n",
    "    if not torch.is_autocast_enabled():\n",
    "        dt = dtype if dtype is not None else tensor.dtype\n",
    "        if x.dtype != dt:\n",
    "            x = x.type(dt)\n",
    "    return x\n",
    "\n",
    "def get_matches_and_diffs(labels, ref_labels=None):\n",
    "    # Returns tensors indicating matches and differences between pairs of labels\n",
    "    if ref_labels is None:\n",
    "        ref_labels = labels\n",
    "    labels1 = labels.unsqueeze(1)  # Expand dimensions for comparison\n",
    "    labels2 = ref_labels.unsqueeze(0)  # Expand dimensions for comparison\n",
    "    matches = (labels1 == labels2).byte()  # Byte tensor of matches\n",
    "    diffs = matches ^ 1  # Byte tensor of differences (inverse of matches)\n",
    "    if ref_labels is labels:\n",
    "        matches.fill_diagonal_(0)  # Remove self-matches\n",
    "    return matches, diffs\n",
    "\n",
    "def get_all_pairs_indices(labels, ref_labels=None):\n",
    "    \"\"\"\n",
    "    Given a tensor of labels, this will return 4 tensors.\n",
    "    The first 2 tensors are the indices which form all positive pairs\n",
    "    The second 2 tensors are the indices which form all negative pairs\n",
    "    \"\"\"\n",
    "    matches, diffs = get_matches_and_diffs(labels, ref_labels)\n",
    "    a1_idx, p_idx = torch.where(matches)  # Indices for positive pairs\n",
    "    a2_idx, n_idx = torch.where(diffs)  # Indices for negative pairs\n",
    "    return a1_idx, p_idx, a2_idx, n_idx\n",
    "\n",
    "def cos_sim(input_embeddings):\n",
    "    # Computes cosine similarity matrix for input embeddings\n",
    "    normed_embeddings = torch.nn.functional.normalize(input_embeddings, dim=-1)  # Normalize embeddings\n",
    "    return normed_embeddings @ normed_embeddings.t()  # Cosine similarity matrix\n",
    "\n",
    "def dcl_loss(pos_pairs, neg_pairs, indices_tuple, temperature=0.07):\n",
    "    # This is the modified InfoNCE loss called \"Decoupled Contrastive Learning\" for small batch sizes\n",
    "    # Basically You remove the numerator from the sum to the denominator\n",
    "\n",
    "    a1, p, a2, _ = indices_tuple  # Unpack indices\n",
    "\n",
    "    if len(a1) > 0 and len(a2) > 0:\n",
    "        dtype = neg_pairs.dtype\n",
    "        pos_pairs = pos_pairs.unsqueeze(1) / temperature  # Scale positive pairs by temperature\n",
    "        neg_pairs = neg_pairs / temperature  # Scale negative pairs by temperature\n",
    "        n_per_p = to_dtype(a2.unsqueeze(0) == a1.unsqueeze(1), dtype=dtype)  # Indicator matrix for matching pairs\n",
    "        neg_pairs = neg_pairs * n_per_p  # Zero out non-matching pairs\n",
    "        neg_pairs[n_per_p == 0] = neg_inf(dtype)  # Replace non-matching pairs with negative infinity\n",
    "\n",
    "        # Compute the maximum value for numerical stability\n",
    "        max_val = torch.max(\n",
    "            pos_pairs, torch.max(neg_pairs, dim=1, keepdim=True)[0]\n",
    "        ).detach()\n",
    "        # Compute numerator and denominator for the loss\n",
    "        numerator = torch.exp(pos_pairs - max_val).squeeze(1)\n",
    "        denominator = torch.sum(torch.exp(neg_pairs - max_val), dim=1)\n",
    "        log_exp = torch.log((numerator / denominator) + small_val(dtype))\n",
    "        return -log_exp  # Return the negative log of the exponential\n",
    "    return 0\n",
    "\n",
    "def pair_based_loss(mat, indices_tuple, lossfunc):\n",
    "    # Computes pair-based loss using the provided loss function\n",
    "    a1, p, a2, n = indices_tuple  # Unpack indices\n",
    "    pos_pair, neg_pair = [], []\n",
    "    if len(a1) > 0:\n",
    "        pos_pair = mat[a1, p]  # Extract positive pairs\n",
    "    if len(a2) > 0:\n",
    "        neg_pair = mat[a2, n]  # Extract negative pairs\n",
    "    return lossfunc(pos_pair, neg_pair, indices_tuple)  # Apply loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pIBjtdIDeeVg",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 1: Building the model\n",
    "\n",
    "### What is contrastive learning?\n",
    "\n",
    "Contrastive learning is often referred to as \"self-supervised learning (SSL)\" and has historically been known as \"metric learning.\" The essence of contrastive/metric learning is that instead of outputting a classification one-hot/softmax vector, or a regression value, you directly output a high-dimensional embedding.\n",
    "\n",
    "Here is an example: given multiple data points from a single class (for example, three photos of you from different viewpoints) and different classes (for example, 10 photos from one or multiple people who are not you), you want the three embeddings from your photos to be closer to each other while being farther away from the ten embeddings from the different classes.\n",
    "\n",
    "Hence the name \"metric learning,\" where you seek to learn a metric/distance that fits the constraints of the data.\n",
    "\n",
    "### Why contrastive learning?\n",
    "\n",
    "It may not be immediately obvious why you would want to engage in contrastive or metric learning. Can't you just use a large 1000-class ImageNet-trained classifier to recognize every image? However, metric learning proves useful when the number of classes is not known ahead of time. For example, if you wanted a network to recognize human faces, there are approximately 7 billion people on this planet, making it impractical to train a classification network with 7 billion output neurons. Instead, you can train a network to output a high-dimensional embedding for each image. With this approach, given a reference image of a person, your network can determine if a new photo is similar to or different from the reference image.\n",
    "\n",
    "### Analysis of the results\n",
    "\n",
    "As we move forward, we'll employ PCA (Principal Component Analysis) and t-SNE (t-Distributed Stochastic Neighbor Embedding) as our primary tools for visualizing data. These techniques are instrumental in reducing the dimensionality of the data, allowing us to observe patterns and relationships that are otherwise difficult to discern in high-dimensional spaces. By visualizing data in this way, we can gain insightful perspectives that are crucial for understanding complex datasets.\n",
    "\n",
    "### Mini residual block\n",
    "\n",
    "Our initial focus will be on creating a mini_residual block. This block adopts a modern approach to the residual design, featuring a prenormalization step as suggested by Kaiming He. We will also incorporate the LeakyReLU activation function. LeakyReLU is particularly favored in generative adversarial networks (GANs) due to its ability to maintain non-zero gradients, which helps in the training process by avoiding the vanishing gradient problem. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c432bf-1de5-43f6-b887-087d17e9ec0f",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "class mini_residual(nn.Module):\n",
    "    # Follows \"Identity Mappings in Deep Residual Networks\", uses LayerNorm instead of BatchNorm, and LeakyReLU instead of ReLU\n",
    "    def __init__(self, feat_in=128, feat_out=128, feat_hidden=256, use_norm=True):\n",
    "        super().__init__()\n",
    "        # Define the residual block with or without normalization\n",
    "        if use_norm:\n",
    "            self.block = nn.Sequential(\n",
    "                nn.LayerNorm(feat_in),  # Layer normalization on input features\n",
    "                nn.LeakyReLU(negative_slope=0.1),  # LeakyReLU activation\n",
    "                nn.Linear(feat_in, feat_hidden),  # Linear layer transforming input to hidden features\n",
    "                nn.LayerNorm(feat_hidden),  # Layer normalization on hidden features\n",
    "                nn.LeakyReLU(negative_slope=0.1),  # LeakyReLU activation\n",
    "                nn.Linear(feat_hidden, feat_out)  # Linear layer transforming hidden to output features\n",
    "            )\n",
    "        else:\n",
    "            self.block = nn.Sequential(\n",
    "                nn.LeakyReLU(negative_slope=0.1),  # LeakyReLU activation\n",
    "                nn.Linear(feat_in, feat_hidden),  # Linear layer transforming input to hidden features\n",
    "                nn.LeakyReLU(negative_slope=0.1),  # LeakyReLU activation\n",
    "                nn.Linear(feat_hidden, feat_out)  # Linear layer transforming hidden to output features\n",
    "            )\n",
    "\n",
    "        # Define the bypass connection\n",
    "        if feat_in != feat_out:\n",
    "            self.bypass = nn.Linear(feat_in, feat_out)  # Linear layer to match dimensions if they differ\n",
    "        else:\n",
    "            self.bypass = nn.Identity()  # Identity layer if input and output dimensions are the same\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        # Forward pass: apply the block and add the bypass connection\n",
    "        return self.block(input_data) + self.bypass(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1725ea-5d80-4cb8-8b9b-b39d69f21dcf",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Full model construction\n",
    "\n",
    "Following the mini_residual block, we will construct the full model. This model will consist of a series of residual blocks stacked together. In PyTorch, the components of a model are organized in a sequence using nn.Sequential, which executes the blocks from the first to the last. This sequential arrangement simplifies the process of defining forward pass operations, ensuring that data flows through the blocks in the intended order. By stacking these blocks, the model can learn complex patterns from the data, enhancing its predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdc7833-aafd-4ee0-83fd-af9922dd5857",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_dim, num_blocks=4):\n",
    "        super().__init__()\n",
    "        # Initial linear projection from input dimension to hidden dimension\n",
    "        self.in_proj = nn.Linear(in_dim, hidden_dim)\n",
    "        # Sequence of residual blocks\n",
    "        self.hidden = nn.Sequential(\n",
    "            *[mini_residual(feat_in=hidden_dim, feat_out=hidden_dim, feat_hidden=hidden_dim) for i in range(num_blocks)]\n",
    "        )\n",
    "        # Output linear projection from hidden dimension to output dimension\n",
    "        self.out = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass: input projection, passing through residual blocks, and final output projection\n",
    "        in_proj_out = self.in_proj(x)\n",
    "        hidden_out = self.hidden(in_proj_out)\n",
    "        return self.out(hidden_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "C29eZ1BkfuIr",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now, let's move on to defining the loss function for our model using an approach derived from the PyTorch metric learning package for better clarity. We will implement a variant of the InfoNCE loss function, which is widely recognized as one of the most effective contrastive or metric learning losses. It has been prominently used in various models, including OpenAI's CLIP, due to its ability to enhance feature discrimination by contrasting positive pairs against negative pairs.\n",
    "\n",
    "To clarify, positive pairs refer to two data points that should be close together in embedding space. For example, two photos of you in different lighting conditions. On the other hand, negative pairs refer to two data points that should be far apart in embedding space, such as a photo of you versus a photo of a dog (assuming you are not a dog). Note that positive pairs and negative pairs do not have to be images. For instance, a picture and its corresponding text could also form a positive pair. Recent work has also explored defining positive pairs using an older version of the encoder, as seen in Googleâs Momentum Contrast (MoCo) or EMA Contrastive methods.\n",
    "\n",
    "InfoNCE is one of the most common contrastive losses. It is essentially a cross-entropy loss used for classifying the correct positive pair from a pool of pairs. Variants like MIL-NCE allow for multiple positive pairs. This loss typically requires substantial batch sizesâcommonly 128 or largerâto perform optimally. The need for large batch sizes stems from the necessity for diverse negative samples in the batch to effectively learn the contrasts. However, large batch sizes can be impractical in resource-constrained settings or when data availability is limited.\n",
    "\n",
    "To address this, we will implement a modified version of InfoNCE as described in the [\"Decoupled Contrastive Learning\"] (https://link.springer.com/chapter/10.1007/978-3-031-19809-0_38) paper. This variant adapts the loss to be more suitable for smaller batch sizes by modifying the denominator of the InfoNCE formula. Specifically, it removes the positive example from the denominator, which reduces the computational demand and stabilizes training when fewer examples are available. This adjustment not only makes the loss function more flexible but also maintains robustness in learning discriminative features even with smaller batch sizes.\n",
    "\n",
    "Here is what the default InfoNCE loss looks like. Note that prior to the dot product, the vectors are normalized to unit norm.\n",
    "\n",
    "$$ \\mathcal{L}_q = -\\log \\left( \\frac{\\exp(q \\cdot k_{+} / \\tau)}{\\sum_{i=0}^{K} \\exp(q \\cdot k_i / \\tau)} \\right) $$\n",
    "\n",
    "Remember, the goal is to minimize the loss function. The numerator $ {\\exp(q \\cdot k_{+} / \\tau)} $ represents the similarity between the query and the positive key. By maximizing this term, the model learns to bring the positive pairs closer together in the embedding space. The denominator $ {\\sum_{i=0}^{K} \\exp(q \\cdot k_i / \\tau)} $ includes the similarities of the query with all other pairs (positive and negative). By normalizing with this sum, the model is encouraged to push the positive pairs closer together while pushing the negative pairs further apart. Essentially, we want the similarity of positive pairs to be higher relative to the similarity of all pairs. The Decoupled Contrastive Learning (DCL) loss modifies this slightly by removing the positive pair from the denominator, as detailed in their paper.\n",
    "\n",
    "Now, we will create the PyTorch dataset object. This object defines how data is loaded from disk for each batch and what transformations are applied. It is important to note that you are not limited to using torchvision transforms; it is quite common to write custom transformation code within the dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72d9231-35a5-405e-b2f6-135e134e0ae9",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Define the transformations for the MNIST dataset\n",
    "mnist_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),  # Convert images to tensor\n",
    "    torchvision.transforms.Normalize((0.1307,), (0.3081,))  # Normalize the images with mean and standard deviation\n",
    "])\n",
    "\n",
    "# Load the MNIST test dataset with the defined transformations\n",
    "test_dset = torchvision.datasets.MNIST(\"./\", train=False, transform=mnist_transforms, download=True)\n",
    "\n",
    "# Calculate the height and width of the MNIST images (28x28)\n",
    "height = int(784**0.5)\n",
    "width = height\n",
    "\n",
    "# Select the first image from the test dataset\n",
    "idx = 0\n",
    "data_point = test_dset[idx]\n",
    "\n",
    "# Display the image using matplotlib\n",
    "plt.imshow(data_point[0][0].numpy(), cmap='gray')  # Display the image in grayscale\n",
    "plt.show()\n",
    "\n",
    "# Print the label of the selected image\n",
    "print(data_point[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CkhXcVrGhUM5",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now we will create the model using the definition we wrote previously and move it to the desired device. It is important to note that in PyTorch, calling .to(device) on a module (such as a neural network model) acts on the module itself, meaning it is an in-place operation. However, when calling this function on a tensor directly, it is not an in-place operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd3ab89-ffd9-408c-8396-25b88e0930fb",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Initialize the model with specified input, output, and hidden dimensions\n",
    "mynet = Model(in_dim=784, out_dim=128, hidden_dim=256)\n",
    "\n",
    "# Automatically select the device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Output the device that will be used\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move the model to the selected device\n",
    "_ = mynet.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pi1Seq52ho57",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Let's create a test DataLoader and examine the representations produced by the untrained network for each number. We will compute the cosine similarity for each handwritten character within the same class, setting the diagonal to np.nan to avoid self-comparison.\n",
    "\n",
    "Additionally, we will compute the cosine similarity for each handwritten character across different classes.\n",
    "\n",
    "Remember to call network.eval() before evaluating the network. This is an in-place operation that instructs PyTorch to freeze certain buffers (such as those in batch normalization) and disable dropout.\n",
    "\n",
    "We will use torch.inference_mode() to disable gradient computation and speed up the testing process. However, if this causes issues, you can replace it with torch.no_grad(). Note that inference_mode does not automatically enable eval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f9122b-ce83-464f-b6c9-5af2feceac62",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# First try with untrained network, find the cosine similarities within a class and across classes\n",
    "\n",
    "# Create a DataLoader for the test dataset with a batch size of 50\n",
    "test_loader = DataLoader(test_dset, batch_size=50, shuffle=False)  # enable persistent_workers=True if more than 1 worker to save CPU\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "mynet.eval()\n",
    "\n",
    "# Initialize lists to store test embeddings and labels\n",
    "test_embeddings = []\n",
    "test_labels = []\n",
    "\n",
    "# Initialize a similarity matrix of size 10x10 for 10 classes\n",
    "sim_matrix = np.zeros((10, 10))\n",
    "\n",
    "# Disable gradient computation for inference\n",
    "with torch.inference_mode():\n",
    "    for data_batch in test_loader:\n",
    "        test_img, test_label = data_batch  # Get images and labels from the batch\n",
    "        batch_size = test_img.shape[0]  # Get the batch size\n",
    "        flat = test_img.reshape(batch_size, -1).to(device, non_blocking=True)  # Flatten the images and move to device\n",
    "        pred_embeddings = mynet(flat).cpu().numpy().tolist()  # Get embeddings from the model and move to CPU\n",
    "        test_embeddings.extend(pred_embeddings)  # Store the embeddings\n",
    "        test_labels.extend(test_label.numpy().tolist())  # Store the labels\n",
    "\n",
    "# Convert embeddings and labels to numpy arrays\n",
    "test_embeddings = np.array(test_embeddings)\n",
    "\n",
    "# Normalize the embeddings\n",
    "test_embeddings_normed = test_embeddings / np.linalg.norm(test_embeddings, axis=1, keepdims=True)\n",
    "\n",
    "# Convert test labels to numpy array\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GtbiyBRmi-e2",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Visualizing the cosine similarity of embeddings within the same class and across different classes before training\n",
    "\n",
    "Ideally, you should observe a very high cosine similarity for images within the same class (along the diagonal) and very low cosine similarity for images from different classes (off-diagonal).\n",
    "\n",
    "However, since our network is untrained, you will notice that there isn't much difference in the cosine similarities. This lack of clear structure in the similarity matrix is expected at this stage because the network has not yet learned to distinguish between different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0JcR1VwliL1f",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Dictionary to store normalized embeddings for each class\n",
    "embeddings = {}\n",
    "for i in range(10):\n",
    "    embeddings[i] = test_embeddings_normed[test_labels == i]\n",
    "\n",
    "# Within class cosine similarity:\n",
    "for i in range(10):\n",
    "    sims = embeddings[i] @ embeddings[i].T  # Compute cosine similarity matrix within the class\n",
    "    np.fill_diagonal(sims, np.nan)  # Ignore diagonal values (self-similarity)\n",
    "    cur_sim = np.nanmean(sims)  # Calculate the mean similarity excluding diagonal\n",
    "    sim_matrix[i, i] = cur_sim  # Store the within-class similarity in the matrix\n",
    "\n",
    "    print(\"Within class {} cosine similarity\".format(i, cur_sim))\n",
    "\n",
    "print(\"==================\")\n",
    "\n",
    "# Between class cosine similarity:\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        if i == j:\n",
    "            continue  # Skip if same class (already computed)\n",
    "        elif i > j:\n",
    "            continue  # Skip if already computed (matrix symmetry)\n",
    "        else:\n",
    "            sims = embeddings[i] @ embeddings[j].T  # Compute cosine similarity between different classes\n",
    "            cur_sim = np.mean(sims)  # Calculate the mean similarity\n",
    "            sim_matrix[i, j] = cur_sim  # Store the similarity in the matrix\n",
    "            sim_matrix[j, i] = cur_sim  # Ensure symmetry in the matrix\n",
    "            print(\"{} and {} cosine similarity {}\".format(i, j, cur_sim))\n",
    "\n",
    "# Plotting the similarity matrix\n",
    "plt.imshow(sim_matrix, vmin=0.0, vmax=1.0)\n",
    "plt.title(\"Untrained Network Cosine Similarity Matrix\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebbe1da-d23c-4367-94d3-5f5d68fd0356",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2: Training the model and visualizing feature similarity \n",
    "\n",
    "Now we will train the network!\n",
    "\n",
    "Notice how we decay the learning rate so that the final learning rate will be half of the initial learning rate. We will use the AdamW optimizer, which is the Adam optimizer with decoupled weight decay. A learning rate of 3e-4 and a weight decay of 1e-2 are typical settings for AdamW.\n",
    "\n",
    "It is important to note that weight decay in AdamW and SGD works differently in PyTorch implementations. In PyTorch, the AdamW weight decay is further scaled by the learning rate (real weight decay = weight decay * lr), but in SGD, it is not scaled by the learning rate. Therefore, in AdamW, it is common to use higher weight decay values than in SGD.\n",
    "\n",
    "Additionally, remember to call mynet.train() before starting the training process. This sets mynet to training mode, enabling the buffers and dropout layers (if they are present in the network architecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b62ecc-c0c4-4b2e-8d93-ab4ea9a952d0",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Number of epochs for training\n",
    "epochs = 10\n",
    "\n",
    "# Automatically select the device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Output the device that will be used\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the MNIST training dataset with the defined transformations\n",
    "train_dset = torchvision.datasets.MNIST(\"./\", train=True, transform=mnist_transforms)\n",
    "train_loader = DataLoader(train_dset, batch_size=50, shuffle=True)  # Enable persistent_workers=True if more than 1 worker to save CPU\n",
    "\n",
    "# Cleanup: delete the optimizer and free up memory if this block is re-run\n",
    "try:\n",
    "    del optimizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Cleanup: delete the network and free up memory if this block is re-run\n",
    "try:\n",
    "    del mynet\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Initialize the model with specified input, output, and hidden dimensions\n",
    "mynet = Model(in_dim=784, out_dim=128, hidden_dim=256)\n",
    "_ = mynet.to(device)  # Move the model to the selected device\n",
    "\n",
    "# Enable training mode, which may affect dropout and other layers\n",
    "mynet.train(mode=True)\n",
    "print(\"Is the network in training mode?\", mynet.training)\n",
    "\n",
    "# Initial learning rate and decay factor for the optimizer\n",
    "init_lr = 3e-4\n",
    "lr_decay_factor = 0.5\n",
    "\n",
    "# Initialize the optimizer with model parameters and learning rate\n",
    "optimizer = torch.optim.AdamW(mynet.parameters(), lr=init_lr, weight_decay=1e-2)\n",
    "\n",
    "# Tracker to keep track of loss values during training\n",
    "loss_tracker = []\n",
    "\n",
    "# Training loop over the specified number of epochs\n",
    "for epoch_id in range(1, epochs+1):\n",
    "    loss_epoch_tracker = 0\n",
    "    batch_counter = 0\n",
    "\n",
    "    # Adjust learning rate for the current epoch\n",
    "    new_lrate = init_lr * (lr_decay_factor ** (epoch_id / epochs))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = new_lrate\n",
    "\n",
    "    batches_in_epoch = len(train_loader)\n",
    "    for data_batch in train_loader:\n",
    "        optimizer.zero_grad()  # Zero out gradients\n",
    "\n",
    "        # Get images and labels from the batch\n",
    "        train_img, train_label = data_batch\n",
    "        batch_size = train_img.shape[0]\n",
    "\n",
    "        # Flatten images and move data to the selected device\n",
    "        flat = train_img.reshape(batch_size, -1).to(device, non_blocking=True)\n",
    "        train_label = train_label.to(device, non_blocking=True)\n",
    "\n",
    "        # Forward pass through the network\n",
    "        predicted_results = mynet(flat)\n",
    "\n",
    "        # Compute cosine similarity matrix for the batch\n",
    "        similarities = cos_sim(predicted_results)\n",
    "\n",
    "        # Get pairs of indices for positive and negative pairs\n",
    "        label_pos_neg = get_all_pairs_indices(train_label)\n",
    "\n",
    "        # Compute the loss using the decoupled contrastive learning loss function\n",
    "        final_loss = torch.mean(pair_based_loss(similarities, label_pos_neg, dcl_loss))\n",
    "\n",
    "        # Compute gradients from the loss\n",
    "        final_loss.backward()\n",
    "\n",
    "        # Update the model parameters using the optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "        # Convert the loss to a single CPU scalar\n",
    "        loss_cpu_number = final_loss.item()\n",
    "\n",
    "        # Keep track of the losses for visualization\n",
    "        loss_epoch_tracker += loss_cpu_number\n",
    "        batch_counter += 1\n",
    "\n",
    "        # Print the current epoch, batch number, and loss every 500 batches\n",
    "        if batch_counter % 500 == 0:\n",
    "            print(\"Epoch {}, Batch {}/{}, loss: {}\".format(epoch_id, batch_counter, batches_in_epoch, loss_cpu_number))\n",
    "\n",
    "    # Print the average loss for the epoch\n",
    "    print(\"Epoch average loss {}\".format(loss_epoch_tracker / batch_counter))\n",
    "\n",
    "# Set the model to test mode (optional, not used here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uvQ4MdHKjY1t",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Let us now extract the features from the trained network!\n",
    "\n",
    "Again, please make it a habit to set the network into eval mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4f11e3-07e8-4104-9ae4-6a9fd6f037eb",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# DataLoader for the test dataset with a batch size of 50\n",
    "test_loader = DataLoader(test_dset, batch_size=50, shuffle=False)  # Enable persistent_workers=True if more than 1 worker to save CPU\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "mynet.eval()\n",
    "\n",
    "# Initialize lists to store test embeddings and labels\n",
    "test_embeddings = []\n",
    "test_labels = []\n",
    "\n",
    "# Disable gradient computation for inference\n",
    "with torch.inference_mode():\n",
    "    for data_batch in test_loader:\n",
    "        test_img, test_label = data_batch  # Get images and labels from the batch\n",
    "        batch_size = test_img.shape[0]  # Get the batch size\n",
    "        flat = test_img.reshape(batch_size, -1).to(device, non_blocking=True)  # Flatten images and move to device\n",
    "        pred_embeddings = mynet(flat).cpu().numpy().tolist()  # Get embeddings from the model and move to CPU\n",
    "        test_embeddings.extend(pred_embeddings)  # Store the embeddings\n",
    "        test_labels.extend(test_label.numpy().tolist())  # Store the labels\n",
    "\n",
    "# Convert test labels to numpy array for further processing\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "# Indicate that feature extraction is complete\n",
    "print(\"Feature extraction done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4GGxdeiWjkIr",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Since the network was trained using InfoNCE, we will normalize each feature to unit norm. Additionally, PCA expects the features to be centered and standardized to have a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767e0f93-3f68-4db0-9edf-4891a3858f63",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Convert list of embeddings to a numpy array\n",
    "test_embeddings = np.array(test_embeddings)\n",
    "\n",
    "# Normalize the embeddings\n",
    "test_embeddings_normed = test_embeddings / np.linalg.norm(test_embeddings, axis=1, keepdims=True)\n",
    "\n",
    "# Center the normalized embeddings by subtracting the mean\n",
    "test_embeddings_normed = test_embeddings_normed - np.mean(test_embeddings_normed, axis=1, keepdims=True)\n",
    "\n",
    "# Standardize the centered embeddings by dividing by the standard deviation\n",
    "test_embeddings_normed = test_embeddings_normed / np.std(test_embeddings_normed, axis=1, keepdims=True)\n",
    "\n",
    "# Initialize PCA with 2 components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit PCA on the embeddings and transform them to 2D\n",
    "pca_embeddings = pca.fit_transform(test_embeddings)\n",
    "\n",
    "# Optional: Print the shape of the resulting PCA embeddings to verify\n",
    "print(\"PCA embeddings shape:\", pca_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8UXxSAmfjoB8",
   "metadata": {
    "execution": {}
   },
   "source": [
    "For t-SNE, we simply normalize each feature to unit norm due to InfoNCE. We will not perform any additional centering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cdd26a-15c9-4c5f-82f7-4a65ca0bd5f7",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Convert list of embeddings to a numpy array\n",
    "test_embeddings = np.array(test_embeddings)\n",
    "\n",
    "# Normalize the embeddings to unit length by dividing each embedding by its L2 norm\n",
    "test_embeddings_normed = test_embeddings / np.linalg.norm(test_embeddings, axis=1, keepdims=True)\n",
    "\n",
    "# Initialize t-SNE with 2 components for dimensionality reduction\n",
    "tsne = TSNE(n_components=2)\n",
    "\n",
    "# Fit t-SNE on the normalized embeddings and transform them to 2D\n",
    "tsne_embeddings = tsne.fit_transform(test_embeddings)\n",
    "\n",
    "# Notify that the t-SNE transformation may take some time\n",
    "print(\"t-SNE transformation in progress... This may take a minute. Go grab a coffee or something.\")\n",
    "\n",
    "# Optional: Print the shape of the resulting t-SNE embeddings to verify\n",
    "print(\"t-SNE embeddings shape:\", tsne_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8de073f-d514-4156-98e2-d7aac5620ca6",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "test_labels.shape, tsne_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eqJJWmx7j1kF",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Observe the distribution of features for each number! Notice how well-separated the embeddings for different characters are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afe3c0d-7eed-4b2b-bdaa-bb258fc92d10",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Use t-SNE embeddings for visualization\n",
    "my_embeddings = tsne_embeddings\n",
    "# TSNE or PCA? TSNE is nicer to look at.\n",
    "\n",
    "# Plot embeddings for digit '0' in red\n",
    "num = 0\n",
    "plt.scatter(my_embeddings[test_labels==num, 0], my_embeddings[test_labels==num, 1], c=\"red\")\n",
    "\n",
    "# Plot embeddings for digit '1' in green\n",
    "num = 1\n",
    "plt.scatter(my_embeddings[test_labels==num, 0], my_embeddings[test_labels==num, 1], c=\"green\")\n",
    "\n",
    "# Plot embeddings for digit '2' in blue\n",
    "num = 2\n",
    "plt.scatter(my_embeddings[test_labels==num, 0], my_embeddings[test_labels==num, 1], c=\"blue\")\n",
    "\n",
    "# Plot embeddings for digit '3' in orange\n",
    "num = 3\n",
    "plt.scatter(my_embeddings[test_labels==num, 0], my_embeddings[test_labels==num, 1], c=\"orange\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0KXB1wpmMp1",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Visualizing the cosine similarity after training\n",
    "\n",
    "Observe that the diagonal elements are significantly more positive than the off-diagonal elements. This indicates that the similarity within the same class is much stronger than the similarity between different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88a8b40-faa7-4071-b070-0ca192e06d55",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Create DataLoader for the test dataset with a batch size of 50\n",
    "test_loader = DataLoader(test_dset, batch_size=50, shuffle=False)  # Enable persistent_workers=True if more than 1 worker to save CPU\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "mynet.eval()\n",
    "\n",
    "# Initialize lists to store test embeddings and labels\n",
    "test_embeddings = []\n",
    "test_labels = []\n",
    "\n",
    "# Initialize a similarity matrix of size 10x10 for 10 classes\n",
    "sim_matrix = np.zeros((10, 10))\n",
    "\n",
    "# Disable gradient computation for inference\n",
    "with torch.inference_mode():\n",
    "    for data_batch in test_loader:\n",
    "        # Get images and labels from the batch\n",
    "        test_img, test_label = data_batch\n",
    "        batch_size = test_img.shape[0]  # Get the batch size\n",
    "\n",
    "        # Flatten images and move data to the selected device\n",
    "        flat = test_img.reshape(batch_size, -1).to(device, non_blocking=True)\n",
    "\n",
    "        # Get embeddings from the model and move to CPU\n",
    "        pred_embeddings = mynet(flat).cpu().numpy().tolist()\n",
    "\n",
    "        # Store the embeddings and labels\n",
    "        test_embeddings.extend(pred_embeddings)\n",
    "        test_labels.extend(test_label.numpy().tolist())\n",
    "\n",
    "# Convert embeddings and labels to numpy arrays for further processing\n",
    "test_embeddings = np.array(test_embeddings)\n",
    "\n",
    "# Normalize the embeddings to unit length by dividing each embedding by its L2 norm\n",
    "test_embeddings_normed = test_embeddings / np.linalg.norm(test_embeddings, axis=1, keepdims=True)\n",
    "\n",
    "# Convert test labels to a numpy array\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "# Dictionary to store normalized embeddings for each class\n",
    "embeddings = {}\n",
    "for i in range(10):\n",
    "    embeddings[i] = test_embeddings_normed[test_labels == i]\n",
    "\n",
    "# Calculate within-class cosine similarity\n",
    "for i in range(10):\n",
    "    # Compute cosine similarity matrix within the class\n",
    "    sims = embeddings[i] @ embeddings[i].T\n",
    "\n",
    "    # Ignore diagonal values (self-similarity)\n",
    "    np.fill_diagonal(sims, np.nan)\n",
    "\n",
    "    # Calculate the mean similarity excluding diagonal\n",
    "    cur_sim = np.nanmean(sims)\n",
    "\n",
    "    # Store the within-class similarity in the matrix\n",
    "    sim_matrix[i, i] = cur_sim\n",
    "\n",
    "    # Print the within-class cosine similarity\n",
    "    print(\"Within class {} cosine similarity\".format(cur_sim))\n",
    "\n",
    "print(\"==================\")\n",
    "\n",
    "# Calculate between-class cosine similarity\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        if i == j:\n",
    "            pass  # Skip if same class (already computed)\n",
    "        elif i > j:\n",
    "            pass  # Skip if already computed (matrix symmetry)\n",
    "        else:\n",
    "            # Compute cosine similarity between different classes\n",
    "            sims = embeddings[i] @ embeddings[j].T\n",
    "\n",
    "            # Calculate the mean similarity\n",
    "            cur_sim = np.mean(sims)\n",
    "\n",
    "            # Store the similarity in the matrix\n",
    "            sim_matrix[i, j] = cur_sim\n",
    "            sim_matrix[j, i] = cur_sim  # Ensure symmetry in the matrix\n",
    "\n",
    "            # Print the between-class cosine similarity\n",
    "            print(\"{} and {} cosine similarity {}\".format(i, j, cur_sim))\n",
    "\n",
    "# Plot the similarity matrix using matplotlib\n",
    "plt.imshow(sim_matrix, vmin=0.0, vmax=1.0)\n",
    "plt.title(\"Trained Network Cosine Similarity Matrix\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8wRjpsrmva7",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Using the network to identify nearest neighbors in the test set.\n",
    "\n",
    "How do people actually use a contrastive learning network? In Person Re-Identification (Person Re-ID), a network computes embeddings for two images and checks if the cosine or Euclidean similarity between these embeddings exceeds a certain threshold to determine if they depict the same person.\n",
    "\n",
    "In foundation model training, such as with CLIP, the typical approach is to fine-tune the entire network or train a linear probe or small network on the outputs of the last layer.\n",
    "\n",
    "Here, we will follow the Person Re-ID setup to find the most similar image in a test set and determine if they represent the same character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013D0NbXowjj",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Calculate the cosine similarity matrix for all embeddings\n",
    "sims_all = test_embeddings_normed @ test_embeddings_normed.T\n",
    "\n",
    "# Set diagonal elements to a large negative value to avoid self-matching\n",
    "np.fill_diagonal(sims_all, -1000.0)  # Set to a small value so it doesn't give us the same number for argmax\n",
    "\n",
    "# Index of the embedding to check for the most similar embedding\n",
    "idx_to_check = 3029\n",
    "\n",
    "# Find the index of the most similar embedding (excluding itself)\n",
    "best_idx = np.argmax(sims_all[idx_to_check])\n",
    "\n",
    "# Plot the image corresponding to the index to check\n",
    "plt.imshow(test_dset[idx_to_check][0][0].cpu().numpy())\n",
    "plt.show()\n",
    "\n",
    "# Plot the image corresponding to the most similar embedding\n",
    "plt.imshow(test_dset[best_idx][0][0].cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8444cc4a-fd26-471c-ac34-7e515dbe946a",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### How is contrastive learning used in practice?\n",
    "\n",
    "Nearly all vision foundation models, such as DINO, DINOv2, CLIP, and their derivatives (including OpenCLIP and EVA-CLIP), are trained using contrastive losses. DINO and DINOv2 are trained solely on images, while CLIP is trained on a combination of images and text.\n",
    "\n",
    "When only images are used, the contrastive learning loss is applied to augmentations of the same image. These augmentations can include crops, flips, and rotations, and this approach is referred to as a \"pretext task.\" Typically, augmentations of the same image are treated as instances where the embeddings should be the same. For example, a network should recognize a photo of you and a photo of you flipped, with altered brightness, noise added, or converted to black and white, as representing the same person.\n",
    "\n",
    "When images and text are used together, as in CLIP, the training data consists of images and their corresponding captions. For example, the caption \"A photo of a dog\" might be paired with a picture of a blue heeler puppy. These captions are typically scraped from online sources and collected into datasets like LAION-2B, COYO-700M, and CommonCrawl. Although these captions are often of varying quality, the sheer volume of data helps to mitigate this issue.\n",
    "\n",
    "In this case, contrastive learning typically employs a dual encoder systemâone for text and one for images. The network is trained using a loss function that minimizes the distance between the correct text-image pairs while maximizing the distance between incorrect pairs. For example, the caption \"A photo of a dog\" should have embeddings close to the image of the blue heeler puppy and far from the image of a cat. To compute the \"distance\" of the embeddings, methods such as normalized dot-product (cosine similarity), angular distance (Universal Sentence Encoder), Euclidean distance, or squared Euclidean distance are often used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43cc3bb-d225-4594-b2c3-fec226b6f539",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### References:\n",
    "\n",
    "[1] Unsupervised feature learning via non-parametric instance discrimination (2018)\n",
    "\n",
    "[2] Representation learning with contrastive predictive coding (2018)\n",
    "\n",
    "[3] A simple framework for contrastive learning of visual representations (2020)\n",
    "\n",
    "[4] Improved Deep Metric Learning with Multi-class N-pair Loss Objective (2016)\n",
    "\n",
    "[4] Noise-contrastive estimation: A new estimation principle for unnormalize statistical models (2010)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "gpuType": "T4",
   "include_colab_link": true,
   "name": "W1D2_Tutorial2",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}