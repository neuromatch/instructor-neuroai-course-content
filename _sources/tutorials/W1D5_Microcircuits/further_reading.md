# Suggested further readings 

## Tutorial 1: Sparsity and Sparse Coding

- [Effects of monocular deprivation in kittens](http://hubel.med.harvard.edu/papers/HubelWiesel1964NaunynSchmiedebergsArchExpPatholPharmakol.pdf)
- [Emergence of simple-cell receptive field properties by learning a sparse code for natural images](https://www.nature.com/articles/381607a0)
- [Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition](https://www.khoury.northeastern.edu/home/eelhami/courses/EE290A/OMP_Krishnaprasad.pdf)

## Tutorial 2: Normalization


## Tutorial 3: Attention

- [Attention and the detection of signals](https://pubmed.ncbi.nlm.nih.gov/7381367/)
- [Parallel Distributed Processing](https://mitpress.mit.edu/9780262680530/parallel-distributed-processing/)
- [Attention is all you need](https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)
- [Inductive biases and variable creation in self-attention mechanisms](https://proceedings.mlr.press/v162/edelman22a/edelman22a.pdf)
- [Transformer as a Graph Neural Network](https://docs.dgl.ai/en/0.8.x/tutorials/models/4_old_wines/7_transformer.html)
- [Attention? Attention!](https://lilianweng.github.io/posts/2018-06-24-attention/)