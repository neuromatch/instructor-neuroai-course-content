
<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/><meta content="Docutils 0.18.1: http://docutils.sourceforge.net/" name="generator"/>
<title>Tutorial 5: Replay â€” NeuroAI (instructor's version)</title>
<script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
<!-- Loaded before other Sphinx assets -->
<link href="../../../_static/styles/theme.css?digest=796348d33e8b1d947c94" rel="stylesheet"/>
<link href="../../../_static/styles/bootstrap.css?digest=796348d33e8b1d947c94" rel="stylesheet"/>
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=796348d33e8b1d947c94" rel="stylesheet"/>
<link href="../../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=796348d33e8b1d947c94" rel="stylesheet"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" rel="preload" type="font/woff2"/>
<link href="../../../_static/pygments.css" rel="stylesheet" type="text/css">
<link href="../../../_static/styles/sphinx-book-theme.css?digest=4ec06e9971c5264fbd345897d5258098f11cc577" rel="stylesheet" type="text/css">
<link href="../../../_static/togglebutton.css" rel="stylesheet" type="text/css">
<link href="../../../_static/copybutton.css" rel="stylesheet" type="text/css">
<link href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" rel="stylesheet" type="text/css">
<link href="../../../_static/sphinx-thebe.css" rel="stylesheet" type="text/css">
<link href="../../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" rel="stylesheet" type="text/css">
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../../../_static/scripts/bootstrap.js?digest=796348d33e8b1d947c94" rel="preload"/>
<link as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=796348d33e8b1d947c94" rel="preload"/>
<script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
<script src="../../../_static/jquery.js"></script>
<script src="../../../_static/underscore.js"></script>
<script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
<script src="../../../_static/doctools.js"></script>
<script src="../../../_static/clipboard.min.js"></script>
<script src="../../../_static/copybutton.js"></script>
<script src="../../../_static/scripts/sphinx-book-theme.js?digest=8bf782fb4ee92b3d3646425e50f299c4e1fd152d"></script>
<script>let toggleHintShow = 'Click to show';</script>
<script>let toggleHintHide = 'Click to hide';</script>
<script>let toggleOpenOnPrint = 'true';</script>
<script src="../../../_static/togglebutton.js"></script>
<script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
<script src="../../../_static/design-tabs.js"></script>
<script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
<script async="async" src="../../../_static/sphinx-thebe.js"></script>
<script type="application/vnd.jupyter.widget-state+json">{"state": {"672641b80c81403ba588bc4f34bf547b": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "765eb4b996ea42b5a1b481fa388b5e10": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_672641b80c81403ba588bc4f34bf547b", "msg_id": "", "outputs": [{"output_type": "stream", "name": "stdout", "text": "If you want to download the slides: https://osf.io/download/8hgj5/\n"}, {"output_type": "display_data", "metadata": {}, "data": {"text/plain": "<IPython.lib.display.IFrame at 0x7f3944d97bb0>", "text/html": "\n        <iframe\n            width=\"730\"\n            height=\"410\"\n            src=\"https://mfr.ca-1.osf.io/render?url=https://osf.io/8hgj5/?direct%26mode=render%26action=download%26mode=render\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        "}}], "tabbable": null, "tooltip": null}}, "711ff528346f43ec87d1787c3776df51": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "9cec140a88f644079e9fe633941f52f9": {"model_name": "ButtonStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "ButtonStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "StyleView", "button_color": null, "font_family": null, "font_size": null, "font_style": null, "font_variant": null, "font_weight": null, "text_color": null, "text_decoration": null}}, "62f297d9ca7849f7b178bb9afeea5aa8": {"model_name": "ButtonModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "ButtonModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "ButtonView", "button_style": "", "description": "Left", "disabled": false, "icon": "", "layout": "IPY_MODEL_711ff528346f43ec87d1787c3776df51", "style": "IPY_MODEL_9cec140a88f644079e9fe633941f52f9", "tabbable": null, "tooltip": null}}, "8a9de5dfc2ec4de8b7079c1943953fb9": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "033cfff83995421392998a73f8aa0502": {"model_name": "ButtonStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "ButtonStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "StyleView", "button_color": null, "font_family": null, "font_size": null, "font_style": null, "font_variant": null, "font_weight": null, "text_color": null, "text_decoration": null}}, "fe2940c782c44823944b4cb48bd7408b": {"model_name": "ButtonModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "ButtonModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "ButtonView", "button_style": "", "description": "Right", "disabled": false, "icon": "", "layout": "IPY_MODEL_8a9de5dfc2ec4de8b7079c1943953fb9", "style": "IPY_MODEL_033cfff83995421392998a73f8aa0502", "tabbable": null, "tooltip": null}}, "8d3237e788524f96831d7324ad614649": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "639572d26efc474ba2858531adac294e": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_62f297d9ca7849f7b178bb9afeea5aa8", "IPY_MODEL_fe2940c782c44823944b4cb48bd7408b"], "layout": "IPY_MODEL_8d3237e788524f96831d7324ad614649", "tabbable": null, "tooltip": null}}, "77f18a7e3a234651985e7004176a8f95": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "3849a4f547f6437db3e43f27be3ae9b9": {"model_name": "HTMLStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "StyleView", "background": null, "description_width": "", "font_size": null, "text_color": null}}, "1848277e5feb4a679350a3741c802545": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "HTMLView", "description": "", "description_allow_html": false, "layout": "IPY_MODEL_77f18a7e3a234651985e7004176a8f95", "placeholder": "\u200b", "style": "IPY_MODEL_3849a4f547f6437db3e43f27be3ae9b9", "tabbable": null, "tooltip": null, "value": "<h3>Start of the game!</h3>"}}, "076b76984f0e409baee982db73d2109f": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "7b8472c91f5e435ca07b9c5a4577e8fa": {"model_name": "HTMLStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "StyleView", "background": null, "description_width": "", "font_size": null, "text_color": null}}, "529b3acd5c9e41de8ee54c98878ea08c": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "HTMLView", "description": "", "description_allow_html": false, "layout": "IPY_MODEL_076b76984f0e409baee982db73d2109f", "placeholder": "\u200b", "style": "IPY_MODEL_7b8472c91f5e435ca07b9c5a4577e8fa", "tabbable": null, "tooltip": null, "value": "<h3>Total reward: 0</h3>"}}, "d1c4762906754ea9b1347a9f5677880f": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "8122dabd7eb64f478fd5f42dd0a8b8b0": {"model_name": "HTMLStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "StyleView", "background": null, "description_width": "", "font_size": null, "text_color": null}}, "fee16c7ef9d64982bf7625fbbfee2eb0": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "HTMLView", "description": "", "description_allow_html": false, "layout": "IPY_MODEL_d1c4762906754ea9b1347a9f5677880f", "placeholder": "\u200b", "style": "IPY_MODEL_8122dabd7eb64f478fd5f42dd0a8b8b0", "tabbable": null, "tooltip": null, "value": "<h4>Objects:</h4>"}}, "ce1d2651fa5f4eed9962bea647483dcf": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "39eb8f49755b44d4a206a9a28537e79c": {"model_name": "HTMLStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "StyleView", "background": null, "description_width": "", "font_size": null, "text_color": null}}, "b8fb00d37e664cc78ef478eb073cd962": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "HTMLView", "description": "", "description_allow_html": false, "layout": "IPY_MODEL_ce1d2651fa5f4eed9962bea647483dcf", "placeholder": "\u200b", "style": "IPY_MODEL_39eb8f49755b44d4a206a9a28537e79c", "tabbable": null, "tooltip": null, "value": "<h4>Choose Left or Right:</h4>"}}, "a9a5dd096866443591ffe16fb8adc7d5": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "c113dd05b9c34a16a1a10d457ed79d94": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_a9a5dd096866443591ffe16fb8adc7d5", "msg_id": "", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Video available at https://youtube.com/watch?v=d5tkHGwQMIA\n"}, {"output_type": "display_data", "metadata": {}, "data": {"text/plain": "<IPython.lib.display.YouTubeVideo at 0x7f38662823a0>", "text/html": "\n        <iframe\n            width=\"730\"\n            height=\"410\"\n            src=\"https://www.youtube.com/embed/d5tkHGwQMIA?fs=1&rel=0\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        ", "image/jpeg": "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\n"}}], "tabbable": null, "tooltip": null}}, "0ecc7c861845429c92bfa7e7f6a6e11a": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "2ad2662d3e9e4b33a7f4e02d4e062853": {"model_name": "TabModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "2.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "2.0.0", "_model_name": "TabModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "2.0.0", "_view_name": "TabView", "box_style": "", "children": ["IPY_MODEL_c113dd05b9c34a16a1a10d457ed79d94"], "layout": "IPY_MODEL_0ecc7c861845429c92bfa7e7f6a6e11a", "selected_index": 0, "tabbable": null, "titles": ["Youtube"], "tooltip": null}}}, "version_major": 2, "version_minor": 0}</script>
<script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script crossorigin="anonymous" data-jupyter-widgets-cdn="https://cdn.jsdelivr.net/npm/" src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@1.0.6/dist/embed-amd.js"></script>
<script>DOCUMENTATION_OPTIONS.pagename = 'tutorials/W2D4_Macrolearning/instructor/W2D4_Tutorial5';</script>
<link href="../../../_static/ai-logo.png" rel="shortcut icon">
<link href="../../../genindex.html" rel="index" title="Index">
<link href="../../../search.html" rel="search" title="Search"/>
<link href="../further_reading.html" rel="next" title="Suggested further readings"/>
<link href="W2D4_Tutorial4.html" rel="prev" title="Tutorial 4: Biological meta reinforcement learning"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
</link></link></link></link></link></link></link></link></link></head>
<body data-default-mode="" data-offset="180" data-spy="scroll" data-target="#bd-toc-nav">
<a class="skip-link" href="#main-content">Skip to main content</a>
<input class="sidebar-toggle" id="__primary" name="__primary" type="checkbox"/>
<label class="overlay overlay-primary" for="__primary"></label>
<input class="sidebar-toggle" id="__secondary" name="__secondary" type="checkbox"/>
<label class="overlay overlay-secondary" for="__secondary"></label>
<div class="search-button__wrapper">
<div class="search-button__overlay"></div>
<div class="search-button__search-container">
<form action="../../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search this book..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search this book..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
</div>
<nav class="bd-header navbar navbar-expand-lg bd-navbar" id="navbar-main"><div class="bd-header__inner bd-page-width">
<label class="sidebar-toggle primary-toggle" for="__primary">
<span class="fa-solid fa-bars"></span>
</label>
<div id="navbar-start">
<a class="navbar-brand logo" href="../../intro.html">
<img alt="Logo image" class="logo__image only-light" src="../../../_static/ai-logo.png"/>
<img alt="Logo image" class="logo__image only-dark" src="../../../_static/ai-logo.png"/>
</a>
</div>
<div class="col-lg-9 navbar-header-items">
<div class="mr-auto" id="navbar-center">
<div class="navbar-center-item">
<nav class="navbar-nav">
<p aria-label="Site Navigation" aria-level="1" class="sidebar-header-items__title" role="heading">
        Site Navigation
    </p>
<ul class="navbar-nav" id="navbar-main-elements">
<li class="nav-item">
<a class="nav-link nav-internal" href="../../Schedule/schedule_intro.html">
                        Schedule
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../TechnicalHelp/tech_intro.html">
                        Technical Help
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../TechnicalHelp/Links_Policy.html">
                        Quick links and policies
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../W1D1_Generalization/chapter_title.html">
                        Generalization (W1D1)
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../W1D2_ComparingTasks/chapter_title.html">
                        Comparing Tasks (W1D2)
                      </a>
</li>
<div class="nav-item dropdown">
<button aria-expanded="false" aria-haspopup="true" class="btn dropdown-toggle nav-item" data-toggle="dropdown" type="button">
                    More
                </button>
<div class="dropdown-menu">
<li class="nav-item">
<a class="nav-link nav-internal" href="../../W1D3_ComparingArtificialAndBiologicalNetworks/chapter_title.html">
                        Comparing Artificial And Biological Networks (W1D3)
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../W1D5_Microcircuits/chapter_title.html">
                        Microcircuits (W1D5)
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../W2D1_Macrocircuits/chapter_title.html">
                        Macrocircuits (W2D1)
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../W2D2_NeuroSymbolicMethods/chapter_title.html">
                        Neuro Symbolic Methods (W2D2)
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../W2D3_Microlearning/chapter_title.html">
                        Microlearning (W2D3)
                      </a>
</li>
<li class="nav-item current active">
<a class="nav-link nav-internal" href="../chapter_title.html">
                        Macrolearning (W2D4)
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../W2D5_Mysteries/chapter_title.html">
                        Mysteries (W2D5)
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../../projects/README.html">
                        Introduction
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../../projects/docs/project_guidance.html">
                        Daily guide for projects
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../../projects/docs/datasets_overview.html">
                        Project materials
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../../projects/professional_development/README.html">
                        Introduction
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../../projects/professional_development/impact_talks.html">
                        Impact Talks
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../../projects/professional_development/mentorship_program.html">
                        Mentorship Program
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../../projects/professional_development/career_features.html">
                        Career Features
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../../projects/professional_development/career_panels.html">
                        Career Panels
                      </a>
</li>
</div>
</div>
</ul>
</nav>
</div>
</div>
<div id="navbar-end">
<div class="navbar-end-item navbar-persistent--container">
<button aria-label="Search" class="btn btn-sm navbar-btn search-button search-button__button" data-toggle="tooltip" title="Search">
<i class="fa-solid fa-magnifying-glass"></i>
</button>
</div>
<div class="navbar-end-item">
<button aria-label="light/dark" class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" data-toggle="tooltip" title="light/dark">
<span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
<span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
<span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
</button>
</div>
<div class="navbar-end-item">
<ul aria-label="Icon Links" class="navbar-nav" id="navbar-icon-links">
</ul>
</div>
</div>
</div>
<div class="navbar-persistent--mobile">
<button aria-label="Search" class="btn btn-sm navbar-btn search-button search-button__button" data-toggle="tooltip" title="Search">
<i class="fa-solid fa-magnifying-glass"></i>
</button>
</div>
<label class="sidebar-toggle secondary-toggle" for="__secondary">
<span class="fa-solid fa-outdent"></span>
</label>
</div>
</nav>
<div class="bd-container">
<div class="bd-container__inner bd-page-width">
<div class="bd-sidebar-primary bd-sidebar">
<div class="sidebar-header-items sidebar-primary__section">
<div class="sidebar-header-items__center">
<div class="navbar-center-item">
<nav class="navbar-nav">
<p aria-label="Site Navigation" aria-level="1" class="sidebar-header-items__title" role="heading">
        Site Navigation
    </p>
<ul class="navbar-nav" id="navbar-main-elements">
<li class="nav-item">
<a class="nav-link nav-internal" href="../../Schedule/schedule_intro.html">
                        Schedule
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../TechnicalHelp/tech_intro.html">
                        Technical Help
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../TechnicalHelp/Links_Policy.html">
                        Quick links and policies
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../W1D1_Generalization/chapter_title.html">
                        Generalization (W1D1)
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../W1D2_ComparingTasks/chapter_title.html">
                        Comparing Tasks (W1D2)
                      </a>
</li>
<div class="nav-item dropdown">
<button aria-expanded="false" aria-haspopup="true" class="btn dropdown-toggle nav-item" data-toggle="dropdown" type="button">
                    More
                </button>
<div class="dropdown-menu">
<li class="nav-item">
<a class="nav-link nav-internal" href="../../W1D3_ComparingArtificialAndBiologicalNetworks/chapter_title.html">
                        Comparing Artificial And Biological Networks (W1D3)
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../W1D5_Microcircuits/chapter_title.html">
                        Microcircuits (W1D5)
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../W2D1_Macrocircuits/chapter_title.html">
                        Macrocircuits (W2D1)
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../W2D2_NeuroSymbolicMethods/chapter_title.html">
                        Neuro Symbolic Methods (W2D2)
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../W2D3_Microlearning/chapter_title.html">
                        Microlearning (W2D3)
                      </a>
</li>
<li class="nav-item current active">
<a class="nav-link nav-internal" href="../chapter_title.html">
                        Macrolearning (W2D4)
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../W2D5_Mysteries/chapter_title.html">
                        Mysteries (W2D5)
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../../projects/README.html">
                        Introduction
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../../projects/docs/project_guidance.html">
                        Daily guide for projects
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../../projects/docs/datasets_overview.html">
                        Project materials
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../../projects/professional_development/README.html">
                        Introduction
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../../projects/professional_development/impact_talks.html">
                        Impact Talks
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../../projects/professional_development/mentorship_program.html">
                        Mentorship Program
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../../projects/professional_development/career_features.html">
                        Career Features
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../../projects/professional_development/career_panels.html">
                        Career Panels
                      </a>
</li>
</div>
</div>
</ul>
</nav>
</div>
</div>
<div class="sidebar-header-items__end">
<div class="navbar-end-item">
<button aria-label="light/dark" class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" data-toggle="tooltip" title="light/dark">
<span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
<span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
<span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
</button>
</div>
<div class="navbar-end-item">
<ul aria-label="Icon Links" class="navbar-nav" id="navbar-icon-links">
</ul>
</div>
</div>
</div>
<div class="sidebar-start-items sidebar-primary__section">
<div class="sidebar-start-items__item">
<a class="navbar-brand logo" href="../../intro.html">
<img alt="Logo image" class="logo__image only-light" src="../../../_static/ai-logo.png"/>
<img alt="Logo image" class="logo__image only-dark" src="../../../_static/ai-logo.png"/>
</a>
</div>
<div class="sidebar-start-items__item">
<form action="../../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search this book..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search this book..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
<div class="sidebar-start-items__item"><nav aria-label="Main" class="bd-links" id="bd-docs-nav">
<div class="bd-toc-item navbar-nav active">
<ul class="nav bd-sidenav bd-sidenav__home-link">
<li class="toctree-l1">
<a class="reference internal" href="../../intro.html">
                    Introduction
                </a>
</li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../Schedule/schedule_intro.html">Schedule</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Schedule/daily_schedules.html">General schedule</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Schedule/shared_calendars.html">Shared calendars</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Schedule/timezone_widget.html">Timezone widget</a></li>
</ul>
</input></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../TechnicalHelp/tech_intro.html">Technical Help</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../TechnicalHelp/Jupyterbook.html">Using jupyterbook</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../TechnicalHelp/Tutorial_colab.html">Using Google Colab</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../TechnicalHelp/Tutorial_kaggle.html">Using Kaggle</a></li>
</ul>
</input></li>
<li class="toctree-l2"><a class="reference internal" href="../../TechnicalHelp/Discord.html">Using discord</a></li>
</ul>
</input></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../TechnicalHelp/Links_Policy.html">Quick links and policies</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../W1D1_Generalization/chapter_title.html">Generalization (W1D1)</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../W1D1_Generalization/instructor/W1D1_Intro.html">Intro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W1D1_Generalization/instructor/W1D1_Tutorial1.html">Tutorial 1: Generalization in AI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W1D1_Generalization/instructor/W1D1_Tutorial2.html">Tutorial 2: Generalization in Neuroscience</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W1D1_Generalization/instructor/W1D1_Tutorial3.html">Tutorial 3: Generalization in Cognitive Science</a></li>
</ul>
</input></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../W1D2_ComparingTasks/chapter_title.html">Comparing Tasks (W1D2)</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../W1D2_ComparingTasks/instructor/W1D2_Intro.html">Intro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W1D2_ComparingTasks/instructor/W1D2_Tutorial1.html">Tutorial 1: Task definition, application, relations and impacts on generalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W1D2_ComparingTasks/instructor/W1D2_Tutorial2.html">Tutorial 2: Contrastive learning for object recognition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W1D2_ComparingTasks/instructor/W1D2_Tutorial3.html">Tutorial 3: Reinforcement learning across temporal scales</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../W1D3_ComparingArtificialAndBiologicalNetworks/chapter_title.html">Comparing Artificial And Biological Networks (W1D3)</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../W1D3_ComparingArtificialAndBiologicalNetworks/instructor/W1D3_Intro.html">Intro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W1D3_ComparingArtificialAndBiologicalNetworks/instructor/W1D3_Tutorial1.html">Tutorial 1: Generalization and representational geometry</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W1D3_ComparingArtificialAndBiologicalNetworks/instructor/W1D3_Tutorial2.html">Tutorial 2: Computation as transformation of representational geometries</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W1D3_ComparingArtificialAndBiologicalNetworks/instructor/W1D3_Tutorial3.html">Tutorial 3: Representational geometry &amp; noise</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W1D3_ComparingArtificialAndBiologicalNetworks/instructor/W1D3_Tutorial4.html">Tutorial 4: Statistical inference on representational geometries</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Architectures</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../W1D5_Microcircuits/chapter_title.html">Microcircuits (W1D5)</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../W1D5_Microcircuits/instructor/W1D5_Intro.html">Intro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W1D5_Microcircuits/instructor/W1D5_Tutorial1.html">Tutorial 1: Sparsity and Sparse Coding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W1D5_Microcircuits/instructor/W1D5_Tutorial2.html">Tutorial 2: Normalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W1D5_Microcircuits/instructor/W1D5_Tutorial3.html">Tutorial 3: Attention</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../W2D1_Macrocircuits/chapter_title.html">Macrocircuits (W2D1)</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../W2D1_Macrocircuits/instructor/W2D1_Intro.html">Intro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W2D1_Macrocircuits/instructor/W2D1_Tutorial1.html">Tutorial 1: Depth vs Width</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../W2D2_NeuroSymbolicMethods/chapter_title.html">Neuro Symbolic Methods (W2D2)</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../W2D2_NeuroSymbolicMethods/instructor/W2D2_Intro.html">Intro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W2D2_NeuroSymbolicMethods/instructor/W2D2_Tutorial1.html">Tutorial 1: Basic operations of vector symbolic algebra</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W2D2_NeuroSymbolicMethods/instructor/W2D2_Tutorial2.html">Tutorial 2: Learning with structures</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W2D2_NeuroSymbolicMethods/instructor/W2D2_Tutorial3.html">Tutorial 3: Representations in continuous space</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W2D2_NeuroSymbolicMethods/further_reading.html">Suggested further readings</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Learning</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../W2D3_Microlearning/chapter_title.html">Microlearning (W2D3)</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../W2D3_Microlearning/instructor/W2D3_Intro.html">Intro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W2D3_Microlearning/instructor/W2D3_Tutorial1.html">Tutorial 1: Microlearning</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../chapter_title.html">Macrolearning (W2D4)</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="W2D4_Intro.html">Intro</a></li>
<li class="toctree-l2"><a class="reference internal" href="W2D4_Tutorial1.html">Tutorial 1: The problem of changing data distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="W2D4_Tutorial2.html">Tutorial 2: Continual learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="W2D4_Tutorial3.html">Tutorial 3: Meta-learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="W2D4_Tutorial4.html">Tutorial 4: Biological meta reinforcement learning</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Tutorial 5: Replay</a></li>
<li class="toctree-l2"><a class="reference internal" href="../further_reading.html">Suggested further readings</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Mysteries</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../W2D5_Mysteries/chapter_title.html">Mysteries (W2D5)</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../W2D5_Mysteries/instructor/W2D5_Intro.html">Intro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W2D5_Mysteries/instructor/W2D5_Tutorial1.html">Tutorial 1: Consciousness</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W2D5_Mysteries/instructor/W2D5_Tutorial2.html">Tutorial 2: Ethics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W2D5_Mysteries/instructor/W2D5_Outro.html">Outro</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Project Booklet</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../projects/README.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../projects/docs/project_guidance.html">Daily guide for projects</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../projects/docs/datasets_overview.html">Project materials</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../projects/project-notebooks/Macrocircuits.html">Macrocircuits</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../projects/project-notebooks/Microlearning.html">Microlearning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../projects/project-notebooks/ComparingNetworks.html">Comparing Networks</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Professional Development</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../projects/professional_development/README.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../projects/professional_development/impact_talks.html">Impact Talks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../projects/professional_development/mentorship_program.html">Professional developemnt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../projects/professional_development/career_features.html">Career Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../projects/professional_development/career_panels.html">Career Panels</a></li>
</ul>
</div>
</nav>
</div>
</div>
<div class="sidebar-end-items sidebar-primary__section">
<div class="sidebar-end-items__item">
</div>
</div>
<div id="rtd-footer-container"></div>
</div>
<main class="bd-main" id="main-content">
<div class="sbt-scroll-pixel-helper"></div>
<div class="bd-content">
<div class="bd-article-container">
<div class="bd-header-article">
<div class="col py-1 d-flex header-article-main">
<div class="header-article__left">
<label class="sidebar-toggle primary-toggle btn btn-sm" data-placement="right" data-toggle="tooltip" for="__primary" title="Toggle primary sidebar">
<span class="fa-solid fa-bars"></span>
</label>
</div>
<div class="header-article__right">
<div class="dropdown dropdown-launch-buttons">
<button aria-expanded="false" aria-label="Launch interactive content" class="btn dropdown-toggle" data-bs-toggle="dropdown" type="button">
<i class="fas fa-rocket"></i>
</button>
<ul class="dropdown-menu">
</ul>
</div>
<button class="btn btn-sm" data-placement="bottom" data-toggle="tooltip" onclick="toggleFullScreen()" title="Fullscreen mode">
<span class="btn__icon-container">
<i class="fas fa-expand"></i>
</span>
</button>
<div class="dropdown dropdown-repository-buttons">
<button aria-expanded="false" aria-label="Source repositories" class="btn dropdown-toggle" data-bs-toggle="dropdown" type="button">
<i class="fab fa-github"></i>
</button>
<ul class="dropdown-menu">
<li><a class="btn btn-sm dropdown-item" data-placement="left" data-toggle="tooltip" href="https://github.com/neuromatch/instructor-neuroai-course-content" target="_blank" title="Source repository">
<span class="btn__icon-container">
<i class="fab fa-github"></i>
</span>
<span class="btn__text-container">repository</span>
</a>

<li><a class="btn btn-sm dropdown-item" data-placement="left" data-toggle="tooltip" href="https://github.com/neuromatch/instructor-neuroai-course-content/issues/new?title=Issue%20on%20page%20%2Ftutorials/W2D4_Macrolearning/instructor/W2D4_Tutorial5.html&amp;body=Your%20issue%20content%20here." target="_blank" title="Open an issue">
<span class="btn__icon-container">
<i class="fas fa-lightbulb"></i>
</span>
<span class="btn__text-container">open issue</span>
</a>

</li></li></ul>
</div>
<div class="dropdown dropdown-download-buttons">
<button aria-expanded="false" aria-label="Download this page" class="btn dropdown-toggle" data-bs-toggle="dropdown" type="button">
<i class="fas fa-download"></i>
</button>
<ul class="dropdown-menu">
<li><a class="btn btn-sm dropdown-item" data-placement="left" data-toggle="tooltip" href="../../../_sources/tutorials/W2D4_Macrolearning/instructor/W2D4_Tutorial5.ipynb" target="_blank" title="Download source file">
<span class="btn__icon-container">
<i class="fas fa-file"></i>
</span>
<span class="btn__text-container">.ipynb</span>
</a>

<li>
<button class="btn btn-sm dropdown-item" data-placement="left" data-toggle="tooltip" onclick="printPdf(this)" title="Print to PDF">
<span class="btn__icon-container">
<i class="fas fa-file-pdf"></i>
</span>
<span class="btn__text-container">.pdf</span>
</button>

</li></li></ul>
</div>
<label class="sidebar-toggle secondary-toggle btn btn-sm" data-placement="left" data-toggle="tooltip" for="__secondary" title="Toggle secondary sidebar">
<span class="fa-solid fa-list"></span>
</label>
</div>
</div>
</div>
<div class="onlyprint" id="jb-print-docs-body">
<h1>Tutorial 5: Replay</h1>
<!-- Table of contents -->
<div id="print-main-content">
<div id="jb-print-toc">
<div>
<h2> Contents </h2>
</div>
<nav aria-label="Page">
<ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#">
   Tutorial 5: Replay
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#tutorial-objectives">
   Tutorial Objectives
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#setup">
   Setup
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#install-and-import-feedback-gadget">
     Install and import feedback gadget
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#figure-settings">
     Figure settings
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#plotting-functions">
     Plotting functions
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#helper-functions">
     Helper functions
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#data-retrieval">
     Data retrieval
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#set-device-gpu-or-cpu">
     Set device (GPU or CPU).
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#set-random-seed">
     Set random seed
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-0-let-s-play-new-game">
   Section 0: Letâ€™s play new game!
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-1-replay">
     Video 1: Replay
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#submit-your-feedback">
     Submit your feedback
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-changing-environment">
   Section 1: Changing Environment
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-1-colorful-state">
     Coding Exercise 1: Colorful State
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#id1">
       Submit your feedback
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-a2c-agent-in-changing-environment">
   Section 2: A2C Agent in Changing Environment
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#id2">
     Submit your feedback
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-replay-buffer">
   Section 3: Replay Buffer
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-2-experience-again">
     Coding Exercise 2: Experience Again
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#id3">
       Submit your feedback
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#summary">
   Summary
  </a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<article class="bd-article" role="main">
<p><a href="https://colab.research.google.com/github/neuromatch/NeuroAI_Course/blob/main/tutorials/W2D4_Macrolearning/student/W2D4_Tutorial5.ipynb" target="_blank"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg"/></a> Â  <a href="https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/neuromatch/NeuroAI_Course/main/tutorials/W2D4_Macrolearning/student/W2D4_Tutorial5.ipynb" target="_blank"><img alt="Open in Kaggle" src="https://kaggle.com/static/images/open-in-kaggle.svg"/></a></p>
<section class="tex2jax_ignore mathjax_ignore" id="tutorial-5-replay">
<h1>Tutorial 5: Replay<a class="headerlink" href="#tutorial-5-replay" title="Permalink to this heading">#</a></h1>
<p><strong>Week 2, Day 4: Macro-Learning</strong></p>
<p><strong>By Neuromatch Academy</strong></p>
<p><strong>Content creators:</strong> Hlib Solodzhuk, Ximeng Mao, Grace Lindsay</p>
<p><strong>Content reviewers:</strong> Aakash Agrawal, Alish Dipani, Hossein Rezaei, Yousef Ghanbari, Mostafa Abdollahi, Hlib Solodzhuk, Ximeng Mao, Grace Lindsay</p>
<p><strong>Production editors:</strong> Konstantine Tsafatinos, Ella Batty, Spiros Chavlis, Samuele Bolotta, Hlib Solodzhuk</p>
</section>
<hr class="docutils"/>
<section class="tex2jax_ignore mathjax_ignore" id="tutorial-objectives">
<h1>Tutorial Objectives<a class="headerlink" href="#tutorial-objectives" title="Permalink to this heading">#</a></h1>
<p><em>Estimated timing of tutorial: 40 minutes</em></p>
<p>In this tutorial, you will discover what replay is and how it helps with continual learning.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "765eb4b996ea42b5a1b481fa388b5e10"}</script></div>
</div>
</section>
<hr class="docutils"/>
<section class="tex2jax_ignore mathjax_ignore" id="setup">
<h1>Setup<a class="headerlink" href="#setup" title="Permalink to this heading">#</a></h1>
<section id="install-and-import-feedback-gadget">
<h2>Install and import feedback gadget<a class="headerlink" href="#install-and-import-feedback-gadget" title="Permalink to this heading">#</a></h2>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Install and import feedback gadget</span>

<span class="c1"># !pip3 install vibecheck datatops --quiet</span>

<span class="c1"># from vibecheck import DatatopsContentReviewContainer</span>
<span class="c1"># def content_review(notebook_section: str):</span>
<span class="c1">#     return DatatopsContentReviewContainer(</span>
<span class="c1">#         "",  # No text prompt - leave this as is</span>
<span class="c1">#         notebook_section,</span>
<span class="c1">#         {</span>
<span class="c1">#             "url": "https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab",</span>
<span class="c1">#             "name": "sciencematch_sm", # change the name of the course : neuromatch_dl, climatematch_ct, etc</span>
<span class="c1">#             "user_key": "y1x3mpx5",</span>
<span class="c1">#         },</span>
<span class="c1">#     ).render()</span>

<span class="c1"># feedback_prefix = "W2D4_T5"</span>
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Imports</span>

<span class="c1">#working with data</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">random</span>

<span class="c1">#plotting</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">ConfusionMatrixDisplay</span><span class="p">,</span> <span class="n">confusion_matrix</span>

<span class="c1">#interactive display</span>
<span class="kn">import</span> <span class="nn">ipywidgets</span> <span class="k">as</span> <span class="nn">widgets</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span><span class="p">,</span> <span class="n">clear_output</span>
<span class="kn">from</span> <span class="nn">jupyter_ui_poll</span> <span class="kn">import</span> <span class="n">ui_events</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="c1">#modeling</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="figure-settings">
<h2>Figure settings<a class="headerlink" href="#figure-settings" title="Permalink to this heading">#</a></h2>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Figure settings</span>

<span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s1">'matplotlib.font_manager'</span><span class="p">)</span><span class="o">.</span><span class="n">disabled</span> <span class="o">=</span> <span class="kc">True</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = 'retina' # perfrom high definition rendering for images and plots
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/nma.mplstyle"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
</div>
</section>
<section id="plotting-functions">
<h2>Plotting functions<a class="headerlink" href="#plotting-functions" title="Permalink to this heading">#</a></h2>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Plotting functions</span>

<span class="k">def</span> <span class="nf">plot_rewards</span><span class="p">(</span><span class="n">rewards</span><span class="p">,</span> <span class="n">max_rewards</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Plot the rewards over time.</span>

<span class="sd">    Inputs:</span>
<span class="sd">    - rewards (list): list containing the rewards at each time step.</span>
<span class="sd">    - max_rewards(list): list containing the maximum rewards at each time step.</span>
<span class="sd">    """</span>
    <span class="k">with</span> <span class="n">plt</span><span class="o">.</span><span class="n">xkcd</span><span class="p">():</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">rewards</span><span class="p">)),</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">"Obtained Reward"</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">max_rewards</span><span class="p">)),</span> <span class="n">max_rewards</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'*'</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">"Maximum Reward"</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Time Step'</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Reward Value'</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Reward Over Time'</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">rewards</span><span class="p">),</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">plot_confusion_matrix</span><span class="p">(</span><span class="n">rewards</span><span class="p">,</span> <span class="n">max_rewards</span><span class="p">,</span> <span class="n">mode</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Plots the confusion matrix for the chosen rewards and the maximum ones.</span>

<span class="sd">    Inputs:</span>
<span class="sd">    - rewards (list): list containing the rewards at each time step.</span>
<span class="sd">    - max_rewards (list): list containing the maximum rewards at each time step.</span>
<span class="sd">    - mode (int, default = 1): mode of the environment.</span>
<span class="sd">    """</span>
    <span class="k">with</span> <span class="n">plt</span><span class="o">.</span><span class="n">xkcd</span><span class="p">():</span>

      <span class="n">all_colors</span> <span class="o">=</span> <span class="p">[</span><span class="n">color</span> <span class="k">for</span> <span class="n">color</span> <span class="ow">in</span> <span class="n">mode_colors</span><span class="p">[</span><span class="n">mode</span><span class="p">]]</span>

      <span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">max_rewards</span><span class="p">,</span> <span class="n">rewards</span><span class="p">)</span>

      <span class="n">missing_classes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">setdiff1d</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">color_names_rewards</span><span class="p">[</span><span class="n">color_name</span><span class="p">]</span> <span class="k">for</span> <span class="n">color_name</span> <span class="ow">in</span> <span class="n">all_colors</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">max_rewards</span> <span class="o">+</span> <span class="n">rewards</span><span class="p">))</span>
      <span class="k">for</span> <span class="bp">cls</span> <span class="ow">in</span> <span class="n">missing_classes</span><span class="p">:</span>
          <span class="n">cm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="bp">cls</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
          <span class="n">cm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="bp">cls</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

      <span class="n">cm</span> <span class="o">=</span> <span class="n">ConfusionMatrixDisplay</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="o">=</span><span class="n">cm</span><span class="p">,</span> <span class="n">display_labels</span> <span class="o">=</span> <span class="n">all_colors</span><span class="p">)</span>
      <span class="n">cm</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
      <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"Chosen color"</span><span class="p">)</span>
      <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"Maximum-reward color"</span><span class="p">)</span>
      <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
</div>
</section>
<section id="helper-functions">
<h2>Helper functions<a class="headerlink" href="#helper-functions" title="Permalink to this heading">#</a></h2>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Helper functions</span>

<span class="k">def</span> <span class="nf">run_dummy_agent</span><span class="p">(</span><span class="n">env</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Implement dummy agent strategy: chooses random action.</span>

<span class="sd">    Inputs:</span>
<span class="sd">    - env (ChangingEnv): An environment.</span>
<span class="sd">    """</span>
    <span class="n">action</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">max_rewards</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_trials</span><span class="p">)):</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">max_reward</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
        <span class="n">max_rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">max_reward</span><span class="p">)</span>

        <span class="c1">#dummy agent</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mf">0.5</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">action</span> <span class="c1">#change action</span>
    <span class="k">return</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">max_rewards</span>

<span class="n">color_names_rewards</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"red"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s2">"yellow"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="s2">"green"</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
    <span class="s2">"blue"</span><span class="p">:</span> <span class="mi">4</span>
<span class="p">}</span>

<span class="n">color_names_values</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"red"</span><span class="p">:</span> <span class="p">[</span><span class="mi">255</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="s2">"yellow"</span><span class="p">:</span> <span class="p">[</span><span class="mi">255</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="s2">"green"</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="s2">"blue"</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">]</span>
<span class="p">}</span>

<span class="n">first_mode</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"red"</span><span class="p">,</span> <span class="s2">"yellow"</span><span class="p">,</span> <span class="s2">"green"</span><span class="p">]</span>
<span class="n">second_mode</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"red"</span><span class="p">,</span> <span class="s2">"green"</span><span class="p">,</span> <span class="s2">"blue"</span><span class="p">]</span>

<span class="n">mode_colors</span> <span class="o">=</span> <span class="p">{</span>
    <span class="mi">1</span><span class="p">:</span> <span class="n">first_mode</span><span class="p">,</span>
    <span class="mi">2</span><span class="p">:</span> <span class="n">second_mode</span>
<span class="p">}</span>

<span class="k">def</span> <span class="nf">game</span><span class="p">():</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Create interactvie game for this tutorial.</span>
<span class="sd">    """</span>

    <span class="n">total_reward</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">message</span> <span class="o">=</span> <span class="s2">"Start of the game!"</span>

    <span class="n">left_button</span> <span class="o">=</span> <span class="n">widgets</span><span class="o">.</span><span class="n">Button</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">"Left"</span><span class="p">)</span>
    <span class="n">right_button</span> <span class="o">=</span> <span class="n">widgets</span><span class="o">.</span><span class="n">Button</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">"Right"</span><span class="p">)</span>
    <span class="n">button_box</span> <span class="o">=</span> <span class="n">widgets</span><span class="o">.</span><span class="n">HBox</span><span class="p">([</span><span class="n">left_button</span><span class="p">,</span> <span class="n">right_button</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">define_choice</span><span class="p">(</span><span class="n">button</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Change `choice` variable with respect to the pressed button.</span>
<span class="sd">        """</span>
        <span class="k">nonlocal</span> <span class="n">choice</span>
        <span class="n">display</span><span class="p">(</span><span class="n">widgets</span><span class="o">.</span><span class="n">HTML</span><span class="p">(</span><span class="sa">f</span><span class="s2">"&lt;h3&gt;</span><span class="si">{</span><span class="n">button</span><span class="o">.</span><span class="n">description</span><span class="si">}</span><span class="s2">&lt;/h3&gt;"</span><span class="p">))</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">button</span><span class="o">.</span><span class="n">description</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">button</span><span class="o">.</span><span class="n">description</span> <span class="o">==</span> <span class="s2">"Left"</span><span class="p">:</span>
            <span class="n">choice</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">choice</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="n">left_button</span><span class="o">.</span><span class="n">on_click</span><span class="p">(</span><span class="n">define_choice</span><span class="p">)</span>
    <span class="n">right_button</span><span class="o">.</span><span class="n">on_click</span><span class="p">(</span><span class="n">define_choice</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">mode</span> <span class="ow">in</span> <span class="p">[</span><span class="n">first_mode</span><span class="p">,</span> <span class="n">second_mode</span><span class="p">]:</span>
        <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">15</span><span class="p">):</span>
            <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
            <span class="n">first_color</span><span class="p">,</span> <span class="n">second_color</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">replace</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
            <span class="n">clear_output</span><span class="p">()</span>
            <span class="n">display</span><span class="p">(</span><span class="n">widgets</span><span class="o">.</span><span class="n">HTML</span><span class="p">(</span><span class="sa">f</span><span class="s2">"&lt;h3&gt;</span><span class="si">{</span><span class="n">message</span><span class="si">}</span><span class="s2">&lt;/h3&gt;"</span><span class="p">))</span>
            <span class="n">display</span><span class="p">(</span><span class="n">widgets</span><span class="o">.</span><span class="n">HTML</span><span class="p">(</span><span class="sa">f</span><span class="s2">"&lt;h3&gt;Total reward: </span><span class="si">{</span><span class="n">total_reward</span><span class="si">}</span><span class="s2">&lt;/h3&gt;"</span><span class="p">))</span>
            <span class="n">display</span><span class="p">(</span><span class="n">widgets</span><span class="o">.</span><span class="n">HTML</span><span class="p">(</span><span class="sa">f</span><span class="s2">"&lt;h4&gt;Objects:&lt;/h4&gt;"</span><span class="p">))</span>

            <span class="k">with</span> <span class="n">plt</span><span class="o">.</span><span class="n">xkcd</span><span class="p">():</span>

                <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

                <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">Circle</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">first_color</span><span class="p">))</span>
                <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'off'</span><span class="p">)</span>

                <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">Circle</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">second_color</span><span class="p">))</span>
                <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'off'</span><span class="p">)</span>

                <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

            <span class="n">display</span><span class="p">(</span><span class="n">widgets</span><span class="o">.</span><span class="n">HTML</span><span class="p">(</span><span class="s2">"&lt;h4&gt;Choose Left or Right:&lt;/h4&gt;"</span><span class="p">))</span>
            <span class="n">display</span><span class="p">(</span><span class="n">button_box</span><span class="p">)</span>

            <span class="n">choice</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="k">with</span> <span class="n">ui_events</span><span class="p">()</span> <span class="k">as</span> <span class="n">poll</span><span class="p">:</span>
                <span class="k">while</span> <span class="n">choice</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                    <span class="n">poll</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
                    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span> <span class="o">&gt;</span> <span class="mi">60</span><span class="p">:</span>
                        <span class="k">return</span>
            <span class="k">if</span> <span class="n">choice</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">reward</span> <span class="o">=</span> <span class="n">color_names_rewards</span><span class="p">[</span><span class="n">first_color</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">reward</span> <span class="o">=</span> <span class="n">color_names_rewards</span><span class="p">[</span><span class="n">second_color</span><span class="p">]</span>
            <span class="n">total_reward</span> <span class="o">+=</span> <span class="n">reward</span>
            <span class="n">message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"You received a reward of +</span><span class="si">{</span><span class="n">reward</span><span class="si">}</span><span class="s2">."</span>
    <span class="n">clear_output</span><span class="p">()</span>
    <span class="n">display</span><span class="p">(</span><span class="n">widgets</span><span class="o">.</span><span class="n">HTML</span><span class="p">(</span><span class="sa">f</span><span class="s2">"&lt;h3&gt;Your total reward: </span><span class="si">{</span><span class="n">total_reward</span><span class="si">}</span><span class="s2">, congratulations! Do you have any idea what should you do to maximize the reward?&lt;/h3&gt;"</span><span class="p">))</span>
</pre></div>
</div>
</div>
</details>
</div>
</section>
<section id="data-retrieval">
<h2>Data retrieval<a class="headerlink" href="#data-retrieval" title="Permalink to this heading">#</a></h2>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Data retrieval</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">hashlib</span>

<span class="c1"># Variables for file and download URL</span>
<span class="n">fnames</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"FirstModeAgent.pt"</span><span class="p">,</span> <span class="s2">"SecondModeAgent.pt"</span><span class="p">]</span> <span class="c1"># The names of the files to be downloaded</span>
<span class="n">urls</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"https://osf.io/zuxc4/download"</span><span class="p">,</span> <span class="s2">"https://osf.io/j9kht/download"</span><span class="p">]</span> <span class="c1"># URLs from where the files will be downloaded</span>
<span class="n">expected_md5s</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"eca5aa69751dad8ca06742c819f2dc76"</span><span class="p">,</span> <span class="s2">"cdd0338d0b40ade20d6433cd615aaa82"</span><span class="p">]</span> <span class="c1"># MD5 hashes for verifying files integrity</span>

<span class="k">for</span> <span class="n">fname</span><span class="p">,</span> <span class="n">url</span><span class="p">,</span> <span class="n">expected_md5</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">fnames</span><span class="p">,</span> <span class="n">urls</span><span class="p">,</span> <span class="n">expected_md5s</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">fname</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Attempt to download the file</span>
            <span class="n">r</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span> <span class="c1"># Make a GET request to the specified URL</span>
        <span class="k">except</span> <span class="n">requests</span><span class="o">.</span><span class="n">ConnectionError</span><span class="p">:</span>
            <span class="c1"># Handle connection errors during the download</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"!!! Failed to download data !!!"</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># No connection errors, proceed to check the response</span>
            <span class="k">if</span> <span class="n">r</span><span class="o">.</span><span class="n">status_code</span> <span class="o">!=</span> <span class="n">requests</span><span class="o">.</span><span class="n">codes</span><span class="o">.</span><span class="n">ok</span><span class="p">:</span>
                <span class="c1"># Check if the HTTP response status code indicates a successful download</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">"!!! Failed to download data !!!"</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">hashlib</span><span class="o">.</span><span class="n">md5</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">content</span><span class="p">)</span><span class="o">.</span><span class="n">hexdigest</span><span class="p">()</span> <span class="o">!=</span> <span class="n">expected_md5</span><span class="p">:</span>
                <span class="c1"># Verify the integrity of the downloaded file using MD5 checksum</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">"!!! Data download appears corrupted !!!"</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># If download is successful and data is not corrupted, save the file</span>
                <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">fname</span><span class="p">,</span> <span class="s2">"wb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">fid</span><span class="p">:</span>
                    <span class="n">fid</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">content</span><span class="p">)</span> <span class="c1"># Write the downloaded content to a file</span>
</pre></div>
</div>
</div>
</details>
</div>
</section>
<section id="set-device-gpu-or-cpu">
<h2>Set device (GPU or CPU).<a class="headerlink" href="#set-device-gpu-or-cpu" title="Permalink to this heading">#</a></h2>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Set device (GPU or CPU).</span>

<span class="k">def</span> <span class="nf">set_device</span><span class="p">():</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Determines and sets the computational device for PyTorch operations based on the availability of a CUDA-capable GPU.</span>

<span class="sd">    Outputs:</span>
<span class="sd">    - device (str): The device that PyTorch will use for computations ('cuda' or 'cpu'). This string can be directly used</span>
<span class="sd">    in PyTorch operations to specify the device.</span>
<span class="sd">    """</span>

    <span class="n">device</span> <span class="o">=</span> <span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span>
    <span class="k">if</span> <span class="n">device</span> <span class="o">!=</span> <span class="s2">"cuda"</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"GPU is not enabled in this notebook. </span><span class="se">\n</span><span class="s2">"</span>
              <span class="s2">"If you want to enable it, in the menu under `Runtime` -&gt; </span><span class="se">\n</span><span class="s2">"</span>
              <span class="s2">"`Hardware accelerator.` and select `GPU` from the dropdown menu"</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"GPU is enabled in this notebook. </span><span class="se">\n</span><span class="s2">"</span>
              <span class="s2">"If you want to disable it, in the menu under `Runtime` -&gt; </span><span class="se">\n</span><span class="s2">"</span>
              <span class="s2">"`Hardware accelerator.` and select `None` from the dropdown menu"</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">device</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">set_device</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GPU is not enabled in this notebook. 
If you want to enable it, in the menu under `Runtime` -&gt; 
`Hardware accelerator.` and select `GPU` from the dropdown menu
</pre></div>
</div>
</div>
</div>
</section>
<section id="set-random-seed">
<h2>Set random seed<a class="headerlink" href="#set-random-seed" title="Permalink to this heading">#</a></h2>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Set random seed</span>

<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="k">def</span> <span class="nf">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">seed_torch</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">seed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="mi">32</span><span class="p">)</span>
  <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">seed_torch</span><span class="p">:</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed_all</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="kc">True</span>

<span class="n">set_seed</span><span class="p">(</span><span class="n">seed</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
</div>
</section>
</section>
<hr class="docutils"/>
<section class="tex2jax_ignore mathjax_ignore" id="section-0-let-s-play-new-game">
<h1>Section 0: Letâ€™s play new game!<a class="headerlink" href="#section-0-let-s-play-new-game" title="Permalink to this heading">#</a></h1>
<p>As in the previous tutorial, this one is going to be focused on an RL setup, thus, we would like you to play a slightly different game to get an idea of what the agent is going to learn. The rules are the same: you need to pick one of two displayed objects. Please notice any interesting patterns and observations and discuss them in your group before going to the video.</p>
<p>Make sure you execute this cell to play the game!</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Make sure you execute this cell to play the game!</span>

<span class="n">game</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "1848277e5feb4a679350a3741c802545"}</script><script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "529b3acd5c9e41de8ee54c98878ea08c"}</script><script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "fee16c7ef9d64982bf7625fbbfee2eb0"}</script><img alt="../../../_images/3175b306c5563b939324225e7f95dd9dedc264594a7370b9701c35505e454d39.png" src="../../../_images/3175b306c5563b939324225e7f95dd9dedc264594a7370b9701c35505e454d39.png">
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "b8fb00d37e664cc78ef478eb073cd962"}</script><script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "639572d26efc474ba2858531adac294e"}</script></img></div>
</div>
<section id="video-1-replay">
<h2>Video 1: Replay<a class="headerlink" href="#video-1-replay" title="Permalink to this heading">#</a></h2>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "2ad2662d3e9e4b33a7f4e02d4e062853"}</script></div>
</div>
</section>
<section id="submit-your-feedback">
<h2>Submit your feedback<a class="headerlink" href="#submit-your-feedback" title="Permalink to this heading">#</a></h2>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Submit your feedback</span>
<span class="c1"># content_review(f"{feedback_prefix}_replay")</span>
</pre></div>
</div>
</div>
</details>
</div>
</section>
</section>
<hr class="docutils"/>
<section class="tex2jax_ignore mathjax_ignore" id="section-1-changing-environment">
<h1>Section 1: Changing Environment<a class="headerlink" href="#section-1-changing-environment" title="Permalink to this heading">#</a></h1>
<p>As mentioned in the video, to study replay we need to use a slightly different task that is inspired by the Harlow task, but creates an incentive to remember past data. In this section we will introduce this new task environment, which replicates the game you played.</p>
<section id="coding-exercise-1-colorful-state">
<h2>Coding Exercise 1: Colorful State<a class="headerlink" href="#coding-exercise-1-colorful-state" title="Permalink to this heading">#</a></h2>
<p>For this tutorial, each state will be represented by its color (via its RGB values; thus, it is vector of 3 values) and each color is associated with a stable reward that is unchanged over time (the rewards will correspond to the position of the color in the rainbow).</p>
<p>While the reward associated with each color does not change over time, the colors presented to the agent will change. Specifically, on each trial, the agent is presented two colors and should choose the one associated with higher reward. Initially (in â€˜mode 1â€™), colors will be chosen from a set of 3 possible colors. Over time, one of these colors will be replaced by another creating a different set of three possible colors (â€˜mode 2â€™). This constitutes a covariate distribution shift, and may cause the agent to forget the reward associated with the dropped color.</p>
<p>For this exercise, you should complete the missing parts of the enviornment.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">color_names_rewards</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"red"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s2">"yellow"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="s2">"green"</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
    <span class="s2">"blue"</span><span class="p">:</span> <span class="mi">4</span>
<span class="p">}</span>

<span class="n">color_names_values</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"red"</span><span class="p">:</span> <span class="p">[</span><span class="mi">255</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="s2">"yellow"</span><span class="p">:</span> <span class="p">[</span><span class="mi">255</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="s2">"green"</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="s2">"blue"</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">]</span>
<span class="p">}</span>

<span class="n">first_mode</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"red"</span><span class="p">,</span> <span class="s2">"yellow"</span><span class="p">,</span> <span class="s2">"green"</span><span class="p">]</span>
<span class="n">second_mode</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"red"</span><span class="p">,</span> <span class="s2">"green"</span><span class="p">,</span> <span class="s2">"blue"</span><span class="p">]</span>

<span class="n">mode_colors</span> <span class="o">=</span> <span class="p">{</span>
    <span class="mi">1</span><span class="p">:</span> <span class="n">first_mode</span><span class="p">,</span>
    <span class="mi">2</span><span class="p">:</span> <span class="n">second_mode</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ChangingEnv</span><span class="p">():</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Initialize changing environment.</span>

<span class="sd">        Inputs:</span>
<span class="sd">        - mode (int, default = 1): defines mode of the enviornment. Should be only 1 or 2.</span>
<span class="sd">        """</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"Mode is out of allowed range. Please consider entering 1 or 2 as digit."</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">colors</span> <span class="o">=</span> <span class="n">mode_colors</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_state</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">update_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Update state which depends on the mode of the environment."""</span>
        <span class="c1">###################################################################</span>
        <span class="c1">## Fill out the following then remove</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Student exercise: complete state update and choose appropriate feedback."</span><span class="p">)</span>
        <span class="c1">###################################################################</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">first_color</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">second_color</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">replace</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">color_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">first_color</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">second_color</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">color_names_values</span><span class="p">[</span><span class="o">...</span><span class="p">],</span> <span class="n">color_names_values</span><span class="p">[</span><span class="o">...</span><span class="p">]])</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Reset environment by updating its mode (colors to sample from). Set the first state in the given mode."""</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">colors</span> <span class="o">=</span> <span class="n">mode_colors</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_state</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Evaluate agent's perfromance, return reward, max reward (for tracking agent's performance) and next observation."""</span>
        <span class="n">feedback</span> <span class="o">=</span> <span class="n">color_names_rewards</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">color_state</span><span class="p">[</span><span class="o">...</span><span class="p">]]</span>
        <span class="n">max_feedback</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">([</span><span class="o">...</span><span class="p">,</span> <span class="o">...</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_state</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">,</span> <span class="n">feedback</span><span class="p">,</span> <span class="n">max_feedback</span>

</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># to_remove solution</span>

<span class="k">class</span> <span class="nc">ChangingEnv</span><span class="p">():</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Initialize changing environment.</span>

<span class="sd">        Inputs:</span>
<span class="sd">        - mode (int, default = 1): defines mode of the enviornment. Should be only 1 or 2.</span>
<span class="sd">        """</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"Mode is out of allowed range. Please consider entering 1 or 2 as digit."</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">colors</span> <span class="o">=</span> <span class="n">mode_colors</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_state</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">update_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Update state which depends on the mode of the environment."""</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">first_color</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">second_color</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">colors</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">replace</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">color_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">first_color</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">second_color</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">color_names_values</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">first_color</span><span class="p">],</span> <span class="n">color_names_values</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">second_color</span><span class="p">]])</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Reset environment by updating its mode (colors to sample from). Set the first state in the given mode."""</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">colors</span> <span class="o">=</span> <span class="n">mode_colors</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_state</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Evaluate agent's perfromance, return reward, max reward (for tracking agent's performance) and next observation."""</span>
        <span class="n">feedback</span> <span class="o">=</span> <span class="n">color_names_rewards</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">color_state</span><span class="p">[</span><span class="n">action</span><span class="p">]]</span>
        <span class="n">max_feedback</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">([</span><span class="n">color_names_rewards</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">color_state</span><span class="p">[</span><span class="n">action</span><span class="p">]],</span> <span class="n">color_names_rewards</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">color_state</span><span class="p">[</span><span class="mi">1</span> <span class="o">-</span> <span class="n">action</span><span class="p">]]])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_state</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">,</span> <span class="n">feedback</span><span class="p">,</span> <span class="n">max_feedback</span>
</pre></div>
</div>
</div>
</div>
<p>As in previous tutorial, let us test the environment with a dummy agent. For this particular enviornment (in mode 1), we will use a random strategy - just select one of two colors by tossing a fair coin.</p>
<p>Make sure you execute this cell to observe the plot!</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Make sure you execute this cell to observe the plot!</span>

<span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">num_trials</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">ChangingEnv</span><span class="p">()</span>
<span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="n">rewards</span><span class="p">,</span> <span class="n">max_rewards</span> <span class="o">=</span> <span class="n">run_dummy_agent</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>

<span class="n">plot_rewards</span><span class="p">(</span><span class="n">rewards</span><span class="p">,</span> <span class="n">max_rewards</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../../_images/7de94577b223395a7408fd849ff9260a6340b0ada7f20d9ce698c4635c01bdc7.png" src="../../../_images/7de94577b223395a7408fd849ff9260a6340b0ada7f20d9ce698c4635c01bdc7.png">
</img></div>
</div>
<p>Observe that the maximum reward is always higher than obtained reward or coincides with it (when agent luckily chooses more rewarded color).</p>
<section id="id1">
<h3>Submit your feedback<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h3>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Submit your feedback</span>
<span class="c1"># content_review(f"{feedback_prefix}_colorful_state")</span>
</pre></div>
</div>
</div>
</details>
</div>
</section>
</section>
</section>
<hr class="docutils"/>
<section class="tex2jax_ignore mathjax_ignore" id="section-2-a2c-agent-in-changing-environment">
<h1>Section 2: A2C Agent in Changing Environment<a class="headerlink" href="#section-2-a2c-agent-in-changing-environment" title="Permalink to this heading">#</a></h1>
<p><em>Estimated timing to here from start of tutorial: 10 minutes</em></p>
<p><strong>For now, simply run all the following 2 cells (<code class="docutils literal notranslate"><span class="pre">ActorCritic</span></code> class and <code class="docutils literal notranslate"><span class="pre">train_agent</span></code> function) without exploring the content. You can come back to the code if you have time at the end.</strong></p>
<p>Welcome the friend from the previous tutorial, the A2C agent ;) Here we slightly modify the architecture (with LSTM cells being replaced with 1 linear layer with ReLUs on top of it). The variable <code class="docutils literal notranslate"><span class="pre">num_inputs</span></code> is changed too as now input is represented by a 3-dimensional vector and not a single digit. Moreover, we will make training and evaluation functions separate - as we donâ€™t have a â€œtaskâ€ and â€œmeta-space of tasksâ€ notions here, we donâ€™t need to keep track of this.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ActorCritic</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_inputs</span> <span class="o">=</span> <span class="mi">9</span><span class="p">,</span> <span class="n">num_actions</span> <span class="o">=</span> <span class="mi">2</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Initialize Actor-Critic agent."""</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ActorCritic</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1">#num_actions is 2 because left/right hand</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_actions</span> <span class="o">=</span> <span class="n">num_actions</span>

        <span class="c1">#num_inputs is 9 because one-hot encoding of action (2) + reward (1) + previous state (2*3 = 6)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_inputs</span> <span class="o">=</span> <span class="n">num_inputs</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>

        <span class="c1">#hyperparameters involved in training (important to keep assigned to the agent)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.00075</span> <span class="c1">#learning rate for optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">discount_factor</span> <span class="o">=</span> <span class="mf">0.91</span> <span class="c1">#gamma</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state_value_estimate_cost</span> <span class="o">=</span> <span class="mf">0.4</span> <span class="c1">#beta_v</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">entropy_cost</span> <span class="o">=</span> <span class="mf">0.001</span> <span class="c1">#beta_e</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">emb</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_inputs</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">critic_linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">actor_linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_actions</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Implement forward pass through agent."""</span>
        <span class="c1">#at first, input goes through embedding</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">emb</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">emb</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu1</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="o">.</span><span class="n">bias</span><span class="p">))</span>

        <span class="c1">#critic -&gt; value</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">critic_linear</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">critic_linear</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>

        <span class="c1">#actor -&gt; policy</span>
        <span class="n">policy_logits</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">actor_linear</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">actor_linear</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">value</span><span class="p">,</span> <span class="n">policy_logits</span>
</pre></div>
</div>
</div>
</div>
<p>In the cell below we define the training procedure for the A2C agent as well as its evaluation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train_agent</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">optimizer_func</span><span class="p">,</span> <span class="n">mode</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_gradient_steps</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">num_trials</span> <span class="o">=</span> <span class="mi">100</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Training for agent in changing colorful environment.</span>
<span class="sd">    Observe that training happens for one particular mode.</span>

<span class="sd">    Inputs:</span>
<span class="sd">    - env (ChangingEnv): environment.</span>
<span class="sd">    - agent (ActorCritic): particular instance of Actor Critic agent to train.</span>
<span class="sd">    - optimizer_func (torch.Optim): optimizer to use for training.</span>
<span class="sd">    - mode (int, default = 1): mode of the environment.</span>
<span class="sd">    - num_gradient_steps (int, default = 1000): number of gradient steps to perform.</span>
<span class="sd">    - num_trials (int, default = 200): number of times the agent is exposed to the environment per gradient step to be trained.</span>
<span class="sd">    """</span>

    <span class="c1">#reset environment</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">(</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span><span class="p">)</span>

    <span class="c1">#define optimizer</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer_func</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">agent</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_gradient_steps</span><span class="p">):</span>

      <span class="c1">#for storing variables for training</span>
      <span class="n">log_probs</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="n">values</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="n">entropy_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.</span><span class="p">)</span>

      <span class="c1">#start conditions</span>
      <span class="n">preceding_reward</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">])</span>
      <span class="n">preceding_action</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>

      <span class="k">for</span> <span class="n">trial</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_trials</span><span class="p">):</span>
          <span class="c1">#state + reward + one-hot encoding of action; notice that we normalize state before pass to agent!</span>
          <span class="n">full_state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span> <span class="o">/</span> <span class="mi">255</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">preceding_reward</span><span class="p">,</span> <span class="n">preceding_action</span><span class="p">),</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
          <span class="n">value</span><span class="p">,</span> <span class="n">policy_logits</span> <span class="o">=</span> <span class="n">agent</span><span class="p">(</span><span class="n">full_state</span><span class="p">)</span>
          <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

          <span class="c1">#sample action from policy</span>
          <span class="n">dist</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">policy_logits</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
          <span class="n">action</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

          <span class="c1">#perform action to get reward and new state</span>
          <span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

          <span class="c1">#we normalize reward too</span>
          <span class="n">reward</span> <span class="o">/=</span> <span class="mi">4</span>

          <span class="c1">#update preceding variables</span>
          <span class="n">preceding_reward</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="n">reward</span><span class="p">])</span>
          <span class="n">preceding_action</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">action</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
          <span class="n">state</span> <span class="o">=</span> <span class="n">new_state</span>

          <span class="c1">#for training</span>
          <span class="n">log_prob</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
          <span class="n">entropy</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">entropy</span><span class="p">()</span>
          <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
          <span class="n">values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
          <span class="n">log_probs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">log_prob</span><span class="p">)</span>
          <span class="n">entropy_term</span> <span class="o">+=</span> <span class="n">entropy</span>

      <span class="c1">#calculataing loss</span>
      <span class="n">Qval</span> <span class="o">=</span> <span class="mi">0</span>
      <span class="n">Qvals</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">rewards</span><span class="p">))</span>
      <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">rewards</span><span class="p">))):</span>
        <span class="n">Qval</span> <span class="o">=</span> <span class="n">rewards</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="n">agent</span><span class="o">.</span><span class="n">discount_factor</span> <span class="o">*</span> <span class="n">Qval</span>
        <span class="n">Qvals</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">Qval</span>
      <span class="n">values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
      <span class="n">log_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">log_probs</span><span class="p">)</span>
      <span class="n">advantage</span> <span class="o">=</span> <span class="n">Qvals</span> <span class="o">-</span> <span class="n">values</span>
      <span class="n">actor_loss</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">log_probs</span> <span class="o">*</span> <span class="n">advantage</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
      <span class="n">critic_loss</span> <span class="o">=</span> <span class="n">advantage</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
      <span class="n">entropy_term</span> <span class="o">=</span> <span class="n">entropy_term</span> <span class="o">/</span> <span class="n">num_trials</span>

      <span class="c1">#loss incorporates actor/critic terms + entropy</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">actor_loss</span> <span class="o">+</span> <span class="n">agent</span><span class="o">.</span><span class="n">state_value_estimate_cost</span> <span class="o">*</span> <span class="n">critic_loss</span> <span class="o">-</span> <span class="n">agent</span><span class="o">.</span><span class="n">entropy_cost</span> <span class="o">*</span> <span class="n">entropy_term</span>

      <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
      <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
      <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">evaluate_agent</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">mode</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_evaluation_trials</span> <span class="o">=</span> <span class="mi">20</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Evaluation for agent in changing colorful environment.</span>
<span class="sd">    Observe that evaluation happens for one particular mode which can differ from training one.</span>

<span class="sd">    Inputs:</span>
<span class="sd">    - env (ChangingEnv): environment.</span>
<span class="sd">    - agent (ActorCritic): particular instance of Actor Critic agent to train.</span>
<span class="sd">    - mode (int, default = 1): mode of the environment.</span>
<span class="sd">    - num_evaluation_trials (int, default = 20): number of times the agent is exposed to the environment to evaluate it (no training happend during this phase).</span>

<span class="sd">    Outputs:</span>
<span class="sd">    - scores (list): rewards over all trials of evaluation.</span>
<span class="sd">    - max_scores (list): maximum rewards over all trials of evaluation.</span>
<span class="sd">    """</span>
    <span class="c1">#reset environment</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">(</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span><span class="p">)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">max_scores</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1">#start conditions</span>
    <span class="n">preceding_reward</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">preceding_action</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_evaluation_trials</span><span class="p">):</span>

      <span class="c1">#state + reward + one-hot encoding of action; notice that we normalize state before pass to agent!</span>
      <span class="n">full_state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span> <span class="o">/</span> <span class="mi">255</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">preceding_reward</span><span class="p">,</span> <span class="n">preceding_action</span><span class="p">),</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
      <span class="n">value</span><span class="p">,</span> <span class="n">policy_logits</span> <span class="o">=</span> <span class="n">agent</span><span class="p">(</span><span class="n">full_state</span><span class="p">)</span>
      <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

      <span class="c1">#sample action from policy</span>
      <span class="n">dist</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">policy_logits</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
      <span class="n">action</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

      <span class="c1">#perform action to get reward and new state</span>
      <span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">max_reward</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

      <span class="c1">#update preceding variables; we normalize reward too</span>
      <span class="n">preceding_reward</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="n">reward</span> <span class="o">/</span> <span class="mi">4</span><span class="p">])</span>
      <span class="n">preceding_action</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">action</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
      <span class="n">state</span> <span class="o">=</span> <span class="n">new_state</span>

      <span class="c1">#add reward to the scores of agent</span>
      <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
      <span class="n">max_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">max_reward</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">scores</span><span class="p">,</span> <span class="n">max_scores</span>
</pre></div>
</div>
</div>
</div>
<p>In the following code cell, letâ€™s observe the agentâ€™s performance on the first mode after being trained on it. As training of the agent takes around 3 minutes, we have provided you with an already trained version (still feel free to uncomment the training code to receive the same results). You will also have the opportunity to train the agent from scratch in the next section!</p>
<p>Make sure you execute this cell to observe the plot!</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Make sure you execute this cell to observe the plot!</span>

<span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1">#define environment</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">ChangingEnv</span><span class="p">()</span>

<span class="c1">#train agent</span>
<span class="c1">##UNCOMMENT TO TRAIN</span>
<span class="c1"># agent = ActorCritic(hidden_size = 100)</span>
<span class="c1"># optimizer_func = optim.RMSprop</span>
<span class="c1"># train_agent(env, agent, optimizer_func)</span>
<span class="c1">##UNCOMMENT TO TRAIN</span>

<span class="c1">#load agent</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">"FirstModeAgent.pt"</span><span class="p">)</span>

<span class="c1">#evaluate agent</span>
<span class="n">rewards</span><span class="p">,</span> <span class="n">max_rewards</span> <span class="o">=</span> <span class="n">evaluate_agent</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">)</span>
<span class="n">plot_rewards</span><span class="p">(</span><span class="n">rewards</span><span class="p">,</span> <span class="n">max_rewards</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../../_images/7b93091dabc339d06491a7af02c0fa46ded81b46359d459c065fa7792bc36fc7.png" src="../../../_images/7b93091dabc339d06491a7af02c0fa46ded81b46359d459c065fa7792bc36fc7.png"/>
</div>
</div>
<p>Pretty nice! Let us also observe the confusion matrix. Indeed, it might reveal to us the weaknesses being incorporated in the particular colors. We will increase the number of evaluation trials to get the more statistically true results.</p>
<p>Make sure you execute this cell to observe the plot!</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Make sure you execute this cell to observe the plot!</span>
<span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">rewards</span><span class="p">,</span> <span class="n">max_rewards</span> <span class="o">=</span> <span class="n">evaluate_agent</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">num_evaluation_trials</span> <span class="o">=</span> <span class="mi">5000</span><span class="p">)</span>
<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">rewards</span><span class="p">,</span> <span class="n">max_rewards</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../../_images/5a78ed8dc2f506aac14f73399e272947ca0e5e2c368bf10f73c47b227f61f84c.png" src="../../../_images/5a78ed8dc2f506aac14f73399e272947ca0e5e2c368bf10f73c47b227f61f84c.png"/>
</div>
</div>
<p>No specific patterns here; the only thing (which is also expected) is that whenever colors are close in their rewards, the agent makes more mistakes for those.</p>
<p>Notice that the blue color is missing as it is indeed excluded from the first mode. Let us evaluate the agent on the second mode.</p>
<p>Make sure you execute this cell to observe the plot!</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Make sure you execute this cell to observe the plot!</span>
<span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">rewards</span><span class="p">,</span> <span class="n">max_rewards</span> <span class="o">=</span> <span class="n">evaluate_agent</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">mode</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plot_rewards</span><span class="p">(</span><span class="n">rewards</span><span class="p">,</span> <span class="n">max_rewards</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../../_images/bea8f9d29e4e1091c07b0ef8d8001c8ef014cfd9a001553f47c59e46132444f8.png" src="../../../_images/bea8f9d29e4e1091c07b0ef8d8001c8ef014cfd9a001553f47c59e46132444f8.png"/>
</div>
</div>
<p>Letâ€™s check with confusion matrix. We can see that green color is chosen often times when the blue one provides higher reward (which the agent doesnâ€™t know yet).</p>
<p>Make sure you execute this cell to observe the plot!</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Make sure you execute this cell to observe the plot!</span>
<span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">rewards</span><span class="p">,</span> <span class="n">max_rewards</span> <span class="o">=</span> <span class="n">evaluate_agent</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">mode</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">num_evaluation_trials</span> <span class="o">=</span> <span class="mi">5000</span><span class="p">)</span>
<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">rewards</span><span class="p">,</span> <span class="n">max_rewards</span><span class="p">,</span> <span class="n">mode</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../../_images/8f2e796c2c5a3e429ee9096efa02d8507e52abbcdfd5fdce08b2b3b4612c98c5.png" src="../../../_images/8f2e796c2c5a3e429ee9096efa02d8507e52abbcdfd5fdce08b2b3b4612c98c5.png"/>
</div>
</div>
<p>As expected, agent doesnâ€™t know perfectly what to do with new color.</p>
<p>Letâ€™s continue training the same agent now in the second mode and see whether we can improve this situation. Again, you are given a pretrained agent.</p>
<p>Make sure you execute this cell to observe the plot!</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Make sure you execute this cell to observe the plot!</span>
<span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1">##UNCOMMENT TO TRAIN</span>
<span class="c1"># env = ChangingEnv()</span>
<span class="c1"># optimizer_func = optim.RMSprop</span>
<span class="c1"># train_agent(env, agent, optimizer_func, mode = 2)</span>
<span class="c1">##UNCOMMENT TO TRAIN</span>

<span class="c1">#load agent</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">"SecondModeAgent.pt"</span><span class="p">)</span>

<span class="n">rewards</span><span class="p">,</span> <span class="n">max_rewards</span> <span class="o">=</span> <span class="n">evaluate_agent</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">mode</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plot_rewards</span><span class="p">(</span><span class="n">rewards</span><span class="p">,</span> <span class="n">max_rewards</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../../_images/1501d099a7cf1d8a6c059bc7568164cd6bbc22ebe67ea6f639f6629974824ed3.png" src="../../../_images/1501d099a7cf1d8a6c059bc7568164cd6bbc22ebe67ea6f639f6629974824ed3.png"/>
</div>
</div>
<p>Make sure you execute this cell to observe the plot!</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Make sure you execute this cell to observe the plot!</span>
<span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">rewards</span><span class="p">,</span> <span class="n">max_rewards</span> <span class="o">=</span> <span class="n">evaluate_agent</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">mode</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">num_evaluation_trials</span> <span class="o">=</span> <span class="mi">5000</span><span class="p">)</span>
<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">rewards</span><span class="p">,</span> <span class="n">max_rewards</span><span class="p">,</span> <span class="n">mode</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../../_images/47737ffff7e603e6893e8368ef6427e226079baafa01c3e827aa9d019f4d759b.png" src="../../../_images/47737ffff7e603e6893e8368ef6427e226079baafa01c3e827aa9d019f4d759b.png"/>
</div>
</div>
<p>Awesome, agent has improved its ability to perform in the second mode. Still, what about the first one? Did the agent forget the previously seen colors?</p>
<p>Make sure you execute this cell to observe the plot!</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Make sure you execute this cell to observe the plot!</span>
<span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">rewards</span><span class="p">,</span> <span class="n">max_rewards</span> <span class="o">=</span> <span class="n">evaluate_agent</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">mode</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plot_rewards</span><span class="p">(</span><span class="n">rewards</span><span class="p">,</span> <span class="n">max_rewards</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../../_images/e8a852d49ea5a8f42c82af6d14cd609420ff21ce1211838e14129adc644051aa.png" src="../../../_images/e8a852d49ea5a8f42c82af6d14cd609420ff21ce1211838e14129adc644051aa.png"/>
</div>
</div>
<p>Make sure you execute this cell to observe the plot!</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Make sure you execute this cell to observe the plot!</span>
<span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">rewards</span><span class="p">,</span> <span class="n">max_rewards</span> <span class="o">=</span> <span class="n">evaluate_agent</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">mode</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_evaluation_trials</span> <span class="o">=</span> <span class="mi">5000</span><span class="p">)</span>
<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">rewards</span><span class="p">,</span> <span class="n">max_rewards</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../../_images/958dbef9fda0df2a46a8af1c98867f468e82624cf7d95d214a4912673e4479ba.png" src="../../../_images/958dbef9fda0df2a46a8af1c98867f468e82624cf7d95d214a4912673e4479ba.png"/>
</div>
</div>
<p>Oops! Introduction of blue color in the second mode messed up learned relations between red and yellow (we didnâ€™t include yellow in the second mode). What to do? You will explore the bio-inspired mechanism which allows for correcting this behvaiour in the next section!</p>
<section id="id2">
<h2>Submit your feedback<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h2>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Submit your feedback</span>
<span class="c1"># content_review(f"{feedback_prefix}_a2c_agent_in_changing_enviornment")</span>
</pre></div>
</div>
</div>
</details>
</div>
</section>
</section>
<hr class="docutils"/>
<section class="tex2jax_ignore mathjax_ignore" id="section-3-replay-buffer">
<h1>Section 3: Replay Buffer<a class="headerlink" href="#section-3-replay-buffer" title="Permalink to this heading">#</a></h1>
<p><em>Estimated timing to here from start of tutorial: 25 minutes</em></p>
<p>This section discusses the underlying biological reasoning behind the replay buffer as well as proposes its code implementation.</p>
<section id="coding-exercise-2-experience-again">
<h2>Coding Exercise 2: Experience Again<a class="headerlink" href="#coding-exercise-2-experience-again" title="Permalink to this heading">#</a></h2>
<p>A replay buffer is a mechanism which allows an animal to remember certain experiences within an environment, and these can be replayed in its mind at a later time. This can be seen as akin to joint training, as it lets information from a past environment impact current learning.</p>
<p>Each of the gradient steps in the first mode is going to be an â€œexperienceâ€ we are going to save and which we will play artificially (train) during training in the second mode. For that, before going to the coding part, let us take a look at the training function defined earlier â€“ which variables do you think we need to preserve in the proposed auxiliary storage which will allow the agent to implement the replay?</p>
<p>The procedure for the retrieval of the past experience is the following: for each gradient step in the new mode there is going to be one gradient step from a remembered experience from the previous mode.</p>
<p>In this exercise you need to complete <code class="docutils literal notranslate"><span class="pre">ReplayBuffer</span></code> class which will remember information about training experience. Observe that <code class="docutils literal notranslate"><span class="pre">train_agent</span></code> is redefined and slightly modified so it accepts <code class="docutils literal notranslate"><span class="pre">ReplayBuffer</span></code> instance as input.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ReplayBuffer</span><span class="p">():</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_experience</span> <span class="o">=</span> <span class="mi">250</span><span class="p">,</span> <span class="n">num_trials</span> <span class="o">=</span> <span class="mi">100</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Initialize replay buffer.</span>
<span class="sd">        Notice that when replay buffer is full of experience and new one should be remembered, it replaces existing ones, starting</span>
<span class="sd">        from the oldest.</span>

<span class="sd">        Inputs:</span>
<span class="sd">        - max_experience (int, default = 250): the maximum number of experience (gradient steps) which can be stored.</span>
<span class="sd">        - num_trials (int, default = 100): number of times the agent is exposed to the environment per gradient step to be trained.</span>
<span class="sd">        """</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_experience</span> <span class="o">=</span> <span class="n">max_experience</span>

        <span class="c1">#variable which fully describe experience</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">losses</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_experience</span><span class="p">)]</span>

        <span class="c1">#number of memory cell to point to (write or overwrite experience)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">writing_pointer</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reading_pointer</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1">#to keep track how many experience there were</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_experience</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">write_experience</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Write new experience."""</span>
        <span class="c1">###################################################################</span>
        <span class="c1">## Fill out the following then remove</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Student exercise: complete retrieval and storing procedure for replay buffer."</span><span class="p">)</span>
        <span class="c1">###################################################################</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">losses</span><span class="p">[</span><span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="o">...</span>

        <span class="c1">#so that pointer is in range of max_experience and will point to the older experience while full</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">writing_pointer</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">writing_pointer</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_experience</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_experience</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="nf">read_experience</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Read existing experience."""</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">losses</span><span class="p">[</span><span class="o">...</span><span class="p">]</span>

        <span class="c1">#so that pointer is in range of self.max_experience and will point to the older experience while full</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reading_pointer</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reading_pointer</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_experience</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_experience</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>

</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># to_remove solution</span>

<span class="k">class</span> <span class="nc">ReplayBuffer</span><span class="p">():</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_experience</span> <span class="o">=</span> <span class="mi">250</span><span class="p">,</span> <span class="n">num_trials</span> <span class="o">=</span> <span class="mi">100</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Initialize replay buffer.</span>
<span class="sd">        Notice that when replay buffer is full of experience and new one should be remembered, it replaces existing ones, starting</span>
<span class="sd">        from the oldest.</span>

<span class="sd">        Inputs:</span>
<span class="sd">        - max_experience (int, default = 250): the maximum number of experience (gradient steps) which can be stored.</span>
<span class="sd">        - num_trials (int, default = 100): number of times the agent is exposed to the environment per gradient step to be trained.</span>
<span class="sd">        """</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_experience</span> <span class="o">=</span> <span class="n">max_experience</span>

        <span class="c1">#variable which fully describe experience</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">losses</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_experience</span><span class="p">)]</span>

        <span class="c1">#number of memory cell to point to (write or overwrite experience)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">writing_pointer</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reading_pointer</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1">#to keep track how many experience there were</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_experience</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">write_experience</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Write new experience."""</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">losses</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">writing_pointer</span><span class="p">]</span> <span class="o">=</span> <span class="n">loss</span>

        <span class="c1">#so that pointer is in range of max_experience and will point to the older experience while full</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">writing_pointer</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">writing_pointer</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_experience</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_experience</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="nf">read_experience</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Read existing experience."""</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">losses</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">reading_pointer</span><span class="p">]</span>

        <span class="c1">#so that pointer is in range of self.max_experience and will point to the older experience while full</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reading_pointer</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reading_pointer</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_experience</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_experience</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train_agent_with_replay</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">optimizer_func</span><span class="p">,</span> <span class="n">replay</span><span class="p">,</span> <span class="n">mode</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">training_mode</span> <span class="o">=</span> <span class="s2">"write"</span><span class="p">,</span> <span class="n">num_gradient_steps</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">num_trials</span> <span class="o">=</span> <span class="mi">100</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Training for agent in changing colorful environment.</span>
<span class="sd">    Observe that training happens for one particular mode.</span>

<span class="sd">    Inputs:</span>
<span class="sd">    - env (ChangingEnv): environment.</span>
<span class="sd">    - agent (ActorCritic): particular instance of Actor Critic agent to train.</span>
<span class="sd">    - optimizer_func (torch.Optim): optimizer to use for training.</span>
<span class="sd">    - replay (ReplayBuffer): replay buffer which is used during training.</span>
<span class="sd">    - mode (int, default = 1): mode of the environment.</span>
<span class="sd">    - training_mode (str, default = "write"): training mode with replay buffer ("write", "read").</span>
<span class="sd">    - num_gradient_steps (int, default = 1000): number of gradient steps to perform.</span>
<span class="sd">    - num_trials (int, default = 100): number of times the agent is exposed to the environment per gradient step to be trained.</span>
<span class="sd">    """</span>
    <span class="c1">#reset environment</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">(</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span><span class="p">)</span>

    <span class="c1">#define optimizer</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer_func</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">agent</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_gradient_steps</span><span class="p">):</span>

      <span class="c1">#for storing variables for training</span>
      <span class="n">log_probs</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="n">values</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="n">entropy_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.</span><span class="p">)</span>

      <span class="c1">#start conditions</span>
      <span class="n">preceding_reward</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">])</span>
      <span class="n">preceding_action</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>

      <span class="k">for</span> <span class="n">trial</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_trials</span><span class="p">):</span>
          <span class="c1">#state + reward + one-hot encoding of action; notice that we normalize state before pass to agent!</span>
          <span class="n">full_state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span> <span class="o">/</span> <span class="mi">255</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">preceding_reward</span><span class="p">,</span> <span class="n">preceding_action</span><span class="p">),</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
          <span class="n">value</span><span class="p">,</span> <span class="n">policy_logits</span> <span class="o">=</span> <span class="n">agent</span><span class="p">(</span><span class="n">full_state</span><span class="p">)</span>
          <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

          <span class="c1">#sample action from policy</span>
          <span class="n">dist</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">policy_logits</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
          <span class="n">action</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

          <span class="c1">#perform action to get reward and new state</span>
          <span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

          <span class="c1">#we normalize reward too</span>
          <span class="n">reward</span> <span class="o">/=</span> <span class="mi">4</span>

          <span class="c1">#update preceding variables</span>
          <span class="n">preceding_reward</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="n">reward</span><span class="p">])</span>
          <span class="n">preceding_action</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">action</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
          <span class="n">state</span> <span class="o">=</span> <span class="n">new_state</span>

          <span class="c1">#for training</span>
          <span class="n">log_prob</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
          <span class="n">entropy</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">entropy</span><span class="p">()</span>
          <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
          <span class="n">values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
          <span class="n">log_probs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">log_prob</span><span class="p">)</span>
          <span class="n">entropy_term</span> <span class="o">+=</span> <span class="n">entropy</span>

      <span class="c1">#calculataing loss</span>
      <span class="n">Qval</span> <span class="o">=</span> <span class="mi">0</span>
      <span class="n">Qvals</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">rewards</span><span class="p">))</span>
      <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">rewards</span><span class="p">))):</span>
        <span class="n">Qval</span> <span class="o">=</span> <span class="n">rewards</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="n">agent</span><span class="o">.</span><span class="n">discount_factor</span> <span class="o">*</span> <span class="n">Qval</span>
        <span class="n">Qvals</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">Qval</span>
      <span class="n">values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
      <span class="n">log_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">log_probs</span><span class="p">)</span>
      <span class="n">advantage</span> <span class="o">=</span> <span class="n">Qvals</span> <span class="o">-</span> <span class="n">values</span>
      <span class="n">actor_loss</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">log_probs</span> <span class="o">*</span> <span class="n">advantage</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
      <span class="n">critic_loss</span> <span class="o">=</span> <span class="n">advantage</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
      <span class="n">entropy_term</span> <span class="o">=</span> <span class="n">entropy_term</span> <span class="o">/</span> <span class="n">num_trials</span>

      <span class="c1">#loss incorporates actor/critic terms + entropy</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">actor_loss</span> <span class="o">+</span> <span class="n">agent</span><span class="o">.</span><span class="n">state_value_estimate_cost</span> <span class="o">*</span> <span class="n">critic_loss</span> <span class="o">-</span> <span class="n">agent</span><span class="o">.</span><span class="n">entropy_cost</span> <span class="o">*</span> <span class="n">entropy_term</span>

      <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
      <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">retain_graph</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
      <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

      <span class="c1"># write this training example into memory</span>
      <span class="k">if</span> <span class="n">training_mode</span> <span class="o">==</span> <span class="s2">"write"</span><span class="p">:</span>
          <span class="n">replay</span><span class="o">.</span><span class="n">write_experience</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

      <span class="c1">#retrieve previous experience</span>
      <span class="k">if</span> <span class="n">training_mode</span> <span class="o">==</span> <span class="s2">"read"</span><span class="p">:</span>
          <span class="n">replay_loss</span> <span class="o">=</span> <span class="n">replay</span><span class="o">.</span><span class="n">read_experience</span><span class="p">()</span>
          <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
          <span class="n">replay_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">retain_graph</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
          <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>At first, we are going to train the newbie agent in the first mode using the writing mode of the replay buffer. Then, during the training in the second mode, we will incorporate reading from this replay buffer and observe whether it impacts the agentâ€™s performance.</p>
<p>The training time will take around 3 minutes.</p>
<p>Make sure you execute this cell to observe the plot!</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Make sure you execute this cell to observe the plot!</span>

<span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1">#define environment</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">ChangingEnv</span><span class="p">()</span>
<span class="n">replay</span> <span class="o">=</span> <span class="n">ReplayBuffer</span><span class="p">()</span>

<span class="c1">#define agent and optimizer</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">ActorCritic</span><span class="p">(</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">optimizer_func</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">RMSprop</span>

<span class="c1">#train agent</span>
<span class="n">train_agent_with_replay</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">optimizer_func</span><span class="p">,</span> <span class="n">replay</span><span class="p">)</span>

<span class="n">rewards</span><span class="p">,</span> <span class="n">max_rewards</span> <span class="o">=</span> <span class="n">evaluate_agent</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">num_evaluation_trials</span> <span class="o">=</span> <span class="mi">5000</span><span class="p">)</span>
<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">rewards</span><span class="p">,</span> <span class="n">max_rewards</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../../_images/894035d035bd3953adcc9bdd33051cbdefd81ea50bccfa89a4998ff3c73aee3c.png" src="../../../_images/894035d035bd3953adcc9bdd33051cbdefd81ea50bccfa89a4998ff3c73aee3c.png"/>
</div>
</div>
<p>Great! Weâ€™ve trained agent in the first mode and saved the experience in replay buffer. Now, let us change the mode to â€œreadâ€ and train the agent in the second mode while replaying saved experience with each gradient step of the new one. The observed plot is the confusion matrix for the second mode.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">train_agent_with_replay</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">optimizer_func</span><span class="p">,</span> <span class="n">replay</span><span class="p">,</span> <span class="n">mode</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">training_mode</span> <span class="o">=</span> <span class="s2">"read"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Make sure you execute this cell to observe the plot!</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Make sure you execute this cell to observe the plot!</span>
<span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">rewards</span><span class="p">,</span> <span class="n">max_rewards</span> <span class="o">=</span> <span class="n">evaluate_agent</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">mode</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">num_evaluation_trials</span> <span class="o">=</span> <span class="mi">5000</span><span class="p">)</span>
<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">rewards</span><span class="p">,</span> <span class="n">max_rewards</span><span class="p">,</span> <span class="n">mode</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../../_images/136834b1bbe0679d13d5f88067552668e5a92cdf09e94b7839b308bc5683da3d.png" src="../../../_images/136834b1bbe0679d13d5f88067552668e5a92cdf09e94b7839b308bc5683da3d.png"/>
</div>
</div>
<p>Make sure you execute this cell to observe the plot!</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Make sure you execute this cell to observe the plot!</span>

<span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">rewards</span><span class="p">,</span> <span class="n">max_rewards</span> <span class="o">=</span> <span class="n">evaluate_agent</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">num_evaluation_trials</span> <span class="o">=</span> <span class="mi">5000</span><span class="p">)</span>
<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">rewards</span><span class="p">,</span> <span class="n">max_rewards</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../../_images/e7bd01f2f99e7965ddbfb5f76fceeda2682ea07db4262a8ccabc8f66d3b0c191.png" src="../../../_images/e7bd01f2f99e7965ddbfb5f76fceeda2682ea07db4262a8ccabc8f66d3b0c191.png"/>
</div>
</div>
<p>Perfect match!</p>
<section id="id3">
<h3>Submit your feedback<a class="headerlink" href="#id3" title="Permalink to this heading">#</a></h3>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Submit your feedback</span>
<span class="c1"># content_review(f"{feedback_prefix}_experience_again")</span>
</pre></div>
</div>
</div>
</details>
</div>
</section>
</section>
</section>
<hr class="docutils"/>
<section class="tex2jax_ignore mathjax_ignore" id="summary">
<h1>Summary<a class="headerlink" href="#summary" title="Permalink to this heading">#</a></h1>
<p><em>Estimated timing of tutorial: 40 minutes</em></p>
<p>Here we have learned:</p>
<ul class="simple">
<li><p>Reinforcement learning also suffers from forgetting after learning on a new distribution.</p></li>
<li><p>Replay is a biologically-inspired way to learn from memories of past actions and rewards, thus preventing forgetting.</p></li>
</ul>
</section>
<script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./tutorials/W2D4_Macrolearning/instructor"
        },
        predefinedOutput: true
    }
    </script>
<script>kernelName = 'python3'</script>
</article>
<footer class="bd-footer-article">
<!-- Previous / next buttons -->
<div class="prev-next-area">
<a class="left-prev" href="W2D4_Tutorial4.html" id="prev-link" title="previous page">
<i class="fa-solid fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title">Tutorial 4: Biological meta reinforcement learning</p>
</div>
</a>
<a class="right-next" href="../further_reading.html" id="next-link" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title">Suggested further readings</p>
</div>
<i class="fa-solid fa-angle-right"></i>
</a>
</div>
</footer>
</div>
<div class="bd-sidebar-secondary bd-toc">
<div class="toc-item">
<div class="tocsection onthispage">
<i class="fa-solid fa-list"></i> On this page
</div>
<nav class="page-toc" id="bd-toc-nav">
<ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#">
   Tutorial 5: Replay
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#tutorial-objectives">
   Tutorial Objectives
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#setup">
   Setup
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#install-and-import-feedback-gadget">
     Install and import feedback gadget
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#figure-settings">
     Figure settings
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#plotting-functions">
     Plotting functions
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#helper-functions">
     Helper functions
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#data-retrieval">
     Data retrieval
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#set-device-gpu-or-cpu">
     Set device (GPU or CPU).
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#set-random-seed">
     Set random seed
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-0-let-s-play-new-game">
   Section 0: Letâ€™s play new game!
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-1-replay">
     Video 1: Replay
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#submit-your-feedback">
     Submit your feedback
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-changing-environment">
   Section 1: Changing Environment
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-1-colorful-state">
     Coding Exercise 1: Colorful State
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#id1">
       Submit your feedback
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-a2c-agent-in-changing-environment">
   Section 2: A2C Agent in Changing Environment
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#id2">
     Submit your feedback
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-replay-buffer">
   Section 3: Replay Buffer
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-2-experience-again">
     Coding Exercise 2: Experience Again
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#id3">
       Submit your feedback
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#summary">
   Summary
  </a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<footer class="bd-footer-content">
<div class="bd-footer-content__inner">
<div class="bd-footer-content__inner container">
<div class="footer-item">
<p class="component-author">
By Neuromatch
</p>
</div>
<div class="footer-item">
</div>
<div class="footer-item">
<p class="last-updated">
Last updated on None.<br/>
</p>
</div>
<div class="footer-item">
<div class="extra_footer">
<div>
<a href="http://creativecommons.org/licenses/by/4.0/"><img src="https://i.creativecommons.org/l/by/4.0/88x31.png"/></a>
<a href="https://opensource.org/licenses/BSD-3-Clause"><img src="https://camo.githubusercontent.com/9b9ea65d95c9ef878afa1987df65731d47681336/68747470733a2f2f696d672e736869656c64732e696f2f707970692f6c2f736561626f726e2e737667"/></a>
The contents of this repository are shared under the <a href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.
Software elements are additionally licensed under the <a href="https://opensource.org/licenses/BSD-3-Clause">BSD (3-Clause) License</a>.
</div>
</div>
</div>
</div>
</div>
</footer>
</main>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="../../../_static/scripts/bootstrap.js?digest=796348d33e8b1d947c94"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=796348d33e8b1d947c94"></script>
</body>
</html>