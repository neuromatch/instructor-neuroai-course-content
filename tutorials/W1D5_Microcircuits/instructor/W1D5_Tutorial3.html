
<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/><meta content="Docutils 0.18.1: http://docutils.sourceforge.net/" name="generator"/>
<title>Tutorial 3: Attention — NeuroAI (instructor's version)</title>
<script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
<!-- Loaded before other Sphinx assets -->
<link href="../../../_static/styles/theme.css?digest=796348d33e8b1d947c94" rel="stylesheet"/>
<link href="../../../_static/styles/bootstrap.css?digest=796348d33e8b1d947c94" rel="stylesheet"/>
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=796348d33e8b1d947c94" rel="stylesheet"/>
<link href="../../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=796348d33e8b1d947c94" rel="stylesheet"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" rel="preload" type="font/woff2"/>
<link href="../../../_static/pygments.css" rel="stylesheet" type="text/css">
<link href="../../../_static/styles/sphinx-book-theme.css?digest=4ec06e9971c5264fbd345897d5258098f11cc577" rel="stylesheet" type="text/css">
<link href="../../../_static/togglebutton.css" rel="stylesheet" type="text/css">
<link href="../../../_static/copybutton.css" rel="stylesheet" type="text/css">
<link href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" rel="stylesheet" type="text/css">
<link href="../../../_static/sphinx-thebe.css" rel="stylesheet" type="text/css">
<link href="../../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" rel="stylesheet" type="text/css">
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../../../_static/scripts/bootstrap.js?digest=796348d33e8b1d947c94" rel="preload"/>
<link as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=796348d33e8b1d947c94" rel="preload"/>
<script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
<script src="../../../_static/jquery.js"></script>
<script src="../../../_static/underscore.js"></script>
<script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
<script src="../../../_static/doctools.js"></script>
<script src="../../../_static/clipboard.min.js"></script>
<script src="../../../_static/copybutton.js"></script>
<script src="../../../_static/scripts/sphinx-book-theme.js?digest=8bf782fb4ee92b3d3646425e50f299c4e1fd152d"></script>
<script>let toggleHintShow = 'Click to show';</script>
<script>let toggleHintHide = 'Click to hide';</script>
<script>let toggleOpenOnPrint = 'true';</script>
<script src="../../../_static/togglebutton.js"></script>
<script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
<script src="../../../_static/design-tabs.js"></script>
<script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
<script async="async" src="../../../_static/sphinx-thebe.js"></script>
<script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
<script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script type="application/vnd.jupyter.widget-state+json">{"state": {"8b46c87be66446be9c702dfae45af04e": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "2.0.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "2.0.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "2.0.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border_bottom": null, "border_left": null, "border_right": null, "border_top": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "7c1967b3de84461eba851f5a154de808": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_8b46c87be66446be9c702dfae45af04e", "msg_id": "", "outputs": [{"output_type": "stream", "name": "stdout", "text": "If you want to download the slides: https://osf.io/download/cp57z/\n"}, {"output_type": "display_data", "metadata": {}, "data": {"text/plain": "<IPython.lib.display.IFrame at 0x7fcca84140a0>", "text/html": "\n        <iframe\n            width=\"730\"\n            height=\"410\"\n            src=\"https://mfr.ca-1.osf.io/render?url=https://osf.io/cp57z/?direct%26mode=render%26action=download%26mode=render\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        "}}], "tabbable": null, "tooltip": null}}}, "version_major": 2, "version_minor": 0}</script>
<script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script crossorigin="anonymous" data-jupyter-widgets-cdn="https://cdn.jsdelivr.net/npm/" src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@1.0.6/dist/embed-amd.js"></script>
<script>DOCUMENTATION_OPTIONS.pagename = 'tutorials/W1D5_Microcircuits/instructor/W1D5_Tutorial3';</script>
<link href="../../../_static/ai-logo.png" rel="shortcut icon">
<link href="../../../genindex.html" rel="index" title="Index">
<link href="../../../search.html" rel="search" title="Search"/>
<link href="../../W2D1_Macrocircuits/chapter_title.html" rel="next" title="Macrocircuits"/>
<link href="W1D5_Tutorial2.html" rel="prev" title="Tutorial 2: Normalization"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
</link></link></link></link></link></link></link></link></link></head>
<body data-default-mode="" data-offset="180" data-spy="scroll" data-target="#bd-toc-nav">
<a class="skip-link" href="#main-content">Skip to main content</a>
<input class="sidebar-toggle" id="__primary" name="__primary" type="checkbox"/>
<label class="overlay overlay-primary" for="__primary"></label>
<input class="sidebar-toggle" id="__secondary" name="__secondary" type="checkbox"/>
<label class="overlay overlay-secondary" for="__secondary"></label>
<div class="search-button__wrapper">
<div class="search-button__overlay"></div>
<div class="search-button__search-container">
<form action="../../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search this book..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search this book..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
</div>
<nav class="bd-header navbar navbar-expand-lg bd-navbar" id="navbar-main"><div class="bd-header__inner bd-page-width">
<label class="sidebar-toggle primary-toggle" for="__primary">
<span class="fa-solid fa-bars"></span>
</label>
<div id="navbar-start">
<a class="navbar-brand logo" href="../../intro.html">
<img alt="Logo image" class="logo__image only-light" src="../../../_static/ai-logo.png"/>
<img alt="Logo image" class="logo__image only-dark" src="../../../_static/ai-logo.png"/>
</a>
</div>
<div class="col-lg-9 navbar-header-items">
<div class="mr-auto" id="navbar-center">
<div class="navbar-center-item">
<nav class="navbar-nav">
<p aria-label="Site Navigation" aria-level="1" class="sidebar-header-items__title" role="heading">
        Site Navigation
    </p>
<ul class="navbar-nav" id="navbar-main-elements">
<li class="nav-item">
<a class="nav-link nav-internal" href="../../Schedule/schedule_intro.html">
                        Schedule
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../TechnicalHelp/tech_intro.html">
                        Technical Help
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../TechnicalHelp/Links_Policy.html">
                        Quick links and policies
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../W1D1_Generalization/chapter_title.html">
                        Generalization (W1D1)
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../W1D2_ComparingTasks/chapter_title.html">
                        Comparing Tasks (W1D2)
                      </a>
</li>
<div class="nav-item dropdown">
<button aria-expanded="false" aria-haspopup="true" class="btn dropdown-toggle nav-item" data-toggle="dropdown" type="button">
                    More
                </button>
<div class="dropdown-menu">
<li class="nav-item">
<a class="nav-link nav-internal" href="../../W1D3_ComparingArtificialAndBiologicalNetworks/chapter_title.html">
                        Comparing Artificial And Biological Networks (W1D3)
                      </a>
</li>
<li class="nav-item current active">
<a class="nav-link nav-internal" href="../chapter_title.html">
                        Microcircuits (W1D5)
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../W2D1_Macrocircuits/chapter_title.html">
                        Macrocircuits (W2D1)
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../W2D2_NeuroSymbolicMethods/chapter_title.html">
                        Neuro Symbolic Methods (W2D2)
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../W2D3_Microlearning/chapter_title.html">
                        Microlearning (W2D3)
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../W2D4_Macrolearning/chapter_title.html">
                        Macrolearning (W2D4)
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../W2D5_Mysteries/chapter_title.html">
                        Mysteries (W2D5)
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../../projects/README.html">
                        Introduction
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../../projects/docs/project_guidance.html">
                        Daily guide for projects
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../../projects/docs/datasets_overview.html">
                        Project materials
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../../projects/professional_development/README.html">
                        Introduction
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../../projects/professional_development/impact_talks.html">
                        Impact Talks
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../../projects/professional_development/mentorship_program.html">
                        Mentorship Program
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../../projects/professional_development/career_features.html">
                        Career Features
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../../projects/professional_development/career_panels.html">
                        Career Panels
                      </a>
</li>
</div>
</div>
</ul>
</nav>
</div>
</div>
<div id="navbar-end">
<div class="navbar-end-item navbar-persistent--container">
<button aria-label="Search" class="btn btn-sm navbar-btn search-button search-button__button" data-toggle="tooltip" title="Search">
<i class="fa-solid fa-magnifying-glass"></i>
</button>
</div>
<div class="navbar-end-item">
<button aria-label="light/dark" class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" data-toggle="tooltip" title="light/dark">
<span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
<span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
<span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
</button>
</div>
<div class="navbar-end-item">
<ul aria-label="Icon Links" class="navbar-nav" id="navbar-icon-links">
</ul>
</div>
</div>
</div>
<div class="navbar-persistent--mobile">
<button aria-label="Search" class="btn btn-sm navbar-btn search-button search-button__button" data-toggle="tooltip" title="Search">
<i class="fa-solid fa-magnifying-glass"></i>
</button>
</div>
<label class="sidebar-toggle secondary-toggle" for="__secondary">
<span class="fa-solid fa-outdent"></span>
</label>
</div>
</nav>
<div class="bd-container">
<div class="bd-container__inner bd-page-width">
<div class="bd-sidebar-primary bd-sidebar">
<div class="sidebar-header-items sidebar-primary__section">
<div class="sidebar-header-items__center">
<div class="navbar-center-item">
<nav class="navbar-nav">
<p aria-label="Site Navigation" aria-level="1" class="sidebar-header-items__title" role="heading">
        Site Navigation
    </p>
<ul class="navbar-nav" id="navbar-main-elements">
<li class="nav-item">
<a class="nav-link nav-internal" href="../../Schedule/schedule_intro.html">
                        Schedule
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../TechnicalHelp/tech_intro.html">
                        Technical Help
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../TechnicalHelp/Links_Policy.html">
                        Quick links and policies
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../W1D1_Generalization/chapter_title.html">
                        Generalization (W1D1)
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../W1D2_ComparingTasks/chapter_title.html">
                        Comparing Tasks (W1D2)
                      </a>
</li>
<div class="nav-item dropdown">
<button aria-expanded="false" aria-haspopup="true" class="btn dropdown-toggle nav-item" data-toggle="dropdown" type="button">
                    More
                </button>
<div class="dropdown-menu">
<li class="nav-item">
<a class="nav-link nav-internal" href="../../W1D3_ComparingArtificialAndBiologicalNetworks/chapter_title.html">
                        Comparing Artificial And Biological Networks (W1D3)
                      </a>
</li>
<li class="nav-item current active">
<a class="nav-link nav-internal" href="../chapter_title.html">
                        Microcircuits (W1D5)
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../W2D1_Macrocircuits/chapter_title.html">
                        Macrocircuits (W2D1)
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../W2D2_NeuroSymbolicMethods/chapter_title.html">
                        Neuro Symbolic Methods (W2D2)
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../W2D3_Microlearning/chapter_title.html">
                        Microlearning (W2D3)
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../W2D4_Macrolearning/chapter_title.html">
                        Macrolearning (W2D4)
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../W2D5_Mysteries/chapter_title.html">
                        Mysteries (W2D5)
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../../projects/README.html">
                        Introduction
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../../projects/docs/project_guidance.html">
                        Daily guide for projects
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../../projects/docs/datasets_overview.html">
                        Project materials
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../../projects/professional_development/README.html">
                        Introduction
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../../projects/professional_development/impact_talks.html">
                        Impact Talks
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../../projects/professional_development/mentorship_program.html">
                        Mentorship Program
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../../projects/professional_development/career_features.html">
                        Career Features
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../../../projects/professional_development/career_panels.html">
                        Career Panels
                      </a>
</li>
</div>
</div>
</ul>
</nav>
</div>
</div>
<div class="sidebar-header-items__end">
<div class="navbar-end-item">
<button aria-label="light/dark" class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" data-toggle="tooltip" title="light/dark">
<span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
<span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
<span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
</button>
</div>
<div class="navbar-end-item">
<ul aria-label="Icon Links" class="navbar-nav" id="navbar-icon-links">
</ul>
</div>
</div>
</div>
<div class="sidebar-start-items sidebar-primary__section">
<div class="sidebar-start-items__item">
<a class="navbar-brand logo" href="../../intro.html">
<img alt="Logo image" class="logo__image only-light" src="../../../_static/ai-logo.png"/>
<img alt="Logo image" class="logo__image only-dark" src="../../../_static/ai-logo.png"/>
</a>
</div>
<div class="sidebar-start-items__item">
<form action="../../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search this book..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search this book..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
<div class="sidebar-start-items__item"><nav aria-label="Main" class="bd-links" id="bd-docs-nav">
<div class="bd-toc-item navbar-nav active">
<ul class="nav bd-sidenav bd-sidenav__home-link">
<li class="toctree-l1">
<a class="reference internal" href="../../intro.html">
                    Introduction
                </a>
</li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../Schedule/schedule_intro.html">Schedule</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Schedule/daily_schedules.html">General schedule</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Schedule/shared_calendars.html">Shared calendars</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Schedule/timezone_widget.html">Timezone widget</a></li>
</ul>
</input></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../TechnicalHelp/tech_intro.html">Technical Help</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../TechnicalHelp/Jupyterbook.html">Using jupyterbook</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../TechnicalHelp/Tutorial_colab.html">Using Google Colab</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../TechnicalHelp/Tutorial_kaggle.html">Using Kaggle</a></li>
</ul>
</input></li>
<li class="toctree-l2"><a class="reference internal" href="../../TechnicalHelp/Discord.html">Using discord</a></li>
</ul>
</input></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../TechnicalHelp/Links_Policy.html">Quick links and policies</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../W1D1_Generalization/chapter_title.html">Generalization (W1D1)</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../W1D1_Generalization/instructor/W1D1_Intro.html">Intro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W1D1_Generalization/instructor/W1D1_Tutorial1.html">Tutorial 1: Generalization in AI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W1D1_Generalization/instructor/W1D1_Tutorial2.html">Tutorial 2: Generalization in Neuroscience</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W1D1_Generalization/instructor/W1D1_Tutorial3.html">Tutorial 3: Generalization in Cognitive Science</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W1D1_Generalization/further_reading.html">References</a></li>
</ul>
</input></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../W1D2_ComparingTasks/chapter_title.html">Comparing Tasks (W1D2)</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../W1D2_ComparingTasks/instructor/W1D2_Intro.html">Intro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W1D2_ComparingTasks/instructor/W1D2_Tutorial1.html">Tutorial 1: Task definition, application, relations and impacts on generalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W1D2_ComparingTasks/instructor/W1D2_Tutorial2.html">Tutorial 2: Contrastive learning for object recognition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W1D2_ComparingTasks/instructor/W1D2_Tutorial3.html">Tutorial 3: Reinforcement learning across temporal scales</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W1D2_ComparingTasks/further_reading.html">References</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../W1D3_ComparingArtificialAndBiologicalNetworks/chapter_title.html">Comparing Artificial And Biological Networks (W1D3)</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../W1D3_ComparingArtificialAndBiologicalNetworks/instructor/W1D3_Intro.html">Intro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W1D3_ComparingArtificialAndBiologicalNetworks/instructor/W1D3_Tutorial1.html">Tutorial 1: Generalization and representational geometry</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W1D3_ComparingArtificialAndBiologicalNetworks/instructor/W1D3_Tutorial2.html">Tutorial 2: Computation as transformation of representational geometries</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W1D3_ComparingArtificialAndBiologicalNetworks/instructor/W1D3_Tutorial3.html">Tutorial 3: Statistical inference on representational geometries</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W1D3_ComparingArtificialAndBiologicalNetworks/instructor/W1D3_Tutorial4.html">Tutorial 4: Representational geometry &amp; noise</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Architectures</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../chapter_title.html">Microcircuits (W1D5)</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="W1D5_Intro.html">Intro</a></li>
<li class="toctree-l2"><a class="reference internal" href="W1D5_Tutorial1.html">Tutorial 1: Sparsity and Sparse Coding</a></li>
<li class="toctree-l2"><a class="reference internal" href="W1D5_Tutorial2.html">Tutorial 2: Normalization</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Tutorial 3: Attention</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../W2D1_Macrocircuits/chapter_title.html">Macrocircuits (W2D1)</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../W2D1_Macrocircuits/instructor/W2D1_Intro.html">Intro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W2D1_Macrocircuits/instructor/W2D1_Tutorial1.html">Tutorial 1: Depth vs width</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W2D1_Macrocircuits/instructor/W2D1_Tutorial2.html">Tutorial 2: Double descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W2D1_Macrocircuits/instructor/W2D1_Tutorial3.html">Tutorial 3: Neural network modularity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W2D1_Macrocircuits/further_reading.html">Suggested further readings</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../W2D2_NeuroSymbolicMethods/chapter_title.html">Neuro Symbolic Methods (W2D2)</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../W2D2_NeuroSymbolicMethods/instructor/W2D2_Intro.html">Intro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W2D2_NeuroSymbolicMethods/instructor/W2D2_Tutorial1.html">Tutorial 1: Basic operations of vector symbolic algebra</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W2D2_NeuroSymbolicMethods/instructor/W2D2_Tutorial2.html">Tutorial 2: Learning with structure</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W2D2_NeuroSymbolicMethods/instructor/W2D2_Tutorial3.html">Tutorial 3: Representations in continuous space</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W2D2_NeuroSymbolicMethods/further_reading.html">Suggested further readings</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../W2D3_Microlearning/chapter_title.html">Microlearning (W2D3)</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../W2D3_Microlearning/instructor/W2D3_Intro.html">Intro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W2D3_Microlearning/instructor/W2D3_Tutorial1.html">Tutorial 1: Microlearning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W2D3_Microlearning/further_reading.html">Suggested further readings</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../W2D4_Macrolearning/chapter_title.html">Macrolearning (W2D4)</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../W2D4_Macrolearning/instructor/W2D4_Intro.html">Intro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W2D4_Macrolearning/instructor/W2D4_Tutorial1.html">Tutorial 1: The problem of changing data distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W2D4_Macrolearning/instructor/W2D4_Tutorial2.html">Tutorial 2: Continual learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W2D4_Macrolearning/instructor/W2D4_Tutorial3.html">Tutorial 3: Meta-learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W2D4_Macrolearning/instructor/W2D4_Tutorial4.html">Tutorial 4: Biological meta reinforcement learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W2D4_Macrolearning/instructor/W2D4_Tutorial5.html">Tutorial 5: Replay</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W2D4_Macrolearning/further_reading.html">Suggested further readings</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Mysteries</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../W2D5_Mysteries/chapter_title.html">Mysteries (W2D5)</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../W2D5_Mysteries/instructor/W2D5_Intro.html">Intro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W2D5_Mysteries/instructor/W2D5_Tutorial1.html">Tutorial 1: Consciousness</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W2D5_Mysteries/instructor/W2D5_Tutorial2.html">Tutorial 2: Ethics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../W2D5_Mysteries/instructor/W2D5_Outro.html">Outro</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Project Booklet</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../projects/README.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../projects/docs/project_guidance.html">Daily guide for projects</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../projects/docs/datasets_overview.html">Project materials</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../projects/project-notebooks/Macrocircuits.html">Macrocircuits</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../projects/project-notebooks/Microlearning.html">Microlearning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../projects/project-notebooks/ComparingNetworks.html">Comparing Networks</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Professional Development</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../projects/professional_development/README.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../projects/professional_development/impact_talks.html">Impact Talks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../projects/professional_development/mentorship_program.html">Professional developemnt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../projects/professional_development/career_features.html">Career Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../projects/professional_development/career_panels.html">Career Panels</a></li>
</ul>
</div>
</nav>
</div>
</div>
<div class="sidebar-end-items sidebar-primary__section">
<div class="sidebar-end-items__item">
</div>
</div>
<div id="rtd-footer-container"></div>
</div>
<main class="bd-main" id="main-content">
<div class="sbt-scroll-pixel-helper"></div>
<div class="bd-content">
<div class="bd-article-container">
<div class="bd-header-article">
<div class="col py-1 d-flex header-article-main">
<div class="header-article__left">
<label class="sidebar-toggle primary-toggle btn btn-sm" data-placement="right" data-toggle="tooltip" for="__primary" title="Toggle primary sidebar">
<span class="fa-solid fa-bars"></span>
</label>
</div>
<div class="header-article__right">
<div class="dropdown dropdown-launch-buttons">
<button aria-expanded="false" aria-label="Launch interactive content" class="btn dropdown-toggle" data-bs-toggle="dropdown" type="button">
<i class="fas fa-rocket"></i>
</button>
<ul class="dropdown-menu">
</ul>
</div>
<button class="btn btn-sm" data-placement="bottom" data-toggle="tooltip" onclick="toggleFullScreen()" title="Fullscreen mode">
<span class="btn__icon-container">
<i class="fas fa-expand"></i>
</span>
</button>
<div class="dropdown dropdown-repository-buttons">
<button aria-expanded="false" aria-label="Source repositories" class="btn dropdown-toggle" data-bs-toggle="dropdown" type="button">
<i class="fab fa-github"></i>
</button>
<ul class="dropdown-menu">
<li><a class="btn btn-sm dropdown-item" data-placement="left" data-toggle="tooltip" href="https://github.com/neuromatch/instructor-neuroai-course-content" target="_blank" title="Source repository">
<span class="btn__icon-container">
<i class="fab fa-github"></i>
</span>
<span class="btn__text-container">repository</span>
</a>

<li><a class="btn btn-sm dropdown-item" data-placement="left" data-toggle="tooltip" href="https://github.com/neuromatch/instructor-neuroai-course-content/issues/new?title=Issue%20on%20page%20%2Ftutorials/W1D5_Microcircuits/instructor/W1D5_Tutorial3.html&amp;body=Your%20issue%20content%20here." target="_blank" title="Open an issue">
<span class="btn__icon-container">
<i class="fas fa-lightbulb"></i>
</span>
<span class="btn__text-container">open issue</span>
</a>

</li></li></ul>
</div>
<div class="dropdown dropdown-download-buttons">
<button aria-expanded="false" aria-label="Download this page" class="btn dropdown-toggle" data-bs-toggle="dropdown" type="button">
<i class="fas fa-download"></i>
</button>
<ul class="dropdown-menu">
<li><a class="btn btn-sm dropdown-item" data-placement="left" data-toggle="tooltip" href="../../../_sources/tutorials/W1D5_Microcircuits/instructor/W1D5_Tutorial3.ipynb" target="_blank" title="Download source file">
<span class="btn__icon-container">
<i class="fas fa-file"></i>
</span>
<span class="btn__text-container">.ipynb</span>
</a>

<li>
<button class="btn btn-sm dropdown-item" data-placement="left" data-toggle="tooltip" onclick="printPdf(this)" title="Print to PDF">
<span class="btn__icon-container">
<i class="fas fa-file-pdf"></i>
</span>
<span class="btn__text-container">.pdf</span>
</button>

</li></li></ul>
</div>
<label class="sidebar-toggle secondary-toggle btn btn-sm" data-placement="left" data-toggle="tooltip" for="__secondary" title="Toggle secondary sidebar">
<span class="fa-solid fa-list"></span>
</label>
</div>
</div>
</div>
<div class="onlyprint" id="jb-print-docs-body">
<h1>Tutorial 3: Attention</h1>
<!-- Table of contents -->
<div id="print-main-content">
<div id="jb-print-toc">
<div>
<h2> Contents </h2>
</div>
<nav aria-label="Page">
<ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#">
   Tutorial 3: Attention
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#tutorial-objectives">
   Tutorial Objectives
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#setup">
   Setup
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#figure-settings">
     Figure settings
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#plotting-functions">
     Plotting functions
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#data-retrieval">
     Data retrieval
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#helper-functions">
     Helper functions
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#set-random-seed">
     Set random seed
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#set-device-gpu-or-cpu-execute-set-device">
     Set device (GPU or CPU). Execute
     <code class="docutils literal notranslate">
<span class="pre">
       set_device()
      </span>
</code>
</a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-intro-to-multiplication">
   Section 1. Intro to multiplication
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#exercise-1">
     Exercise 1
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#run-auto">
       { run: “auto” }
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-self-attention-math">
   Section 2. Self-attention: math
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#exercise-2-implement-self-attention">
     Exercise 2. Implement self-attention
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-inductive-bias-of-self-attention-sparse-variable-creation">
   Section 3. Inductive bias of self-attention: Sparse variable creation
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#dataset">
     Dataset
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#exercise-2-mlp-vs-self-attention">
     Exercise 2. MLP vs. Self-Attention
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#part-1-mlp">
       Part 1: MLP
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#part-2-self-attention">
       Part 2. Self-Attention
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#part-3-coding-exercise-comparing-results">
       Part 3. Coding Exercise Comparing results
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#part-4-sample-complexity">
       Part 4. Sample complexity
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#part-5-attention-weights">
       Part 5. Attention Weights
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#sample-complexity-for-sparse-variable-creation">
     Sample Complexity for sparse variable creation
    </a>
</li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
</div>
<article class="bd-article" role="main">
<p><a href="https://colab.research.google.com/github/neuromatch/NeuroAI_Course/blob/main/tutorials/W1D5_Microcircuits/student/W1D5_Tutorial3.ipynb" target="_blank"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg"/></a>   <a href="https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/neuromatch/NeuroAI_Course/main/tutorials/W1D5_Microcircuits/student/W1D5_Tutorial3.ipynb" target="_blank"><img alt="Open in Kaggle" src="https://kaggle.com/static/images/open-in-kaggle.svg"/></a></p>
<section class="tex2jax_ignore mathjax_ignore" id="tutorial-3-attention">
<h1>Tutorial 3: Attention<a class="headerlink" href="#tutorial-3-attention" title="Permalink to this heading">#</a></h1>
<p><strong>Week 1, Day 5: Microcircuits</strong></p>
<p><strong>By Neuromatch Academy</strong></p>
<p><strong>Content creators:</strong> Saaketh Medepalli, Aditya Singh, Saeed Salehi, Xaq Pitkow</p>
<p><strong>Content reviewers:</strong> Yizhou Chen, RyeongKyung Yoon, Ruiyi Zhang, Lily Chamakura, Hlib Solodzhuk</p>
<p><strong>Production editors:</strong> Konstantine Tsafatinos, Ella Batty, Spiros Chavlis, Samuele Bolotta, Hlib Solodzhuk</p>
</section>
<hr class="docutils"/>
<section class="tex2jax_ignore mathjax_ignore" id="tutorial-objectives">
<h1>Tutorial Objectives<a class="headerlink" href="#tutorial-objectives" title="Permalink to this heading">#</a></h1>
<p>By the end of this tutorial, we aim to:</p>
<ol class="arabic simple">
<li><p>Learn how the brain and AI systems implemention attention</p></li>
<li><p>Understand how multiplicative interactions allow flexible gating of information</p></li>
<li><p>Demonstrate the inductive bias of the self-attention mechanism towards learning functions of sparse subsets of variables</p></li>
</ol>
<p>A key microarchitectural operation in brains and machines is <strong>attention</strong>. The essence of this operation is multiplication. Unlike the vanilla neural network operation, a nonlinear function of a weighted sum of inputs <span class="math notranslate nohighlight">\(f(\sum_j w_{j}x_j)\)</span>, the attention operation allows responses to <em>multiply</em> inputs. This enable computations like modulating weights by other inputs.</p>
<p> </p>
<p>In brains, the theory of the Spotlight of Attention (Posner et al 1980) posited that gain modulation allowed brain computations to highlight or select certain information, prioritizing it for subsequent computation. In machines, the seminal book Parallel Distributed Processing (Rumelhart, Hinton, McClelland 1986) described Sigma-Pi networks that included both sums (<span class="math notranslate nohighlight">\(\Sigma\)</span>) and products (<span class="math notranslate nohighlight">\(\Pi\)</span>) as fundamental operations, and mentioned. The Transformer network introduced in (Vaswani et al 2017) used a specific architecture featuring layers of multiplication and normalization. Many machine learning systems since then have fruitfully applied this architecture to language, vision, and many more modalities. In this tutorial we will isolate the central properties and generalization benefits of multiplicative attention shared by all of these applications.</p>
<p> </p>
<p>Exercises include simple attentional modulation of inputs, coding the self-attention mechanism, demonstrating its inductive bias, and interpreting the consequences of attention.</p>
<p> </p>
<p>References:</p>
<ul class="simple">
<li><p>Posner MI, Snyder CR, Davidson BJ (1980). Attention and the detection of signals. <em>Journal of experimental psychology: General</em>. 109(2):160.</p></li>
<li><p>Rumelhart DE, McClelland JL, PDP Research Group (1986). <em>Parallel Distributed Processing</em>. MIT press.</p></li>
<li><p>Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Ł, Polosukhin I (2017). <a class="reference external" href="https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html">Attention is all you need.</a> <em>Advances in Neural Information Processing Systems</em>.</p></li>
<li><p>Thomas Viehmann, <a class="reference external" href="https://github.com/MathInf/toroidal">toroidal - a lightweight transformer library for PyTorch</a></p></li>
<li><p>Edelman et al. 2022, <em>Inductive biases and variable creation in self-attention mechanisms</em></p></li>
<li><p>Deep Graph Library Tutorials, <a class="reference external" href="https://docs.dgl.ai/en/0.8.x/tutorials/models/4_old_wines/7_transformer.html">Transformer as a Graph Neural Network</a></p></li>
<li><p>Lilian Weng, <a class="reference external" href="https://lilianweng.github.io/posts/2018-06-24-attention/">Attention? Attention!</a></p></li>
</ul>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "7c1967b3de84461eba851f5a154de808"}</script></div>
</div>
</section>
<hr class="docutils"/>
<section class="tex2jax_ignore mathjax_ignore" id="setup">
<h1>Setup<a class="headerlink" href="#setup" title="Permalink to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Imports</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>
</div>
</div>
</div>
<section id="figure-settings">
<h2>Figure settings<a class="headerlink" href="#figure-settings" title="Permalink to this heading">#</a></h2>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Figure settings</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s1">'matplotlib.font_manager'</span><span class="p">)</span><span class="o">.</span><span class="n">disabled</span> <span class="o">=</span> <span class="kc">True</span>
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = 'retina'
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/NMA2020/nma.mplstyle"</span><span class="p">)</span>
<span class="n">fig_w</span><span class="p">,</span> <span class="n">fig_h</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">'figure.figsize'</span><span class="p">]</span>
</pre></div>
</div>
</div>
</details>
</div>
</section>
<section id="plotting-functions">
<h2>Plotting functions<a class="headerlink" href="#plotting-functions" title="Permalink to this heading">#</a></h2>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Plotting functions</span>
<span class="k">def</span> <span class="nf">plot_loss_accuracy</span><span class="p">(</span><span class="n">t_loss</span><span class="p">,</span> <span class="n">t_acc</span><span class="p">,</span> <span class="n">v_loss</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">v_acc</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">"Training and Validation for the Transformer Model"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t_loss</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Training loss"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"red"</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">v_loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># plt.plot(v_loss, label="Valididation loss", color="blue")</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">t_loss</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">v_loss</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Validation loss"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"blue"</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">"*"</span><span class="p">)</span>
        <span class="c1"># plt.text(len(t_loss)-1, v_loss, f"{v_loss:.3f}", va="bottom", ha="right")</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s2">"log"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"Epoch"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"Loss"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">"lower right"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t_acc</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Training accuracy"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"red"</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">"dotted"</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">v_acc</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># plt.plot(v_acc, label="Validation accuracy", color="blue", linestyle="--")</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">t_acc</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">v_acc</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Validation accuracy"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"blue"</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">"*"</span><span class="p">)</span>
        <span class="c1"># plt.text(len(t_acc)-1, v_acc, f"{v_acc:.3f}", va="bottom", ha="right")</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"Epoch"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"Accuracy"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">"lower right"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">plot_samples</span><span class="p">(</span><span class="n">X_plot</span><span class="p">,</span> <span class="n">y_plot</span><span class="p">,</span> <span class="n">correct_ids</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">n_samples</span><span class="p">,</span> <span class="n">seq_length</span> <span class="o">=</span> <span class="n">X_plot</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">rects</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">ri</span> <span class="ow">in</span> <span class="n">correct_ids</span><span class="p">:</span>
        <span class="n">rects</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">Rectangle</span><span class="p">((</span><span class="n">ri</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">"red"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">X_plot</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">"binary"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">rect</span> <span class="ow">in</span> <span class="n">rects</span><span class="p">:</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">rect</span><span class="p">)</span>
    <span class="c1"># axs[0].axis("off")</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([])</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">"Context"</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">"Samples"</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">title</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">y_plot</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">"binary"</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">Rectangle</span><span class="p">((</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_label_position</span><span class="p">(</span><span class="s2">"right"</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">"Labels"</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">wspace</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">plot_attention_weights</span><span class="p">(</span><span class="n">att_weights</span><span class="p">,</span> <span class="n">correct_ids</span><span class="p">,</span> <span class="n">context_length</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="kn">from</span> <span class="nn">matplotlib.lines</span> <span class="kn">import</span> <span class="n">Line2D</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">att_weights</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">aw_flatten</span> <span class="o">=</span> <span class="n">att_weights</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">context_length</span><span class="p">)</span>
    <span class="n">x_axis</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">context_length</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">y_axis</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">context_length</span><span class="p">)</span>
    <span class="n">aw_ravel</span> <span class="o">=</span> <span class="n">aw_flatten</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
    <span class="n">x_ravel</span> <span class="o">=</span> <span class="n">x_axis</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
    <span class="n">y_ravel</span> <span class="o">=</span> <span class="n">y_axis</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"red"</span><span class="p">,</span> <span class="s2">"green"</span><span class="p">]</span>
    <span class="n">labels_legened</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"True"</span><span class="p">,</span> <span class="s2">"False"</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_ravel</span><span class="p">,</span> <span class="n">aw_ravel</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="p">[</span><span class="n">colors</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">y</span><span class="p">)]</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">y_ravel</span><span class="p">])</span>
    <span class="n">rects</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">ri</span> <span class="ow">in</span> <span class="n">correct_ids</span><span class="p">:</span>
        <span class="n">rects</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">Rectangle</span><span class="p">((</span><span class="n">ri</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1e-6</span><span class="p">),</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">"blue"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">rect</span> <span class="ow">in</span> <span class="n">rects</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">rect</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s2">"log"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mf">1e-6</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Attention weights for the whole batch"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"Boolean input index t"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"Attention weight"</span><span class="p">)</span>
    <span class="n">legend_elements</span> <span class="o">=</span> <span class="p">[</span><span class="n">Line2D</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'None'</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'green'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'True'</span><span class="p">,</span> <span class="n">markerfacecolor</span><span class="o">=</span><span class="s1">'g'</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">7</span><span class="p">),</span>
                       <span class="n">Line2D</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'None'</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'False'</span><span class="p">,</span> <span class="n">markerfacecolor</span><span class="o">=</span><span class="s1">'r'</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">7</span><span class="p">)]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">handles</span><span class="o">=</span><span class="n">legend_elements</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s1">'upper right'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">plot_compare</span><span class="p">(</span><span class="n">results_sat_d</span><span class="p">,</span> <span class="n">results_sat_s</span><span class="p">,</span> <span class="n">results_mlp_d</span><span class="p">,</span> <span class="n">results_mlp_s</span><span class="p">,</span>
                 <span class="n">B_t_sat_s</span><span class="p">,</span> <span class="n">B_t_sat_d</span><span class="p">,</span> <span class="n">B_t_mlp_s</span><span class="p">,</span> <span class="n">B_t_mlp_d</span><span class="p">):</span>
    <span class="kn">from</span>  <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">LinearSegmentedColormap</span>
    <span class="n">cmap</span><span class="o">=</span><span class="n">LinearSegmentedColormap</span><span class="o">.</span><span class="n">from_list</span><span class="p">(</span><span class="s1">'rg'</span><span class="p">,[</span><span class="s2">"w"</span><span class="p">,</span> <span class="s2">"w"</span><span class="p">,</span> <span class="s2">"r"</span><span class="p">,</span> <span class="s2">"y"</span><span class="p">,</span> <span class="s2">"g"</span><span class="p">],</span> <span class="n">N</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>

    <span class="n">t_loss_sat_d</span><span class="p">,</span> <span class="n">t_acc_sat_d</span><span class="p">,</span> <span class="n">v_loss_sat_d</span><span class="p">,</span> <span class="n">v_acc_sat_d</span><span class="p">,</span> <span class="n">model_np_sat_d</span> <span class="o">=</span> <span class="n">results_sat_d</span>
    <span class="n">t_loss_sat_s</span><span class="p">,</span> <span class="n">t_acc_sat_s</span><span class="p">,</span> <span class="n">v_loss_sat_s</span><span class="p">,</span> <span class="n">v_acc_sat_s</span><span class="p">,</span> <span class="n">model_np_sat_s</span> <span class="o">=</span> <span class="n">results_sat_s</span>
    <span class="n">t_loss_mlp_d</span><span class="p">,</span> <span class="n">t_acc_mlp_d</span><span class="p">,</span> <span class="n">v_loss_mlp_d</span><span class="p">,</span> <span class="n">v_acc_mlp_d</span><span class="p">,</span> <span class="n">model_np_mlp_d</span> <span class="o">=</span> <span class="n">results_mlp_d</span>
    <span class="n">t_loss_mlp_s</span><span class="p">,</span> <span class="n">t_acc_mlp_s</span><span class="p">,</span> <span class="n">v_loss_mlp_s</span><span class="p">,</span> <span class="n">v_acc_mlp_s</span><span class="p">,</span> <span class="n">model_np_mlp_s</span> <span class="o">=</span> <span class="n">results_mlp_s</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"sparse"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"Attention"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"blue"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"green"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="sa">f</span><span class="s2">"# Parameters: </span><span class="si">{</span><span class="n">model_np_sat_s</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="sa">f</span><span class="s2">"# samples: </span><span class="si">{</span><span class="n">B_t_sat_s</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">t_acc_sat_s</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">t_acc_sat_s</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">v_acc_sat_s</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">v_acc_sat_s</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="sa">f</span><span class="s2">"# training acc: </span><span class="si">{</span><span class="n">t_acc_sat_s</span><span class="si">:</span><span class="s2">.0%</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="sa">f</span><span class="s2">"# validation acc: </span><span class="si">{</span><span class="n">v_acc_sat_s</span><span class="si">:</span><span class="s2">.0%</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"dense"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"blue"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"green"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="sa">f</span><span class="s2">"# Parameters: </span><span class="si">{</span><span class="n">model_np_sat_d</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="sa">f</span><span class="s2">"# samples: </span><span class="si">{</span><span class="n">B_t_sat_d</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">t_acc_sat_d</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">t_acc_sat_d</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">v_acc_sat_d</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">v_acc_sat_d</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="sa">f</span><span class="s2">"# training acc: </span><span class="si">{</span><span class="n">t_acc_sat_d</span><span class="si">:</span><span class="s2">.0%</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="sa">f</span><span class="s2">"# validation acc: </span><span class="si">{</span><span class="n">v_acc_sat_d</span><span class="si">:</span><span class="s2">.0%</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"blue"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"green"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="sa">f</span><span class="s2">"# Parameters: </span><span class="si">{</span><span class="n">model_np_mlp_s</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="sa">f</span><span class="s2">"# samples: </span><span class="si">{</span><span class="n">B_t_mlp_s</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">t_acc_mlp_s</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">t_acc_mlp_s</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">v_acc_mlp_s</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">v_acc_mlp_s</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="sa">f</span><span class="s2">"# training acc: </span><span class="si">{</span><span class="n">t_acc_mlp_s</span><span class="si">:</span><span class="s2">.0%</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="sa">f</span><span class="s2">"# validation acc: </span><span class="si">{</span><span class="n">v_acc_mlp_s</span><span class="si">:</span><span class="s2">.0%</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"MLP"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"blue"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"green"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="sa">f</span><span class="s2">"# Parameters: </span><span class="si">{</span><span class="n">model_np_mlp_d</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="sa">f</span><span class="s2">"# samples: </span><span class="si">{</span><span class="n">B_t_mlp_d</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">t_acc_mlp_d</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">t_acc_mlp_d</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">v_acc_mlp_d</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">v_acc_mlp_d</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="sa">f</span><span class="s2">"# training acc: </span><span class="si">{</span><span class="n">t_acc_mlp_d</span><span class="si">:</span><span class="s2">.0%</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="sa">f</span><span class="s2">"# validation acc: </span><span class="si">{</span><span class="n">v_acc_mlp_d</span><span class="si">:</span><span class="s2">.0%</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">"off"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
</div>
</section>
<section id="data-retrieval">
<h2>Data retrieval<a class="headerlink" href="#data-retrieval" title="Permalink to this heading">#</a></h2>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Data retrieval</span>
<span class="k">class</span> <span class="nc">s_Sparse_AND</span><span class="p">:</span>  <span class="c1"># 1-Dimensional AND</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">T</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">s</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">T</span> <span class="o">=</span> <span class="n">T</span> <span class="c1"># context length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">s</span> <span class="o">=</span> <span class="n">s</span> <span class="c1"># sparsity</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">=</span> <span class="mf">0.5</span><span class="o">**</span><span class="p">(</span><span class="mf">1.0</span><span class="o">/</span><span class="mf">3.0</span><span class="p">)</span>  <span class="c1"># probability chosen for balanced data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">f_i</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">pick_an_f</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">f_i</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">T</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">s</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">others</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">f_i</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">m</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">f_i</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pick_an_f</span><span class="p">()</span>
        <span class="n">max_try</span> <span class="o">=</span> <span class="mi">100</span>
        <span class="n">i_try</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">while</span> <span class="n">i_try</span> <span class="o">&lt;</span> <span class="n">max_try</span><span class="p">:</span>
            <span class="n">i_try</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">T</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">X</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">y</span><span class="p">[</span><span class="n">X</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">f_i</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="n">m</span> <span class="o">&lt;</span> <span class="mf">0.4</span> <span class="ow">or</span> <span class="n">y</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="n">m</span> <span class="o">&gt;</span> <span class="mf">0.6</span><span class="p">:</span>
                <span class="n">verbose</span> <span class="ow">and</span> <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Large imbalance in the training set </span><span class="si">{</span><span class="n">y</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="n">m</span><span class="si">}</span><span class="s2">, retrying..."</span><span class="p">)</span>
                <span class="k">continue</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">verbose</span> <span class="ow">and</span> <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Data-label balance: </span><span class="si">{</span><span class="n">y</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="n">m</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
            <span class="n">bad_batch</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">f_i</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">others</span><span class="p">:</span>
                    <span class="k">if</span> <span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">o</span><span class="p">])</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>
                        <span class="n">verbose</span> <span class="ow">and</span> <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Found at least another compatible hypothesis </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">o</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
                        <span class="n">bad_batch</span> <span class="o">=</span> <span class="kc">True</span>
                        <span class="k">break</span>
            <span class="k">if</span> <span class="n">bad_batch</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">break</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">verbose</span> <span class="ow">and</span> <span class="nb">print</span><span class="p">(</span><span class="s2">"Could not find a compatible hypothesis"</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">X</span><span class="o">.</span><span class="n">long</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>


<span class="c1"># From notebook</span>
<span class="c1"># Load an example image from the imagenet-sample-images repository</span>
<span class="k">def</span> <span class="nf">load_image</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
<span class="w">  </span><span class="sd">"""</span>
<span class="sd">  Load an image from a given path</span>

<span class="sd">  Args:</span>
<span class="sd">    path: String</span>
<span class="sd">      Path to the image</span>

<span class="sd">  Returns:</span>
<span class="sd">    img: PIL Image</span>
<span class="sd">      Image loaded from the path</span>
<span class="sd">  """</span>
  <span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">img</span>
</pre></div>
</div>
</div>
</details>
</div>
</section>
<section id="helper-functions">
<h2>Helper functions<a class="headerlink" href="#helper-functions" title="Permalink to this heading">#</a></h2>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Helper functions</span>

<span class="k">class</span> <span class="nc">BinaryMLP</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_dims</span><span class="p">,</span> <span class="n">h_dims</span><span class="p">,</span> <span class="n">out_dims</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_dims</span> <span class="o">=</span> <span class="n">in_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">h_dims</span> <span class="o">=</span> <span class="n">h_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_dims</span> <span class="o">=</span> <span class="n">out_dims</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_dims</span><span class="p">,</span> <span class="n">h_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">h_dims</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">h_dims</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">h_dims</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]))</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">())</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">h_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">out_dims</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">class</span> <span class="nc">BinarySAT</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Binary Self-Attention Transformer</span>
<span class="sd">    The code is adopted from "https://github.com/MathInf/toroidal" by Thomas Viehmann</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">T</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">d</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">T</span> <span class="o">=</span> <span class="n">T</span>  <span class="c1"># context length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">E</span> <span class="o">=</span> <span class="n">T</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1"># effective length (including cls token)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">d</span>  <span class="c1"># embedding size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>  <span class="c1"># number of heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="p">(</span><span class="n">d</span> <span class="o">//</span> <span class="n">n_heads</span><span class="p">)</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span>  <span class="c1"># scaling factor (1 / sqrt(d_k))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">=</span> <span class="n">n</span>  <span class="c1"># number of hidden units</span>
        <span class="k">assert</span> <span class="n">d</span> <span class="o">%</span> <span class="n">n_heads</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">"embedding size must be divisible by number of heads"</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># vocabulary size (binary input, 0 or 1)</span>
        <span class="n">att_drop</span><span class="o">=</span><span class="mf">0.1</span>
        <span class="n">out_drop</span><span class="o">=</span><span class="mf">0.1</span>
        <span class="n">mlp_drop</span><span class="o">=</span><span class="mf">0.1</span>
        <span class="n">ln_eps</span><span class="o">=</span><span class="mf">1e-6</span>

        <span class="c1"># embedding layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">toke</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>  <span class="c1"># token embedding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cls</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span>  <span class="c1"># "cls / class / global" learnable token</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pose</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">T</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span>  <span class="c1"># positional embedding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">ln_eps</span><span class="p">)</span>  <span class="c1"># [https://arxiv.org/pdf/2002.04745.pdf]</span>

        <span class="c1"># self-attention layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">qkv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">d</span><span class="p">)</span>  <span class="c1"># query, key, value layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_attn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">att_drop</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>  <span class="c1"># projection layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">out_drop</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">ln_eps</span><span class="p">)</span>

        <span class="c1"># MLP layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_l1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_l2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_mlp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">mlp_drop</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>

        <span class="c1"># clasification layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># initialize weights and biases (per description in the paper)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">toke</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pose</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cls</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">ones_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">qkv</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">qkv</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">proj</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">proj</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">ones_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mlp_l1</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mlp_l1</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mlp_l2</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mlp_l2</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">ones_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm3</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm3</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Embedding</span>
        <span class="n">B</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># batch size</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">toke</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pose</span>

        <span class="c1"># Transformer Block</span>
        <span class="n">norm_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># [https://arxiv.org/pdf/2002.04745.pdf]</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">qkv</span><span class="p">(</span><span class="n">norm_x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">E</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="c1"># # (Scaled Dot-Product Attention)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">"bthc,bshc-&gt;bhts"</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>  <span class="c1"># query key product</span>
        <span class="n">logits</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span>  <span class="c1"># normalize against staturation</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_attn</span><span class="p">(</span><span class="n">attn</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">"bhts,bshc-&gt;bthc"</span><span class="p">,</span> <span class="n">attn</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>  <span class="c1"># weighted attention</span>
        <span class="c1"># # concat and linear projection with residual connection</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">E</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d</span><span class="p">)</span>  <span class="c1"># recombine</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>  <span class="c1"># linear layer projection</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_out</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">output</span><span class="p">)</span>  <span class="c1"># normalization and residual connection</span>

        <span class="c1"># MLP with residual connection</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mlp_l1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>  <span class="c1"># nonlinear layer</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_mlp</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_l2</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>  <span class="c1"># linear layer</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm3</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">output</span><span class="p">)</span>  <span class="c1"># normalization and residual connection</span>

        <span class="c1"># projection</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># binary classification task</span>
        <span class="k">return</span> <span class="n">x</span>


<span class="k">def</span> <span class="nf">bin_acc</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Compute the binary accuracy</span>
<span class="sd">    """</span>
    <span class="n">y_</span> <span class="o">=</span> <span class="n">y_hat</span><span class="o">.</span><span class="n">round</span><span class="p">()</span>
    <span class="n">TP_TN</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">FP_FN</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_</span> <span class="o">!=</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="k">assert</span> <span class="n">TP_TN</span> <span class="o">+</span> <span class="n">FP_FN</span> <span class="o">==</span> <span class="n">y</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">TP_TN</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">FP_FN</span><span class="si">}</span><span class="s2"> != </span><span class="si">{</span><span class="n">y</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="si">}</span><span class="s2">"</span>
    <span class="k">return</span> <span class="n">TP_TN</span> <span class="o">/</span> <span class="n">y</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">get_n_parameters</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Get the number of learnable parameters in a model</span>
<span class="sd">    """</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">par</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="n">par</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">i</span>


<span class="k">def</span> <span class="nf">save_model</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s1">'model_states.pt'</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">load_model</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">model_states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">'model_states.pt'</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">model_states</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">evaluator</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">X_v</span><span class="p">,</span> <span class="n">y_v</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">"cpu"</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">X_v</span><span class="p">,</span> <span class="n">y_v</span> <span class="o">=</span> <span class="n">X_v</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_v</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_v</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y_hat</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">y_v</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>
        <span class="n">acc</span> <span class="o">=</span> <span class="n">bin_acc</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y_v</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">acc</span>


<span class="k">def</span> <span class="nf">trainer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">n_epochs</span><span class="p">,</span> <span class="n">X_t</span><span class="p">,</span> <span class="n">y_t</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">"cpu"</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">train_loss</span><span class="p">,</span> <span class="n">train_acc</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">X_t</span><span class="p">,</span> <span class="n">y_t</span> <span class="o">=</span> <span class="n">X_t</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_t</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_t</span><span class="p">)</span>
        <span class="n">loss_t</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y_hat</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">y_t</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>
        <span class="n">loss_t</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">train_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_t</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
            <span class="n">train_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">bin_acc</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y_t</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">train_loss</span><span class="p">,</span> <span class="n">train_acc</span>


<span class="k">def</span> <span class="nf">scaled_dot_product_attention_solution</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
<span class="w">    </span><span class="sd">""" Scaled dot product attention</span>
<span class="sd">    Args:</span>
<span class="sd">        Q: queries (B, H, d, n)</span>
<span class="sd">        K: keys (B, H, d, n)</span>
<span class="sd">        V: values (B, H, d, n)</span>
<span class="sd">    Returns:</span>
<span class="sd">        Attention tensor (B, H, d, n), Scores (B, H, d, d)</span>
<span class="sd">    Notes:</span>
<span class="sd">        (B, H, d, n): batch size, H: number of heads, d: key-query dim, n: embedding dim</span>
<span class="sd">    """</span>

    <span class="k">assert</span> <span class="n">K</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">Q</span><span class="o">.</span><span class="n">shape</span> <span class="ow">and</span> <span class="n">K</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">V</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">"Queries, Keys and Values must have the same shape"</span>
    <span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># batch_size, num_heads, key-query dim, embedding dim</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
    <span class="n">Q_mm_K</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">"bhdn,bhen-&gt;bhde"</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span>  <span class="c1"># dot-product reducing the n dimension</span>
    <span class="n">S</span> <span class="o">=</span> <span class="n">Q_mm_K</span> <span class="o">/</span> <span class="n">scale</span>  <span class="c1"># score or scaled dot product</span>
    <span class="n">S_sm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># softmax</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">"bhde,bhen-&gt;bhdn"</span><span class="p">,</span> <span class="n">S_sm</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>  <span class="c1"># Attention</span>
    <span class="k">return</span> <span class="n">A</span><span class="p">,</span> <span class="n">S</span>


<span class="k">def</span> <span class="nf">make_train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">B_t</span><span class="p">,</span> <span class="n">B_v</span><span class="p">,</span> <span class="n">n_epochs</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">"cpu"</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s2">"MLP"</span><span class="p">,</span> <span class="n">results</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">etta</span> <span class="o">=</span> <span class="mf">1e-3</span>
    <span class="n">data_gen</span> <span class="o">=</span> <span class="n">s_Sparse_AND</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">data_gen</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">B_t</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span> <span class="o">=</span> <span class="n">data_gen</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">B_v</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">etta</span><span class="p">)</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()</span>
    <span class="n">model_np</span> <span class="o">=</span> <span class="n">get_n_parameters</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>  <span class="c1"># number of learnable parameters</span>
    <span class="n">results</span> <span class="ow">and</span> <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Number of model's learnable parameters: </span><span class="si">{</span><span class="n">model_np</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">kind</span> <span class="o">==</span> <span class="s2">"MLP"</span><span class="p">:</span>
        <span class="n">t_loss</span><span class="p">,</span> <span class="n">t_acc</span> <span class="o">=</span> <span class="n">trainer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">n_epochs</span><span class="p">,</span> <span class="n">X_train</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">results</span><span class="p">:</span>
            <span class="n">v_loss</span><span class="p">,</span> <span class="n">v_acc</span> <span class="o">=</span> <span class="n">evaluator</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">X_valid</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">y_valid</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">t_loss</span><span class="p">,</span> <span class="n">t_acc</span> <span class="o">=</span> <span class="n">trainer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">n_epochs</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">results</span><span class="p">:</span>
            <span class="n">v_loss</span><span class="p">,</span> <span class="n">v_acc</span> <span class="o">=</span> <span class="n">evaluator</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">results</span> <span class="ow">and</span> <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Training loss: </span><span class="si">{</span><span class="n">t_loss</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, accuracy: </span><span class="si">{</span><span class="n">t_acc</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="n">results</span> <span class="ow">and</span> <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Validation loss: </span><span class="si">{</span><span class="n">v_loss</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, accuracy: </span><span class="si">{</span><span class="n">v_acc</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">results</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">t_loss</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">t_acc</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">v_loss</span><span class="p">,</span> <span class="n">v_acc</span><span class="p">,</span> <span class="n">model_np</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">data_gen</span>


<span class="k">def</span> <span class="nf">weighted_attention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""This function computes the weighted attention as a method for the BinarySAT class</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): An array of shape (B:batch_size, T:context length) containing the input data</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: weighted attention of shape (B, E, E) where E = T + 1</span>
<span class="sd">    """</span>
    <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">"This function is only implemented for a single head!"</span>
    <span class="c1"># Embedding</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># batch size</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">toke</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># token embedding</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># concatenate cls token</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pose</span>  <span class="c1"># positional embedding</span>
    <span class="n">norm_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># normalization</span>

    <span class="c1"># Scaled Dot-Product Attention (partially implemented)</span>
    <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">qkv</span><span class="p">(</span><span class="n">norm_x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">E</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d</span><span class="p">)</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="c1"># q, k, v all have shape (B, E, h, d) where h is the number of heads (1 in this case)</span>
    <span class="n">W_qk</span> <span class="o">=</span> <span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">W_qk</span> <span class="o">=</span> <span class="n">W_qk</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span>
    <span class="n">W_qk</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">W_qk</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">W_qk</span>


<span class="k">def</span> <span class="nf">results_dict</span><span class="p">(</span><span class="n">results_sat_d</span><span class="p">,</span> <span class="n">results_sat_s</span><span class="p">,</span> <span class="n">results_mlp_d</span><span class="p">,</span> <span class="n">results_mlp_s</span><span class="p">):</span>
    <span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span>
    <span class="n">t_loss_sat_d</span><span class="p">,</span> <span class="n">t_acc_sat_d</span><span class="p">,</span> <span class="n">v_loss_sat_d</span><span class="p">,</span> <span class="n">v_acc_sat_d</span><span class="p">,</span> <span class="n">model_np_sat_d</span> <span class="o">=</span> <span class="n">results_sat_d</span>
    <span class="n">t_loss_sat_s</span><span class="p">,</span> <span class="n">t_acc_sat_s</span><span class="p">,</span> <span class="n">v_loss_sat_s</span><span class="p">,</span> <span class="n">v_acc_sat_s</span><span class="p">,</span> <span class="n">model_np_sat_s</span> <span class="o">=</span> <span class="n">results_sat_s</span>
    <span class="n">t_loss_mlp_d</span><span class="p">,</span> <span class="n">t_acc_mlp_d</span><span class="p">,</span> <span class="n">v_loss_mlp_d</span><span class="p">,</span> <span class="n">v_acc_mlp_d</span><span class="p">,</span> <span class="n">model_np_mlp_d</span> <span class="o">=</span> <span class="n">results_mlp_d</span>
    <span class="n">t_loss_mlp_s</span><span class="p">,</span> <span class="n">t_acc_mlp_s</span><span class="p">,</span> <span class="n">v_loss_mlp_s</span><span class="p">,</span> <span class="n">v_acc_mlp_s</span><span class="p">,</span> <span class="n">model_np_mlp_s</span> <span class="o">=</span> <span class="n">results_mlp_s</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
    <span class="c1"># output["Training Loss SAT dense"] = t_loss_sat_d</span>
    <span class="c1"># output["Training Accuracy SAT dense"] = t_acc_sat_d</span>
    <span class="n">output</span><span class="p">[</span><span class="s2">"Validation Loss SAT dense"</span><span class="p">]</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">v_loss_sat_d</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">output</span><span class="p">[</span><span class="s2">"Validation Accuracy SAT dense"</span><span class="p">]</span> <span class="o">=</span> <span class="n">v_acc_sat_d</span>
    <span class="n">output</span><span class="p">[</span><span class="s2">"Number of Parameters SAT dense"</span><span class="p">]</span> <span class="o">=</span> <span class="n">model_np_sat_d</span>
    <span class="c1"># output["Training Loss SAT sparse"] = t_loss_sat_s</span>
    <span class="c1"># output["Training Accuracy SAT sparse"] = t_acc_sat_s</span>
    <span class="n">output</span><span class="p">[</span><span class="s2">"Validation Loss SAT sparse"</span><span class="p">]</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">v_loss_sat_s</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">output</span><span class="p">[</span><span class="s2">"Validation Accuracy SAT sparse"</span><span class="p">]</span> <span class="o">=</span> <span class="n">v_acc_sat_s</span>
    <span class="n">output</span><span class="p">[</span><span class="s2">"Number of Parameters SAT sparse"</span><span class="p">]</span> <span class="o">=</span> <span class="n">model_np_sat_s</span>
    <span class="c1"># output["Training Loss MLP dense"] = t_loss_mlp_d</span>
    <span class="c1"># output["Training Accuracy MLP dense"] = t_acc_mlp_d</span>
    <span class="n">output</span><span class="p">[</span><span class="s2">"Validation Loss MLP dense"</span><span class="p">]</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">v_loss_mlp_d</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">output</span><span class="p">[</span><span class="s2">"Validation Accuracy MLP dense"</span><span class="p">]</span> <span class="o">=</span> <span class="n">v_acc_mlp_d</span>
    <span class="n">output</span><span class="p">[</span><span class="s2">"Number of Parameters MLP dense"</span><span class="p">]</span> <span class="o">=</span> <span class="n">model_np_mlp_d</span>
    <span class="c1"># output["Training Loss MLP sparse"] = t_loss_mlp_s</span>
    <span class="c1"># output["Training Accuracy MLP sparse"] = t_acc_mlp_s</span>
    <span class="n">output</span><span class="p">[</span><span class="s2">"Validation Loss MLP sparse"</span><span class="p">]</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">v_loss_mlp_s</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">output</span><span class="p">[</span><span class="s2">"Validation Accuracy MLP sparse"</span><span class="p">]</span> <span class="o">=</span> <span class="n">v_acc_mlp_s</span>
    <span class="n">output</span><span class="p">[</span><span class="s2">"Number of Parameters MLP sparse"</span><span class="p">]</span> <span class="o">=</span> <span class="n">model_np_mlp_s</span>
    <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
</div>
</details>
</div>
</section>
<section id="set-random-seed">
<h2>Set random seed<a class="headerlink" href="#set-random-seed" title="Permalink to this heading">#</a></h2>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Set random seed</span>

<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="k">def</span> <span class="nf">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">seed_torch</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">  </span><span class="sd">"""</span>
<span class="sd">  Handles variability by controlling sources of randomness</span>
<span class="sd">  through set seed values</span>

<span class="sd">  Args:</span>
<span class="sd">    seed: Integer</span>
<span class="sd">      Set the seed value to given integer.</span>
<span class="sd">      If no seed, set seed value to random integer in the range 2^32</span>
<span class="sd">    seed_torch: Bool</span>
<span class="sd">      Seeds the random number generator for all devices to</span>
<span class="sd">      offer some guarantees on reproducibility</span>

<span class="sd">  Returns:</span>
<span class="sd">    Nothing</span>
<span class="sd">  """</span>
  <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">seed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="mi">32</span><span class="p">)</span>
  <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">seed_torch</span><span class="p">:</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed_all</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="kc">True</span>

  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Random seed </span><span class="si">{</span><span class="n">seed</span><span class="si">}</span><span class="s1"> has been set.'</span><span class="p">)</span>


<span class="c1"># In case that `DataLoader` is used</span>
<span class="k">def</span> <span class="nf">seed_worker</span><span class="p">(</span><span class="n">worker_id</span><span class="p">):</span>
<span class="w">  </span><span class="sd">"""</span>
<span class="sd">  DataLoader will reseed workers following randomness in</span>
<span class="sd">  multi-process data loading algorithm.</span>

<span class="sd">  Args:</span>
<span class="sd">    worker_id: integer</span>
<span class="sd">      ID of subprocess to seed. 0 means that</span>
<span class="sd">      the data will be loaded in the main process</span>
<span class="sd">      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details</span>

<span class="sd">  Returns:</span>
<span class="sd">    Nothing</span>
<span class="sd">  """</span>
  <span class="n">worker_seed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">initial_seed</span><span class="p">()</span> <span class="o">%</span> <span class="mi">2</span><span class="o">**</span><span class="mi">32</span>
  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">worker_seed</span><span class="p">)</span>
  <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">worker_seed</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
</div>
</section>
<section id="set-device-gpu-or-cpu-execute-set-device">
<h2>Set device (GPU or CPU). Execute <code class="docutils literal notranslate"><span class="pre">set_device()</span></code><a class="headerlink" href="#set-device-gpu-or-cpu-execute-set-device" title="Permalink to this heading">#</a></h2>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Set device (GPU or CPU). Execute `set_device()`</span>
<span class="c1"># especially if torch modules used.</span>

<span class="c1"># inform the user if the notebook uses GPU or CPU.</span>

<span class="k">def</span> <span class="nf">set_device</span><span class="p">():</span>
<span class="w">  </span><span class="sd">"""</span>
<span class="sd">  Set the device. CUDA if available, CPU otherwise</span>

<span class="sd">  Args:</span>
<span class="sd">    None</span>

<span class="sd">  Returns:</span>
<span class="sd">    Nothing</span>
<span class="sd">  """</span>
  <span class="n">device</span> <span class="o">=</span> <span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span>
  <span class="k">if</span> <span class="n">device</span> <span class="o">!=</span> <span class="s2">"cuda"</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"WARNING: For this notebook to perform best, "</span>
        <span class="s2">"if possible, in the menu under `Runtime` -&gt; "</span>
        <span class="s2">"`Change runtime type.`  select `GPU` "</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"GPU is enabled in this notebook."</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">device</span>
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">SEED</span> <span class="o">=</span> <span class="mi">2021</span>
<span class="n">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">SEED</span><span class="p">)</span>
<span class="n">DEVICE</span> <span class="o">=</span> <span class="n">set_device</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Random seed 2021 has been set.
WARNING: For this notebook to perform best, if possible, in the menu under `Runtime` -&gt; `Change runtime type.`  select `GPU` 
cpu
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils"/>
<section class="tex2jax_ignore mathjax_ignore" id="section-1-intro-to-multiplication">
<h1>Section 1. Intro to multiplication<a class="headerlink" href="#section-1-intro-to-multiplication" title="Permalink to this heading">#</a></h1>
<p>In this exercise we will show how signals can be used to selectively gate inputs. This is the essence of attention.</p>
<section id="exercise-1">
<h2>Exercise 1<a class="headerlink" href="#exercise-1" title="Permalink to this heading">#</a></h2>
<p>We’ll implement simple dot product attention for input <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and scalar output <span class="math notranslate nohighlight">\(y\)</span> given by a weighted combination of the inputs,
$<span class="math notranslate nohighlight">\(y = \mathbf{w}\cdot\mathbf{x}\)</span><span class="math notranslate nohighlight">\(
for weights \)</span>\mathbf{w}<span class="math notranslate nohighlight">\(. Unlike usual readouts, however, attention adjusts the weights based on other signals \)</span>\mathbf{z}<span class="math notranslate nohighlight">\(. Here we will use just a two dimensional attentional gain, \)</span>z_1<span class="math notranslate nohighlight">\( and \)</span>z_2<span class="math notranslate nohighlight">\(, each with a corresponding key vector \)</span>\mathbf{q}$ that determines what is attended:</p>
<div class="math notranslate nohighlight">
\[\mathbf{w}(z_1,z_2) = \text{softmax}(z_1 \mathbf{q}_1 + z_2 \mathbf{q}_2)\]</div>
<p>You should code up this function to return the attentional weights and output.</p>
<div>
<img src="https://raw.githubusercontent.com/ssnio/nma_neuroai_d4_t4/main/static/Fig_0_ref2.png">
<figcaption>Figure from Inductive Biases and Variable Creation in Self-Attention Mechanisms</figcaption>
</img></div>
<p><strong>Just include first panel above</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gained_dot_product_attention</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>  <span class="c1"># input vector</span>
                                 <span class="n">q_1</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>  <span class="c1"># query vector 1</span>
                                 <span class="n">q_2</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>  <span class="c1"># query vector 2</span>
                                 <span class="n">z_1</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>  <span class="c1"># query gain 1</span>
                                 <span class="n">z_2</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>  <span class="c1"># query gain 2</span>
                                 <span class="p">):</span>
<span class="w">    </span><span class="sd">"""This function computes the gained dot product attention</span>
<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): input vector</span>
<span class="sd">        q_1 (Tensor): query vector 1</span>
<span class="sd">        q_2 (Tensor): query vector 2</span>
<span class="sd">        z_1 (float): query gain 1</span>
<span class="sd">        z_2 (float): query gain 2</span>
<span class="sd">    Returns:</span>
<span class="sd">        w (Tensor): attention weights</span>
<span class="sd">        y (float): gained dot product attention</span>
<span class="sd">    """</span>
    <span class="c1">#################################################</span>
    <span class="c1">## TODO Implement the `gained_dot_product_attention`</span>
    <span class="c1"># Fill remove the following line of code one you have completed the exercise:</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Student exercise: complete calculation of gained dot product attention."</span><span class="p">)</span>
    <span class="c1">#################################################</span>
    <span class="n">w</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">y</span> <span class="o">=</span> <span class="o">...</span>
    <span class="k">return</span> <span class="n">w</span><span class="p">,</span> <span class="n">y</span>

</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># to_remove solution</span>
<span class="k">def</span> <span class="nf">gained_dot_product_attention</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>  <span class="c1"># input vector</span>
                                 <span class="n">q_1</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>  <span class="c1"># query vector 1</span>
                                 <span class="n">q_2</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>  <span class="c1"># query vector 2</span>
                                 <span class="n">z_1</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>  <span class="c1"># query gain 1</span>
                                 <span class="n">z_2</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>  <span class="c1"># query gain 2</span>
                                 <span class="p">):</span>
<span class="w">    </span><span class="sd">"""This function computes the gained dot product attention</span>
<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): input vector</span>
<span class="sd">        q_1 (Tensor): query vector 1</span>
<span class="sd">        q_2 (Tensor): query vector 2</span>
<span class="sd">        z_1 (float): query gain 1</span>
<span class="sd">        z_2 (float): query gain 2</span>
<span class="sd">    Returns:</span>
<span class="sd">        w (Tensor): attention weights</span>
<span class="sd">        y (float): gained dot product attention</span>
<span class="sd">    """</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">z_1</span> <span class="o">*</span> <span class="n">q_1</span> <span class="o">+</span> <span class="n">z_2</span> <span class="o">*</span> <span class="n">q_2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">w</span><span class="p">,</span> <span class="n">y</span>
</pre></div>
</div>
</div>
</div>
<p>Now we plot the input weights. Manipulate the sliders to control the two attention gains <span class="math notranslate nohighlight">\(z_k\)</span>, which we’ve pre-assigned to two specific sigmoidal pattern vectors <span class="math notranslate nohighlight">\(\mathbf{q}_k\)</span>. (Feel free to change those functions and see how the sliders change what they attend.) Observe how these sliders change which part of the input is amplified and attenuated.</p>
<section id="run-auto">
<h3>{ run: “auto” }<a class="headerlink" href="#run-auto" title="Permalink to this heading">#</a></h3>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title { run: "auto" }</span>
<span class="n">gain_1</span> <span class="o">=</span> <span class="mf">61.8</span> <span class="c1"># @param {type:"slider", min:0.1, max:100.0, step:0.1}</span>
<span class="n">gain_2</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="c1"># @param {type:"slider", min:0.1, max:100.0, step:0.1}</span>
<span class="n">T</span> <span class="o">=</span> <span class="mi">17</span>  <span class="c1"># context length</span>
<span class="c1"># pre-defined signals</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="mf">3.1415</span><span class="p">,</span> <span class="n">T</span><span class="p">))</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">T</span><span class="p">)</span>
<span class="n">q_1</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="n">T</span><span class="p">))</span>
<span class="n">q_1</span> <span class="o">=</span> <span class="n">q_1</span> <span class="o">/</span> <span class="n">q_1</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">q_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">7</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">T</span><span class="p">))</span>
<span class="n">q_2</span> <span class="o">=</span> <span class="n">q_2</span> <span class="o">/</span> <span class="n">q_2</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="n">w</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">gained_dot_product_attention</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">q_1</span><span class="p">,</span> <span class="n">q_2</span><span class="p">,</span> <span class="n">gain_1</span><span class="p">,</span> <span class="n">gain_2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"y: </span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="k">with</span> <span class="n">plt</span><span class="o">.</span><span class="n">xkcd</span><span class="p">():</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">q_1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"$\mathbf</span><span class="si">{q_1}</span><span class="s2">$"</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">"m"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">q_2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"$\mathbf</span><span class="si">{q_2}</span><span class="s2">$"</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">"y"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"$\mathbf</span><span class="si">{w}</span><span class="s2">$"</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">"r"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">q_1</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span><span class="n">q_2</span><span class="o">.</span><span class="n">max</span><span class="p">()))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"$\mathbf</span><span class="si">{x}</span><span class="s2">$"</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">"blue"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">5</span><span class="o">*</span><span class="n">x</span><span class="o">*</span><span class="n">w</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"$5\mathbf</span><span class="si">{w}</span><span class="s2">*\mathbf</span><span class="si">{x}</span><span class="s2">$"</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">"red"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>y: 0.3223479092121124
</pre></div>
</div>
<img alt="../../../_images/c967dfd5cbadbbff652c7f51275dd9001737333d05ddb383f3af3c12bbac95e0.png" src="../../../_images/c967dfd5cbadbbff652c7f51275dd9001737333d05ddb383f3af3c12bbac95e0.png">
</img></div>
</div>
<p>The provided plot illustrates the impact of the gain factor on the attention weights for different input dimensions. The key observations and interpretations are as follows:</p>
<ol class="arabic simple">
<li><p>Sparsity and Selectivity:</p>
<ul class="simple">
<li><p>The softmax includes both an exponentiation that exaggerates large values, and a normalization which ensures that the weights sum to 1. Thus, other weights decrease as the biggest weights increase.</p></li>
<li><p>This increased selectivity can focus on the most relevant features.</p></li>
</ul>
</li>
<li><p>Gain Factor Influence:</p>
<ul class="simple">
<li><p>For small gains, the attention weights are evenly distributed across the input dimensions.</p></li>
<li><p>As the gain factor increases, the attention weights become sparser: more concentrated and peaked.</p></li>
</ul>
</li>
</ol>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="section-2-self-attention-math">
<h1>Section 2. Self-attention: math<a class="headerlink" href="#section-2-self-attention-math" title="Permalink to this heading">#</a></h1>
<p>Where do these gain factors come from?</p>
<p>In dot product attention, the weights were <span class="math notranslate nohighlight">\(\mathbf{w}=\text{softmax}(\mathbf{z}\cdot Q)\)</span>, a function of an external <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> and fixed matrix <span class="math notranslate nohighlight">\(Q\)</span>. (Above we used a 2-dimensional <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>.)</p>
<p>In the <em>self-attention</em> mechanism used in Transformers, the attentional weights are instead functions of the same input <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> that is later modulated by these weights.</p>
<p>A very simple version would simply set <span class="math notranslate nohighlight">\(\mathbf{z}=\mathbf{x}\)</span>, so
$<span class="math notranslate nohighlight">\(\mathbf{w}=\text{softmax}(\mathbf{x}\cdot Q)\)</span>$</p>
<p>However, to provide more flexibility, we allow <span class="math notranslate nohighlight">\(Q\)</span> to <em>also</em> depend on the input: <span class="math notranslate nohighlight">\(Q=W_q\mathbf{x}\)</span>. For even more flexibility, we select only specific aspects of the input to serve as gain modulation,
$<span class="math notranslate nohighlight">\(\mathbf{z}=W_k\mathbf{x}\)</span><span class="math notranslate nohighlight">\(
so
\)</span><span class="math notranslate nohighlight">\(\mathbf{w}=\text{softmax}(W_k \mathbf{x}\,\mathbf{x}^\top W_q)\)</span>$
Notice that the weights are themselves products of the input!</p>
<p>One final specialization is to allow multiple outputs, each being different projections of the weighted inputs: <span class="math notranslate nohighlight">\(y=\mathbf{w}\cdot W_v\mathbf{x}\)</span>. This use of multiple features is known as <em>multi-head</em> attention.</p>
<p>This is commonly described as a query-key-value framework. These various learnable projections of the input are given names:
$<span class="math notranslate nohighlight">\(
Q=W_q\mathbf{x}\ \ \ \ \text{query}\\
K=W_k\mathbf{x}\ \ \ \ \ \ \text{key}\\
V=W_v\mathbf{x}\ \ \ \ \text{value}
\)</span>$</p>
<p>The self-attention mechanism is then usually written as:</p>
<div class="math notranslate nohighlight">
\[\mathbf{y}=\text{softmax}\,(\frac{QK^\top}{\sqrt{d}})V\]</div>
<p>where we get a big weight on inputs with a strong match between Query and Key, and that weight is applied to a particular set of Values. (For different dimensions <span class="math notranslate nohighlight">\(d\)</span> of attended features, there’s also a scaling by <span class="math notranslate nohighlight">\(\sqrt{d}\)</span> in the softmax for convenience.)</p>
<p> </p>
<p>Putting all this together, the self-attention is then
$<span class="math notranslate nohighlight">\(
\mathbf{y}=\text{softmax}(\tfrac{1}{\sqrt{d}}W_q\mathbf{x}\mathbf{x}^\top W_k^\top)W_v\mathbf{x}
\)</span>$
This equation shows that <strong>the attention mechanism in transformers is a composition of multiplication, sparsification, normalization, and another multiplication</strong> — all special microarchitecture operations!</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p><img alt="alt" src="https://thegradient.pub/content/images/2020/09/attention-block.jpg"/></p></th>
<th class="head"><p><img alt="alt" src="https://thegradient.pub/content/images/2020/09/gnn-block.jpg"/></p></th>
</tr>
</thead>
</table>
<p><strong>Revision Note: update notation in figure, use only left panel</strong></p>
<section id="exercise-2-implement-self-attention">
<h2>Exercise 2. Implement self-attention<a class="headerlink" href="#exercise-2-implement-self-attention" title="Permalink to this heading">#</a></h2>
<p>In this exercise, you will implement the Attention operation from the transformer architecture, and test it against pre-written code.</p>
<p> </p>
<p>Assume that you are given the queries <span class="math notranslate nohighlight">\(Q\)</span>, keys <span class="math notranslate nohighlight">\(K\)</span>, and values <span class="math notranslate nohighlight">\(V\)</span>, each as projections of the inputs <span class="math notranslate nohighlight">\(W\mathbf{x}\)</span>.</p>
<p>These objects will have multiple features (heads) for each element. Denoting <span class="math notranslate nohighlight">\(B\)</span> as the batch size, <span class="math notranslate nohighlight">\(H\)</span> as the number of heads, <span class="math notranslate nohighlight">\(n\)</span> as the embedding dimension, and <span class="math notranslate nohighlight">\(d\)</span> as the dimensionality of the queries, keys, and values (for simplicity, we assume they have the same dimensionality), the objects we need to combine are</p>
<div class="math notranslate nohighlight">
\[\begin{split}\text{Query} \ Q\in \mathcal{R}^{B \times H \times d \times n}\\
\text{Key} \ K\in \mathcal{R}^{B \times H \times d \times n}\\
\text{Value} \ V\in \mathcal{R}^{B \times H \times d \times n}
\end{split}\]</div>
<p>The input to the softmax, <span class="math notranslate nohighlight">\(Q K^T/\sqrt{d}\)</span>, is sometimes called the attention Score,
$<span class="math notranslate nohighlight">\(
\text{Score}\ S\in \mathcal{R}^{B \times H \times d \times d}\\
\text{Attention}\ \mathbf{y}\in \mathcal{R}^{B \times H \times d \times n}
\)</span>$</p>
<p><strong>NOTES:</strong>
<span class="math notranslate nohighlight">\(K\)</span>, <span class="math notranslate nohighlight">\(Q\)</span>, <span class="math notranslate nohighlight">\(V\)</span> are multi-dimensional tensors. Since <code class="docutils literal notranslate"><span class="pre">torch.matmul</span></code>’s matrix multiplication does not support tensors, we can either use Einstein-Sum <code class="docutils literal notranslate"><span class="pre">einsum</span></code> or Batch-MatMul <code class="docutils literal notranslate"><span class="pre">bmm</span></code> for the MatMul operation. It is very important to keep in mind the notation and that we are first performing the dot-product (reduction) on the embedding dimension! The softmax is also applied along the context (<span class="math notranslate nohighlight">\(d\)</span>) dimension.</p>
<p><strong>Revision note: explain embedding dimension <span class="math notranslate nohighlight">\(n\)</span></strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
<span class="w">    </span><span class="sd">""" Scaled dot product attention</span>
<span class="sd">    Args:</span>
<span class="sd">        Q: queries (B, H, d, n)</span>
<span class="sd">        K: keys (B, H, d, n)</span>
<span class="sd">        V: values (B, H, d, n)</span>
<span class="sd">    Returns:</span>
<span class="sd">        Attention tensor (B, H, d, n), Scores (B, H, d, d)</span>
<span class="sd">    Notes:</span>
<span class="sd">        (B, H, d, n): batch size, H: number of heads, d: key-query dim, n: embedding dim</span>
<span class="sd">    """</span>

    <span class="k">assert</span> <span class="n">K</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">Q</span><span class="o">.</span><span class="n">shape</span> <span class="ow">and</span> <span class="n">K</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">V</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">"Queries, Keys and Values must have the same shape"</span>
    <span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># batch_size, num_heads, key-query dim, embedding dim</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>

    <span class="c1">#################################################</span>
    <span class="c1"># # TODO Scaled dot product attention</span>
    <span class="c1"># # You should choose between torch.bmm or torch.einsum</span>
    <span class="c1"># # Remove or comment out the lines you don't need</span>
    <span class="c1"># Fill remove the following line of code one you have completed the exercise:</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Student exercise: implement full version of scaled dot product attention."</span><span class="p">)</span>
    <span class="c1">#################################################</span>
    <span class="c1"># # START using torch.bmm #######################</span>
    <span class="n">Q_</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span> <span class="o">*</span> <span class="n">H</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span> <span class="c1"># necessary only if using torch.bmm</span>
    <span class="n">K_</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span> <span class="o">*</span> <span class="n">H</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span> <span class="c1"># necessary only if using torch.bmm</span>
    <span class="n">V_</span> <span class="o">=</span> <span class="n">V</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span> <span class="o">*</span> <span class="n">H</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span> <span class="c1"># necessary only if using torch.bmm</span>
    <span class="n">Q_mm_K</span> <span class="o">=</span> <span class="o">...</span>  <span class="c1"># dot-product reducing the n dimension</span>
    <span class="n">S</span> <span class="o">=</span> <span class="o">...</span>  <span class="c1"># score or scaled dot product</span>
    <span class="n">S_sm</span> <span class="o">=</span> <span class="o">...</span>  <span class="c1"># softmax</span>
    <span class="n">A</span> <span class="o">=</span> <span class="o">...</span>  <span class="c1"># Attention</span>
    <span class="n">S</span> <span class="o">=</span> <span class="n">S</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="c1"># necessary only if using torch.bmm</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span> <span class="c1"># necessary only if using torch.bmm</span>
    <span class="c1"># # END using torch.bmm #########################</span>
    <span class="c1"># # START using torch.einsum ####################</span>
    <span class="n">Q_mm_K</span> <span class="o">=</span> <span class="o">...</span>  <span class="c1"># dot-product reducing the n dimension</span>
    <span class="n">S</span> <span class="o">=</span> <span class="o">...</span>  <span class="c1"># score or scaled dot product</span>
    <span class="n">S_sm</span> <span class="o">=</span> <span class="o">...</span>  <span class="c1"># softmax</span>
    <span class="n">A</span> <span class="o">=</span> <span class="o">...</span>  <span class="c1"># Attention</span>
    <span class="c1"># # END using torch.einsum ######################</span>

    <span class="k">assert</span> <span class="n">S</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">d</span><span class="p">),</span> <span class="s2">"Score tensor does not have the correct shape"</span>
    <span class="k">assert</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="s2">"Attention tensor does not have the correct shape"</span>
    <span class="k">return</span> <span class="n">A</span><span class="p">,</span> <span class="n">S</span>  <span class="c1"># Attention, Score</span>

</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># to_remove solution</span>
<span class="k">def</span> <span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
<span class="w">    </span><span class="sd">""" Scaled dot product attention</span>
<span class="sd">    Args:</span>
<span class="sd">        Q: queries (B, H, d, n)</span>
<span class="sd">        K: keys (B, H, d, n)</span>
<span class="sd">        V: values (B, H, d, n)</span>
<span class="sd">    Returns:</span>
<span class="sd">        Attention tensor (B, H, d, n), Scores (B, H, d, d)</span>
<span class="sd">    Notes:</span>
<span class="sd">        (B, H, d, n): batch size, H: number of heads, d: key-query dim, n: embedding dim</span>
<span class="sd">    """</span>

    <span class="k">assert</span> <span class="n">K</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">Q</span><span class="o">.</span><span class="n">shape</span> <span class="ow">and</span> <span class="n">K</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">V</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">"Queries, Keys and Values must have the same shape"</span>
    <span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># batch_size, num_heads, key-query dim, embedding dim</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>

    <span class="c1"># # START using torch.bmm #######################</span>
    <span class="n">Q_</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span> <span class="o">*</span> <span class="n">H</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span> <span class="c1"># necessary only if using torch.bmm</span>
    <span class="n">K_</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span> <span class="o">*</span> <span class="n">H</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span> <span class="c1"># necessary only if using torch.bmm</span>
    <span class="n">V_</span> <span class="o">=</span> <span class="n">V</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span> <span class="o">*</span> <span class="n">H</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span> <span class="c1"># necessary only if using torch.bmm</span>
    <span class="n">Q_mm_K</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">Q_</span><span class="p">,</span> <span class="n">K_</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>  <span class="c1"># dot-product reducing the n dimension</span>
    <span class="n">S</span> <span class="o">=</span> <span class="n">Q_mm_K</span> <span class="o">/</span> <span class="n">scale</span>  <span class="c1"># score or scaled dot product</span>
    <span class="n">S_sm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># softmax</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">S_sm</span><span class="p">,</span> <span class="n">V_</span><span class="p">)</span>  <span class="c1"># Attention</span>
    <span class="n">S</span> <span class="o">=</span> <span class="n">S</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="c1"># necessary only if using torch.bmm</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span> <span class="c1"># necessary only if using torch.bmm</span>
    <span class="c1"># # END using torch.bmm #########################</span>
    <span class="c1"># # START using torch.einsum ####################</span>
    <span class="n">Q_mm_K</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">"bhdn,bhen-&gt;bhde"</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span>  <span class="c1"># dot-product reducing the n dimension</span>
    <span class="n">S</span> <span class="o">=</span> <span class="n">Q_mm_K</span> <span class="o">/</span> <span class="n">scale</span>  <span class="c1"># score or scaled dot product</span>
    <span class="n">S_sm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># softmax</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">"bhde,bhen-&gt;bhdn"</span><span class="p">,</span> <span class="n">S_sm</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>  <span class="c1"># Attention</span>
    <span class="c1"># # END using torch.einsum ######################</span>

    <span class="k">assert</span> <span class="n">S</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">d</span><span class="p">),</span> <span class="s2">"Score tensor does not have the correct shape"</span>
    <span class="k">assert</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="s2">"Attention tensor does not have the correct shape"</span>
    <span class="k">return</span> <span class="n">A</span><span class="p">,</span> <span class="n">S</span>  <span class="c1"># Attention, Score</span>
</pre></div>
</div>
</div>
</div>
<p>Here is a function to test whether your function matches the correct output for self-attention.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Testing the function scaled_dot_product</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">n_heads</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">key_query_value_dim</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">embed_dim</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">tensor_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">key_query_value_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
<span class="n">queries</span><span class="p">,</span> <span class="n">keys</span><span class="p">,</span> <span class="n">values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">tensor_shape</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">tensor_shape</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">tensor_shape</span><span class="p">)</span>
<span class="n">your_attention</span><span class="p">,</span> <span class="n">your_score</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span> <span class="n">keys</span><span class="p">,</span> <span class="n">values</span><span class="p">)</span>
<span class="n">our_attention</span><span class="p">,</span> <span class="n">our_score</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention_solution</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span> <span class="n">keys</span><span class="p">,</span> <span class="n">values</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">your_attention</span> <span class="o">==</span> <span class="n">our_attention</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">(),</span> <span class="s2">"The two implementations should produce the same result"</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">your_score</span> <span class="o">==</span> <span class="n">our_score</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">(),</span> <span class="s2">"The two implementations should produce the same result"</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils"/>
<section class="tex2jax_ignore mathjax_ignore" id="section-3-inductive-bias-of-self-attention-sparse-variable-creation">
<h1>Section 3. Inductive bias of self-attention: Sparse variable creation<a class="headerlink" href="#section-3-inductive-bias-of-self-attention-sparse-variable-creation" title="Permalink to this heading">#</a></h1>
<p>What is the self-attention mechanism especially good at learning?</p>
<p>In the next exercise, we will redo some results from the paper, <a class="reference external" href="https://arxiv.org/abs/2110.10090">Inductive Biases and Variable Creation in Self-Attention Mechanisms</a>. They show that a single self-attention head can successfully learn to represent a sparse function with low sample complexity.</p>
<p>The sparse functions we will use in this tutorial are s-sparse AND functions: For <span class="math notranslate nohighlight">\(T\)</span> binary innputs (‘context length’) and <em>s</em> pre-selected unique indices, the sequence is labeled as <em>True</em> if the value of <span class="math notranslate nohighlight">\(T\)</span> at <em>all</em> of the chosen indices is <span class="math notranslate nohighlight">\(1\)</span>, otherwise <em>False</em>. This means that the sequence label only depends on <em>s</em> elements of the whole input.</p>
<p>We will compare DL architectures (MLP <em>vs</em> Self-Attention) in learning this sparse boolean function <span class="math notranslate nohighlight">\(f\)</span> with few training samples and good generalization error.</p>
<section id="dataset">
<h2>Dataset<a class="headerlink" href="#dataset" title="Permalink to this heading">#</a></h2>
<p>For this tutorial we have already defined a s-sparse AND dataset generator class <code class="docutils literal notranslate"><span class="pre">s_Sparse_AND</span></code> for you. It generates <span class="math notranslate nohighlight">\(m\)</span> sequences with context length <span class="math notranslate nohighlight">\(T\)</span> and sparsity <span class="math notranslate nohighlight">\(s\)</span>. (Note that Each input element is more likely to be 1 than 0 so that the labels have equal probability.) Here we visualize a few input samples and their corresponding labels. The red rectangles show the relevant indicies for this dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">context_length</span> <span class="o">=</span> <span class="mi">30</span>  <span class="c1"># T: context length</span>
<span class="n">s_sparse</span> <span class="o">=</span> <span class="mi">3</span>  <span class="c1"># s: sparsity (number of function-relevant indices)</span>
<span class="n">n_sequences</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># m: number of samples (sequences)</span>
<span class="n">data_gen</span> <span class="o">=</span> <span class="n">s_Sparse_AND</span><span class="p">(</span><span class="n">context_length</span><span class="p">,</span> <span class="n">s_sparse</span><span class="p">)</span>
<span class="n">X_</span><span class="p">,</span> <span class="n">y_</span> <span class="o">=</span> <span class="n">data_gen</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">n_sequences</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">correct_ids</span> <span class="o">=</span> <span class="n">data_gen</span><span class="o">.</span><span class="n">f_i</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Target (function-relevant indices) indices: </span><span class="si">{</span><span class="n">correct_ids</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">plot_samples</span><span class="p">(</span><span class="n">X_</span><span class="p">,</span> <span class="n">y_</span><span class="p">,</span> <span class="n">correct_ids</span><span class="p">,</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">s_sparse</span><span class="si">}</span><span class="s2">_Sparse_AND samples"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Target (function-relevant indices) indices: [12, 26, 27]
</pre></div>
</div>
<img alt="../../../_images/e7e135723e9e6ea153853b8e826110e08c385afeb45807fb2267ee29aeb93fd9.png" src="../../../_images/e7e135723e9e6ea153853b8e826110e08c385afeb45807fb2267ee29aeb93fd9.png"/>
</div>
</div>
</section>
<section id="exercise-2-mlp-vs-self-attention">
<h2>Exercise 2. MLP vs. Self-Attention<a class="headerlink" href="#exercise-2-mlp-vs-self-attention" title="Permalink to this heading">#</a></h2>
<p>Let’s put the MLP and Self-Attention to the test, and see which of the two architectures can generalize better in this problem. We will test both on sparse function (<em>s</em> = 3) as well as a denser function (<em>s</em> = 15) for the context length of <em>T</em> = 30. We will be using a helper-function <code class="docutils literal notranslate"><span class="pre">make_train</span></code> that takes the model and hyper-parameters and will return the trained model and some results.</p>
<p>This exercise has 4 parts:</p>
<ol class="arabic simple">
<li><p>Create training and validation datasets for MLP, and train an MLP model on the task</p></li>
<li><p>Repeat for the self-attention model</p></li>
<li><p>Plot to compare the results for the two models and datasets</p></li>
<li><p>You finally can change the sample complexity of the MLP dataset, to get to 100% accuracy.</p></li>
<li><p>Plot the attention score (attention weights) for the transformer!</p></li>
</ol>
<p><strong>ABBREVIATIONS:</strong></p>
<ul class="simple">
<li><p>suffix <code class="docutils literal notranslate"><span class="pre">_s</span></code> = sparse</p></li>
<li><p>suffix <code class="docutils literal notranslate"><span class="pre">_d</span></code> = dense</p></li>
<li><p>prefix <code class="docutils literal notranslate"><span class="pre">t_</span></code> = training</p></li>
<li><p>prefix <code class="docutils literal notranslate"><span class="pre">v_</span></code> = validation</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model_np</span></code> = number of parameters</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sat</span></code> = Self-Attention Transformer</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Problem hyperparameters</span>
<span class="n">context_length</span> <span class="o">=</span> <span class="mi">30</span>  <span class="c1"># T: context length</span>
<span class="n">sparse_dense</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>  <span class="c1"># s: sparsity (number of function-relevant indices)</span>
<span class="n">B_valid</span> <span class="o">=</span> <span class="mi">500</span>  <span class="c1"># batch size for validation</span>
</pre></div>
</div>
</div>
</div>
<section id="part-1-mlp">
<h3>Part 1: MLP<a class="headerlink" href="#part-1-mlp" title="Permalink to this heading">#</a></h3>
<p>Training an MLP on the “s-Sparse AND” task. How does the model do?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># # Hyperparameters for sparse MLP</span>
<span class="n">B_t_mlp</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># batch size for training (number of training samples)</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">500</span>  <span class="c1"># number of epochs</span>
<span class="n">s_sparse</span> <span class="o">=</span> <span class="n">sparse_dense</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># sparse</span>
<span class="n">hidden_layers</span> <span class="o">=</span> <span class="p">[</span><span class="mi">512</span><span class="p">]</span>  <span class="c1"># the number of hidden units in each layer [H1, H2, ...]</span>
<span class="n">kind</span> <span class="o">=</span> <span class="s2">"MLP"</span>

<span class="n">mlp_model</span> <span class="o">=</span> <span class="n">BinaryMLP</span><span class="p">(</span><span class="n">context_length</span><span class="p">,</span> <span class="n">hidden_layers</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># MLP model</span>
<span class="n">results_mlp_s</span> <span class="o">=</span> <span class="n">make_train</span><span class="p">(</span><span class="n">mlp_model</span><span class="p">,</span> <span class="n">context_length</span><span class="p">,</span> <span class="n">s_sparse</span><span class="p">,</span> <span class="n">B_t_mlp</span><span class="p">,</span> <span class="n">B_valid</span><span class="p">,</span> <span class="n">n_epochs</span><span class="p">,</span> <span class="n">DEVICE</span><span class="p">,</span> <span class="n">kind</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of model's learnable parameters: 16385
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training loss: 0.003, accuracy: 1.000
Validation loss: 0.626, accuracy: 0.870
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># # Hyperparameters for dense MLP</span>
<span class="n">B_t_mlp</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># batch size for training (number of training samples)</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">500</span>  <span class="c1"># number of epochs</span>
<span class="n">s_sparse</span> <span class="o">=</span> <span class="n">sparse_dense</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># dense</span>
<span class="n">hidden_layers</span> <span class="o">=</span> <span class="p">[</span><span class="mi">512</span><span class="p">]</span>  <span class="c1"># the number of hidden units in each layer [H1, H2, ...]</span>
<span class="n">kind</span> <span class="o">=</span> <span class="s2">"MLP"</span>

<span class="n">mlp_model</span> <span class="o">=</span> <span class="n">BinaryMLP</span><span class="p">(</span><span class="n">context_length</span><span class="p">,</span> <span class="n">hidden_layers</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># MLP model</span>
<span class="n">results_mlp_d</span> <span class="o">=</span> <span class="n">make_train</span><span class="p">(</span><span class="n">mlp_model</span><span class="p">,</span> <span class="n">context_length</span><span class="p">,</span> <span class="n">s_sparse</span><span class="p">,</span> <span class="n">B_t_mlp</span><span class="p">,</span> <span class="n">B_valid</span><span class="p">,</span> <span class="n">n_epochs</span><span class="p">,</span> <span class="n">DEVICE</span><span class="p">,</span> <span class="n">kind</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of model's learnable parameters: 16385
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training loss: 0.030, accuracy: 0.980
Validation loss: 0.502, accuracy: 0.822
</pre></div>
</div>
</div>
</div>
</section>
<section id="part-2-self-attention">
<h3>Part 2. Self-Attention<a class="headerlink" href="#part-2-self-attention" title="Permalink to this heading">#</a></h3>
<p>Build a Transformer model and train it on the given dataset. How does the training results compare to MLP?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># # Hyperparameters for sparse SAT</span>
<span class="n">B_t_sat</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># batch size for training (number of training samples)</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">500</span>  <span class="c1"># number of epochs</span>
<span class="n">s_sparse</span> <span class="o">=</span> <span class="n">sparse_dense</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># sparse</span>
<span class="n">embed_dim</span> <span class="o">=</span> <span class="mi">32</span>  <span class="c1"># embedding dimension</span>
<span class="n">n_heads</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># number of heads</span>
<span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">64</span>  <span class="c1"># number of hidden units</span>
<span class="n">kind</span> <span class="o">=</span> <span class="s2">"SAT"</span>

<span class="n">sat_model_s</span> <span class="o">=</span> <span class="n">BinarySAT</span><span class="p">(</span><span class="n">context_length</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>  <span class="c1"># selt-attention transformer</span>
<span class="n">results_sat_s</span> <span class="o">=</span> <span class="n">make_train</span><span class="p">(</span><span class="n">sat_model_s</span><span class="p">,</span> <span class="n">context_length</span><span class="p">,</span> <span class="n">s_sparse</span><span class="p">,</span> <span class="n">B_t_sat</span><span class="p">,</span> <span class="n">B_valid</span><span class="p">,</span> <span class="n">n_epochs</span><span class="p">,</span> <span class="n">DEVICE</span><span class="p">,</span> <span class="n">kind</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of model's learnable parameters: 9729
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training loss: 0.002, accuracy: 1.000
Validation loss: 0.002, accuracy: 1.000
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># # Hyperparameters for dense SAT</span>
<span class="n">B_t_sat</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># batch size for training (number of training samples)</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">500</span>  <span class="c1"># number of epochs</span>
<span class="n">s_sparse</span> <span class="o">=</span> <span class="n">sparse_dense</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># dense</span>
<span class="n">embed_dim</span> <span class="o">=</span> <span class="mi">32</span>  <span class="c1"># embedding dimension</span>
<span class="n">n_heads</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># number of heads</span>
<span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">64</span>  <span class="c1"># number of hidden units</span>
<span class="n">kind</span> <span class="o">=</span> <span class="s2">"SAT"</span>

<span class="n">sat_model_d</span> <span class="o">=</span> <span class="n">BinarySAT</span><span class="p">(</span><span class="n">context_length</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>  <span class="c1"># selt-attention transformer</span>
<span class="n">results_sat_d</span> <span class="o">=</span> <span class="n">make_train</span><span class="p">(</span><span class="n">sat_model_d</span><span class="p">,</span> <span class="n">context_length</span><span class="p">,</span> <span class="n">s_sparse</span><span class="p">,</span> <span class="n">B_t_sat</span><span class="p">,</span> <span class="n">B_valid</span><span class="p">,</span> <span class="n">n_epochs</span><span class="p">,</span> <span class="n">DEVICE</span><span class="p">,</span> <span class="n">kind</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of model's learnable parameters: 9729
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training loss: 0.002, accuracy: 1.000
Validation loss: 0.617, accuracy: 0.904
</pre></div>
</div>
</div>
</div>
</section>
<section id="part-3-coding-exercise-comparing-results">
<h3>Part 3. Coding Exercise Comparing results<a class="headerlink" href="#part-3-coding-exercise-comparing-results" title="Permalink to this heading">#</a></h3>
<p>Here, we ask you to plot the results of the trainings above and the hyper-parameters you think are important. The goal is to show in one plot how Self-Attention Transformers and Multi-Layer Perceptons compare for both sparse and dense boolean tasks. You can use any or all of the following information in your plot:</p>
<ul class="simple">
<li><p>number of parameters</p></li>
<li><p>training samples</p></li>
<li><p>validation accuracy</p></li>
<li><p>validation loss</p></li>
</ul>
<p>We have provided the results in an ordered dictionary <code class="docutils literal notranslate"><span class="pre">ordered_results</span></code> for your convenience.</p>
<p><strong>Hint:</strong> You can maximise creativity and information-communication :)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Validation loss, accuracy, number of parameters and number of training samples</span>
<span class="n">ordered_results</span> <span class="o">=</span> <span class="n">results_dict</span><span class="p">(</span><span class="n">results_sat_d</span><span class="p">,</span> <span class="n">results_sat_s</span><span class="p">,</span> <span class="n">results_mlp_d</span><span class="p">,</span> <span class="n">results_mlp_s</span><span class="p">)</span>
<span class="n">ordered_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>OrderedDict([('Validation Loss SAT dense', 0.617),
             ('Validation Accuracy SAT dense', 0.904),
             ('Number of Parameters SAT dense', 9729),
             ('Validation Loss SAT sparse', 0.002),
             ('Validation Accuracy SAT sparse', 1.0),
             ('Number of Parameters SAT sparse', 9729),
             ('Validation Loss MLP dense', 0.502),
             ('Validation Accuracy MLP dense', 0.822),
             ('Number of Parameters MLP dense', 16385),
             ('Validation Loss MLP sparse', 0.626),
             ('Validation Accuracy MLP sparse', 0.87),
             ('Number of Parameters MLP sparse', 16385)])
</pre></div>
</div>
</div>
</div>
</section>
<section id="part-4-sample-complexity">
<h3>Part 4. Sample complexity<a class="headerlink" href="#part-4-sample-complexity" title="Permalink to this heading">#</a></h3>
<p>One measure of generalization is sample complexity. You can change the number of training samples to check if MLP can do better. Also, only train the MLP model and avoid retraining the transformer to save energy and time</p>
</section>
<section id="part-5-attention-weights">
<h3>Part 5. Attention Weights<a class="headerlink" href="#part-5-attention-weights" title="Permalink to this heading">#</a></h3>
<p>A common figure in attention literature is the “Attention visualization” which shows how the model is attending to different parts of the sequence. You have already implemented the function that returns the attention weight <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>, and we will use a similar implementation for our transformer model. The blue rectangles show the target (function-relevant indices) indices. You can see for the <code class="docutils literal notranslate"><span class="pre">True</span></code> sequences (where the target label is <code class="docutils literal notranslate"><span class="pre">True</span></code>), the attention weight would push all the irrelevant values down and increase the attention to the target indices.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># # Hyperparameters for sparse SAT</span>
<span class="n">context_length</span> <span class="o">=</span> <span class="mi">30</span>  <span class="c1"># T: context length</span>
<span class="n">B_t_sat</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># batch size for training (number of training samples)</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">500</span>  <span class="c1"># number of epochs</span>
<span class="n">s_sparse</span> <span class="o">=</span> <span class="mi">3</span>  <span class="c1"># sparse</span>
<span class="n">embed_dim</span> <span class="o">=</span> <span class="mi">32</span>  <span class="c1"># embedding dimension</span>
<span class="n">n_heads</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># number of heads</span>
<span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">64</span>  <span class="c1"># number of hidden units</span>
<span class="n">n_sequences</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># number of samples for ploting</span>
<span class="n">kind</span> <span class="o">=</span> <span class="s2">"SAT"</span>

<span class="n">sat_model_s</span> <span class="o">=</span> <span class="n">BinarySAT</span><span class="p">(</span><span class="n">context_length</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>  <span class="c1"># selt-attention transformer</span>
<span class="n">data_gen</span> <span class="o">=</span> <span class="n">make_train</span><span class="p">(</span><span class="n">sat_model_s</span><span class="p">,</span> <span class="n">context_length</span><span class="p">,</span> <span class="n">s_sparse</span><span class="p">,</span> <span class="n">B_t_sat</span><span class="p">,</span> <span class="n">B_valid</span><span class="p">,</span> <span class="n">n_epochs</span><span class="p">,</span> <span class="n">DEVICE</span><span class="p">,</span> <span class="n">kind</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
<span class="n">X_</span><span class="p">,</span> <span class="n">y_</span> <span class="o">=</span> <span class="n">data_gen</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">n_sequences</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">correct_ids</span> <span class="o">=</span> <span class="n">data_gen</span><span class="o">.</span><span class="n">f_i</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Target (function-relevant indices) indices: </span><span class="si">{</span><span class="n">correct_ids</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">w_att</span> <span class="o">=</span> <span class="n">weighted_attention</span><span class="p">(</span><span class="n">sat_model_s</span><span class="p">,</span> <span class="n">X_</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">))</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
<span class="k">with</span> <span class="n">plt</span><span class="o">.</span><span class="n">xkcd</span><span class="p">():</span>
    <span class="n">plot_attention_weights</span><span class="p">(</span><span class="n">w_att</span><span class="p">,</span> <span class="n">correct_ids</span><span class="p">,</span> <span class="n">context_length</span><span class="p">,</span> <span class="n">y_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Target (function-relevant indices) indices: [1, 17, 26]
</pre></div>
</div>
<img alt="../../../_images/85fd1b9240a4f1018caf330cff6fe8eeefe2586020dc8dec7670991ec88d39b5.png" src="../../../_images/85fd1b9240a4f1018caf330cff6fe8eeefe2586020dc8dec7670991ec88d39b5.png"/>
</div>
</div>
</section>
</section>
<section id="sample-complexity-for-sparse-variable-creation">
<h2>Sample Complexity for sparse variable creation<a class="headerlink" href="#sample-complexity-for-sparse-variable-creation" title="Permalink to this heading">#</a></h2>
<p>So far, we saw that a simple transformer model can effectively learn to represent an s-sparse boolean function. Next, we would like to quantify the sample complexity of self-attention for these functions. The Figure below is from the paper <a class="reference external" href="https://proceedings.mlr.press/v162/edelman22a.html">Inductive Biases and Variable Creation in Self-Attention Mechanisms</a>. Given the time and computation limits of our tutorial, we will only show their results here, but you can find the implementation and detailed results on GitHub.</p>
<p>They rigorously show that the number of training samples <span class="math notranslate nohighlight">\(m\)</span> needed to achieve good performance on an s-sparse function (the ‘critical sample size’) grows only <em>logarithmically</em> with context length <span class="math notranslate nohighlight">\(T\)</span>.</p>
<div>
<img src="https://raw.githubusercontent.com/ssnio/nma_neuroai_d4_t4/main/static/Fig_2_ref2.png" width="500"/>
</div></section>
</section>
<script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./tutorials/W1D5_Microcircuits/instructor"
        },
        predefinedOutput: true
    }
    </script>
<script>kernelName = 'python3'</script>
</article>
<footer class="bd-footer-article">
<!-- Previous / next buttons -->
<div class="prev-next-area">
<a class="left-prev" href="W1D5_Tutorial2.html" id="prev-link" title="previous page">
<i class="fa-solid fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title">Tutorial 2: Normalization</p>
</div>
</a>
<a class="right-next" href="../../W2D1_Macrocircuits/chapter_title.html" id="next-link" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title">Macrocircuits</p>
</div>
<i class="fa-solid fa-angle-right"></i>
</a>
</div>
</footer>
</div>
<div class="bd-sidebar-secondary bd-toc">
<div class="toc-item">
<div class="tocsection onthispage">
<i class="fa-solid fa-list"></i> On this page
</div>
<nav class="page-toc" id="bd-toc-nav">
<ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#">
   Tutorial 3: Attention
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#tutorial-objectives">
   Tutorial Objectives
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#setup">
   Setup
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#figure-settings">
     Figure settings
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#plotting-functions">
     Plotting functions
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#data-retrieval">
     Data retrieval
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#helper-functions">
     Helper functions
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#set-random-seed">
     Set random seed
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#set-device-gpu-or-cpu-execute-set-device">
     Set device (GPU or CPU). Execute
     <code class="docutils literal notranslate">
<span class="pre">
       set_device()
      </span>
</code>
</a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-intro-to-multiplication">
   Section 1. Intro to multiplication
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#exercise-1">
     Exercise 1
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#run-auto">
       { run: “auto” }
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-self-attention-math">
   Section 2. Self-attention: math
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#exercise-2-implement-self-attention">
     Exercise 2. Implement self-attention
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-inductive-bias-of-self-attention-sparse-variable-creation">
   Section 3. Inductive bias of self-attention: Sparse variable creation
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#dataset">
     Dataset
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#exercise-2-mlp-vs-self-attention">
     Exercise 2. MLP vs. Self-Attention
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#part-1-mlp">
       Part 1: MLP
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#part-2-self-attention">
       Part 2. Self-Attention
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#part-3-coding-exercise-comparing-results">
       Part 3. Coding Exercise Comparing results
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#part-4-sample-complexity">
       Part 4. Sample complexity
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#part-5-attention-weights">
       Part 5. Attention Weights
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#sample-complexity-for-sparse-variable-creation">
     Sample Complexity for sparse variable creation
    </a>
</li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
</div>
<footer class="bd-footer-content">
<div class="bd-footer-content__inner">
<div class="bd-footer-content__inner container">
<div class="footer-item">
<p class="component-author">
By Neuromatch
</p>
</div>
<div class="footer-item">
</div>
<div class="footer-item">
<p class="last-updated">
Last updated on None.<br/>
</p>
</div>
<div class="footer-item">
<div class="extra_footer">
<div>
<a href="http://creativecommons.org/licenses/by/4.0/"><img src="https://i.creativecommons.org/l/by/4.0/88x31.png"/></a>
<a href="https://opensource.org/licenses/BSD-3-Clause"><img src="https://camo.githubusercontent.com/9b9ea65d95c9ef878afa1987df65731d47681336/68747470733a2f2f696d672e736869656c64732e696f2f707970692f6c2f736561626f726e2e737667"/></a>
The contents of this repository are shared under the <a href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.
Software elements are additionally licensed under the <a href="https://opensource.org/licenses/BSD-3-Clause">BSD (3-Clause) License</a>.
</div>
</div>
</div>
</div>
</div>
</footer>
</main>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="../../../_static/scripts/bootstrap.js?digest=796348d33e8b1d947c94"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=796348d33e8b1d947c94"></script>
</body>
</html>